"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
NKOTRONIC BACKEND - VERSION ULTRA-OPTIMISÃ‰E
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âœ… ModÃ¨le : gemini-2.5-flash
âœ… System prompt optimisÃ© pour rÃ©ponses rapides
âœ… Gestion intelligente de l'historique (limite Ã  10 messages)
âœ… Endpoint /health avec cold start detection
âœ… Message systÃ¨me intÃ©grÃ© dans l'historique
âœ… Cleanup automatique des sessions (PÃ‰RIODIQUE, pas Ã  chaque requÃªte)
âœ… Variable FIRST_REQUEST correctement initialisÃ©e
âœ… Streaming SSE optimisÃ©
âœ… Historique tronquÃ© AVANT et APRÃˆS chaque requÃªte
âœ… max_tokens rÃ©duit Ã  800 (au lieu de 4000)
âœ… Temperature rÃ©duite Ã  0.4 (au lieu de 0.7)
âœ… Logging en WARNING en production
âœ… Timeout de 30s sur les requÃªtes Gemini
âœ… Message de bienvenue supprimÃ© aprÃ¨s premiÃ¨re interaction
âœ… Cleanup pÃ©riodique toutes les 100 requÃªtes
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, List, Optional
from datetime import datetime, timedelta
import google.generativeai as genai
import os
import json
import logging
import asyncio

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION DU LOGGING - âœ… WARNING en production pour performances
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
LOG_LEVEL = os.environ.get("LOG_LEVEL", "WARNING")  # WARNING par dÃ©faut, INFO pour debug
logging.basicConfig(level=getattr(logging, LOG_LEVEL))
logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VARIABLES GLOBALES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

sessions: Dict[str, 'SessionData'] = {}
SERVER_START_TIME = datetime.now()
FIRST_REQUEST = True
LOADING_STATUS = {
    "grammar_loaded": False,
    "grammar_load_time": None
}

# âœ… Compteur pour cleanup pÃ©riodique
REQUEST_COUNTER = 0
CLEANUP_INTERVAL = 100  # Cleanup toutes les 100 requÃªtes au lieu de chaque requÃªte

# Configuration de l'historique - âœ… OptimisÃ© pour vitesse
MAX_HISTORY_MESSAGES = 10  # Limite stricte Ã  10 messages

# Message systÃ¨me affichÃ© Ã  l'utilisateur
SYSTEM_MESSAGE = "Alu ni djÃ¶ ! Je suis Nkotronic, votre assistant du N'ko. Que puis-je faire pour vous ?"

# âœ… System prompt optimisÃ© pour guider le modÃ¨le (INCHANGÃ‰ comme demandÃ©)
SYSTEM_PROMPT = """Tu es Nkotronic, un assistant spÃ©cialisÃ© dans l'Ã©criture N'ko, la culture africaine et la culture mandingue. Tu es citoyen de l'Etat FÃ©dÃ©ral Africain

DIRECTIVES DE RÃ‰PONSE:
- RÃ©ponds de maniÃ¨re concise et directe
- PrivilÃ©gie les rÃ©ponses courtes (2-3 phrases) sauf si l'utilisateur demande des dÃ©tails approfondis
- Pour les traductions, donne le rÃ©sultat immÃ©diatement sans explications superflues
- Pour les traductions en Nko, utilise la grammaire standard de Solomana KantÃ©
- Pour les questions de grammaire N'ko, sois prÃ©cis et pÃ©dagogique mais concis
- Maintiens un ton amical et professionnel
- Si tu ne connais pas la rÃ©ponse exacte, dis-le honnÃªtement en une phrase

EXPERTISE:
- Ã‰criture et alphabet N'ko (ß’ßß)
- Grammaire standard de Solomana KantÃ©
- Culture et histoire africaine et mandingue
- Traduction franÃ§ais â†” N'ko

STYLE:
- Fluide et naturel
- Pas de longs prÃ©ambules
- Va droit au but
- Utilise des exemples concrets quand nÃ©cessaire"""

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION GEMINI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")
if not GEMINI_API_KEY:
    raise ValueError("âŒ GEMINI_API_KEY manquante dans les variables d'environnement")

genai.configure(api_key=GEMINI_API_KEY)

# Configuration de sÃ©curitÃ©
safety_settings = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
]

# âœ… Timeout pour les requÃªtes Gemini (30 secondes)
GEMINI_TIMEOUT = 30

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MODÃˆLES DE DONNÃ‰ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SessionData(BaseModel):
    session_id: str
    history: List[dict]
    created_at: datetime
    last_activity: datetime
    message_count: int = 0
    welcome_shown: bool = False  # âœ… Pour supprimer le message de bienvenue aprÃ¨s 1Ã¨re interaction

class ChatRequest(BaseModel):
    message: str
    session_id: str = "default"
    model: str = "gemini-2.5-flash"
    temperature: float = 0.4  # âœ… RÃ‰DUIT de 0.7 Ã  0.4 pour vitesse
    max_tokens: int = 800      # âœ… RÃ‰DUIT de 4000 Ã  800 pour vitesse

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FASTAPI APP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

app = FastAPI(title="Nkotronic API", version="2.2.0")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FONCTIONS UTILITAIRES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def cleanup_old_sessions():
    """
    âœ… Nettoie les sessions inactives depuis plus de 24h
    APPELÃ‰ PÃ‰RIODIQUEMENT, pas Ã  chaque requÃªte
    """
    now = datetime.now()
    to_delete = []
    
    for session_id, session in sessions.items():
        if (now - session.last_activity) > timedelta(hours=24):
            to_delete.append(session_id)
    
    for session_id in to_delete:
        del sessions[session_id]
        if LOG_LEVEL == "INFO":
            logger.info(f"ğŸ—‘ï¸  Session supprimÃ©e: {session_id}")
    
    if to_delete and LOG_LEVEL == "INFO":
        logger.info(f"ğŸ§¹ Nettoyage: {len(to_delete)} session(s) supprimÃ©e(s)")

def should_cleanup() -> bool:
    """
    âœ… DÃ©termine si un cleanup doit Ãªtre effectuÃ©
    Cleanup toutes les CLEANUP_INTERVAL requÃªtes au lieu de chaque fois
    """
    global REQUEST_COUNTER
    REQUEST_COUNTER += 1
    
    if REQUEST_COUNTER % CLEANUP_INTERVAL == 0:
        if LOG_LEVEL == "INFO":
            logger.info(f"ğŸ”„ Cleanup pÃ©riodique (requÃªte #{REQUEST_COUNTER})")
        return True
    return False

def truncate_history(history: List[dict], max_messages: int = MAX_HISTORY_MESSAGES) -> List[dict]:
    """
    âœ… Tronque l'historique de maniÃ¨re agressive
    
    StratÃ©gie ultra-optimisÃ©e:
    1. Garde UNIQUEMENT le system prompt (2 premiers messages)
    2. Garde les N derniers Ã©changes user/model
    3. SUPPRIME tout le reste
    
    Args:
        history: L'historique complet
        max_messages: Nombre maximum de messages Ã  garder (hors system prompt)
    
    Returns:
        Historique tronquÃ© optimisÃ©
    """
    if len(history) <= max_messages + 2:
        return history
    
    # Garder: [system_prompt, system_response, ...derniers N messages]
    system_messages = history[:2]
    recent_messages = history[-(max_messages):]
    
    truncated = system_messages + recent_messages
    
    if LOG_LEVEL == "INFO":
        logger.info(f"âœ‚ï¸  Historique: {len(history)} â†’ {len(truncated)} messages")
    
    return truncated

def remove_welcome_message(history: List[dict]) -> List[dict]:
    """
    âœ… Supprime le message de bienvenue aprÃ¨s la premiÃ¨re interaction
    Garde uniquement le system prompt pour Ã©conomiser des tokens
    
    Args:
        history: L'historique complet
    
    Returns:
        Historique sans le message de bienvenue
    """
    if len(history) > 4:  # Si plus de 4 messages, on peut supprimer la bienvenue
        # Supprimer messages index 2 et 3 (le "Bonjour" et la rÃ©ponse de bienvenue)
        return history[:2] + history[4:]
    return history

def get_session(session_id: str, initialize: bool = False) -> SessionData:
    """RÃ©cupÃ¨re ou crÃ©e une session"""
    # âœ… Cleanup pÃ©riodique au lieu de systÃ©matique
    if should_cleanup():
        cleanup_old_sessions()
    
    if session_id not in sessions:
        if not initialize:
            raise HTTPException(status_code=404, detail=f"Session {session_id} introuvable")
        
        # CrÃ©er nouvelle session avec system prompt et message bienvenue
        sessions[session_id] = SessionData(
            session_id=session_id,
            history=[
                # System prompt (invisible pour l'utilisateur)
                {"role": "user", "parts": [{"text": SYSTEM_PROMPT}]},
                {"role": "model", "parts": [{"text": "Compris. Je suis Nkotronic, prÃªt Ã  aider avec le N'ko de maniÃ¨re concise et efficace."}]},
                # Message de bienvenue (visible pour l'utilisateur, sera supprimÃ© aprÃ¨s 1Ã¨re interaction)
                {"role": "user", "parts": [{"text": "Bonjour"}]},
                {"role": "model", "parts": [{"text": SYSTEM_MESSAGE}]}
            ],
            created_at=datetime.now(),
            last_activity=datetime.now(),
            message_count=0,
            welcome_shown=False
        )
        if LOG_LEVEL == "INFO":
            logger.info(f"âœ¨ Nouvelle session: {session_id}")
    
    return sessions[session_id]

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENDPOINTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/")
async def root():
    """Endpoint racine"""
    return {
        "service": "Nkotronic API",
        "version": "2.2.0",
        "status": "running",
        "model": "gemini-2.5-flash",
        "uptime_seconds": (datetime.now() - SERVER_START_TIME).total_seconds(),
        "active_sessions": len(sessions),
        "total_requests": REQUEST_COUNTER,
        "optimizations": [
            "System prompt optimisÃ© (INCHANGÃ‰)",
            f"Historique limitÃ© Ã  {MAX_HISTORY_MESSAGES} messages",
            "Troncature agressive avant/aprÃ¨s requÃªte",
            f"max_tokens rÃ©duit Ã  800 (Ã©tait 4000)",
            f"temperature rÃ©duite Ã  0.4 (Ã©tait 0.7)",
            f"Cleanup pÃ©riodique tous les {CLEANUP_INTERVAL} requÃªtes",
            f"Logging en {LOG_LEVEL} pour performances",
            f"Timeout Gemini: {GEMINI_TIMEOUT}s",
            "Message bienvenue supprimÃ© aprÃ¨s 1Ã¨re interaction"
        ]
    }

@app.get("/health")
async def health_check():
    """âœ… Endpoint de health check avec dÃ©tection de cold start"""
    uptime = (datetime.now() - SERVER_START_TIME).total_seconds()
    is_cold_start = uptime < 5
    
    return {
        "status": "healthy",
        "cold_start": is_cold_start,
        "uptime_seconds": uptime,
        "grammar_loaded": LOADING_STATUS["grammar_loaded"],
        "active_sessions": len(sessions),
        "total_requests": REQUEST_COUNTER,
        "model": "gemini-2.5-flash",
        "max_history": MAX_HISTORY_MESSAGES,
        "max_tokens": 800,
        "temperature": 0.4,
        "log_level": LOG_LEVEL
    }

@app.get("/loading-status")
async def loading_status():
    """Status du chargement de la grammaire N'ko"""
    return LOADING_STATUS

@app.get("/sessions")
async def list_sessions():
    """Liste toutes les sessions actives"""
    return {
        "total": len(sessions),
        "sessions": [
            {
                "session_id": s.session_id,
                "created_at": s.created_at.isoformat(),
                "last_activity": s.last_activity.isoformat(),
                "message_count": s.message_count,
                "history_length": len(s.history),
                "welcome_shown": s.welcome_shown
            }
            for s in sessions.values()
        ]
    }

@app.delete("/session/{session_id}")
async def delete_session(session_id: str):
    """Supprime une session spÃ©cifique"""
    if session_id in sessions:
        del sessions[session_id]
        if LOG_LEVEL == "INFO":
            logger.info(f"ğŸ—‘ï¸  Session supprimÃ©e: {session_id}")
        return {"status": "deleted", "session_id": session_id}
    raise HTTPException(status_code=404, detail="Session introuvable")

@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    âœ… Endpoint de chat ultra-optimisÃ©
    - Historique tronquÃ© avant/aprÃ¨s
    - max_tokens: 800 (au lieu de 4000)
    - temperature: 0.4 (au lieu de 0.7)
    - Timeout: 30s
    - Cleanup pÃ©riodique
    - Message bienvenue supprimÃ© aprÃ¨s 1Ã¨re interaction
    """
    global FIRST_REQUEST
    
    session_id = request.session_id
    user_message = request.message
    
    if LOG_LEVEL == "INFO":
        logger.info(f"ğŸ“© Message - Session: {session_id}")
        logger.info(f"ğŸ’¬ Contenu: {user_message[:50]}...")
    
    # Cold start detection
    uptime = (datetime.now() - SERVER_START_TIME).total_seconds()
    is_cold_start = FIRST_REQUEST and uptime < 60
    
    async def generate():
        global FIRST_REQUEST
        
        if not GEMINI_API_KEY:
            logger.error("âŒ ClÃ© API manquante")
            yield f"data: {json.dumps({'error': 'ClÃ© API manquante'})}\n\n"
            return
        
        try:
            # Cold start notification
            if is_cold_start:
                if LOG_LEVEL == "INFO":
                    logger.info("â„ï¸  Cold start dÃ©tectÃ©")
                yield f"data: {json.dumps({'cold_start': True, 'message': 'Initialisation...'})}\n\n"
                FIRST_REQUEST = False
            
            # RÃ©cupÃ©rer/crÃ©er session
            is_new_session = session_id not in sessions
            session = get_session(session_id, initialize=is_new_session)
            
            # âœ… Supprimer le message de bienvenue aprÃ¨s la premiÃ¨re vraie interaction
            if not session.welcome_shown and session.message_count > 0:
                session.history = remove_welcome_message(session.history)
                session.welcome_shown = True
                if LOG_LEVEL == "INFO":
                    logger.info(f"ğŸ‘‹ Message de bienvenue supprimÃ© (Ã©conomie tokens)")
            
            # âœ… CRITIQUE: Tronquer AVANT d'ajouter le nouveau message
            if LOG_LEVEL == "INFO":
                logger.info(f"ğŸ“Š Historique avant: {len(session.history)} messages")
            session.history = truncate_history(session.history, MAX_HISTORY_MESSAGES)
            if LOG_LEVEL == "INFO":
                logger.info(f"ğŸ“Š Historique aprÃ¨s troncature: {len(session.history)} messages")
            
            # Ajouter message utilisateur
            session.history.append({
                "role": "user",
                "parts": [{"text": user_message}]
            })
            
            # CrÃ©er le modÃ¨le
            model = genai.GenerativeModel(
                model_name=request.model,
                safety_settings=safety_settings
            )
            
            # âœ… GÃ©nÃ©rer avec timeout
            if LOG_LEVEL == "INFO":
                logger.info(f"ğŸ¤– GÃ©nÃ©ration (historique: {len(session.history)}, temp: {request.temperature}, max: {request.max_tokens})...")
            
            try:
                # âœ… Wrapper avec timeout de 30s
                response = model.generate_content(
                    session.history,
                    generation_config=genai.types.GenerationConfig(
                        temperature=request.temperature,
                        max_output_tokens=request.max_tokens,
                    ),
                    stream=True,
                    request_options={"timeout": GEMINI_TIMEOUT}
                )
                
                full_response = ""
                
                for chunk in response:
                    if chunk.text:
                        full_response += chunk.text
                        yield f"data: {json.dumps({'content': chunk.text})}\n\n"
                
            except asyncio.TimeoutError:
                logger.error(f"â±ï¸  Timeout aprÃ¨s {GEMINI_TIMEOUT}s")
                yield f"data: {json.dumps({'error': f'Timeout aprÃ¨s {GEMINI_TIMEOUT}s'})}\n\n"
                return
            
            # Ajouter la rÃ©ponse Ã  l'historique
            session.history.append({
                "role": "model",
                "parts": [{"text": full_response}]
            })
            
            # âœ… Tronquer aprÃ¨s ajout de la rÃ©ponse
            session.history = truncate_history(session.history, MAX_HISTORY_MESSAGES)
            
            # Mettre Ã  jour session
            session.last_activity = datetime.now()
            session.message_count += 1
            
            if LOG_LEVEL == "INFO":
                logger.info(f"âœ… RÃ©ponse: {len(full_response)} chars, historique final: {len(session.history)}")
            
            # Signal de fin
            yield f"data: {json.dumps({'done': True, 'message_count': session.message_count})}\n\n"
            
        except Exception as e:
            logger.error(f"âŒ Erreur: {str(e)}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STARTUP/SHUTDOWN EVENTS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.on_event("startup")
async def startup_event():
    """Ã‰vÃ©nement de dÃ©marrage"""
    logger.warning("â•" * 60)
    logger.warning("ğŸš€ NKOTRONIC API - VERSION ULTRA-OPTIMISÃ‰E")
    logger.warning("â•" * 60)
    logger.warning(f"ğŸ“… Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.warning(f"ğŸ¤– ModÃ¨le: gemini-2.5-flash")
    logger.warning(f"ğŸ”‘ ClÃ© API: {'âœ… OK' if GEMINI_API_KEY else 'âŒ KO'}")
    logger.warning(f"ğŸ“ Historique max: {MAX_HISTORY_MESSAGES} messages")
    logger.warning(f"ğŸ¯ max_tokens: 800 (optimisÃ©)")
    logger.warning(f"ğŸŒ¡ï¸  temperature: 0.4 (optimisÃ©)")
    logger.warning(f"â±ï¸  timeout: {GEMINI_TIMEOUT}s")
    logger.warning(f"ğŸ§¹ Cleanup: tous les {CLEANUP_INTERVAL} requÃªtes")
    logger.warning(f"ğŸ“Š Log level: {LOG_LEVEL}")
    logger.warning("â•" * 60)
    
    LOADING_STATUS["grammar_loaded"] = True
    LOADING_STATUS["grammar_load_time"] = datetime.now().isoformat()

@app.on_event("shutdown")
async def shutdown_event():
    """Ã‰vÃ©nement d'arrÃªt"""
    logger.warning("=" * 60)
    logger.warning("ğŸ›‘ NKOTRONIC API - ARRÃŠT")
    logger.warning(f"ğŸ“Š Sessions actives: {len(sessions)}")
    logger.warning(f"ğŸ“¨ Total requÃªtes: {REQUEST_COUNTER}")
    logger.warning(f"â±ï¸  Uptime: {(datetime.now() - SERVER_START_TIME).total_seconds():.0f}s")
    logger.warning("=" * 60)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# POINT D'ENTRÃ‰E
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)