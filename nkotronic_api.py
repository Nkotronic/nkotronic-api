# =================================================================
# Fichier : nkotronic_api.py
# Backend de l'application Nkotronic (API FastAPI) - VERSION V9 (Compl√®te et Corrig√©e)
# =================================================================

import os
import json
import re
import uuid
import time
import asyncio
import hashlib
from typing import Tuple, Optional, Dict, Any, List

# --- Imports pour FastAPI, Pydantic et Configuration ---
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from dotenv import load_dotenv

# --- Imports pour Qdrant et LLM ---
from qdrant_client import QdrantClient, models
from qdrant_client.models import PointStruct, SearchRequest 
from openai import OpenAI, APIError

# --- IMPORTS DE LA BASE DE CONNAISSANCES N'KO ---
# NOTE: Assurez-vous que nko_knowledge_data.py et bcs_data.py sont disponibles
from bcs_data import BCS_INITIAL_FACTS
from nko_knowledge_data import NKO_STRUCTURED_KNOWLEDGE 

# --- 1. CONFIGURATION ET CL√âS SECR√àTES ---
load_dotenv()

# R√©cup√©ration des cl√©s API (DOIVENT √™tre d√©finies dans votre fichier .env)
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
LLM_API_KEY = os.getenv("LLM_API_KEY")

# Configuration des mod√®les
COLLECTION_NAME = "nkotronic_knowledge_base"
EMBEDDING_MODEL = "text-embedding-ada-002"       # Mod√®le de vectorisation (dim 1536)
LLM_MODEL = "gpt-4o-mini"                        # Mod√®le conversationnel
VECTOR_SIZE = 1536                               # Taille des vecteurs pour Qdrant

# --- 2. INITIALISATION DES CLIENTS GLOBALES ---
QDRANT_CLIENT: Optional[QdrantClient] = None
LLM_CLIENT: Optional[OpenAI] = None
QDRANT_LOCK = asyncio.Lock()

try:
    # 1. Initialisation Qdrant
    if QDRANT_URL and QDRANT_API_KEY:
        QDRANT_CLIENT = QdrantClient(
            url=QDRANT_URL,
            api_key=QDRANT_API_KEY,
            # üö® CORRECTION : RETIRER LE PARAM√àTRE INCONNU üö®
            # check_compatibility=False  <-- A RETIRER
        )

    # 2. Initialisation LLM (OpenAI)
    
    # üëá TEST DE DIAGNOSTIC SIMPLIFI√â
    if LLM_API_KEY:
        print(f"DEBUG SUCC√àS : LLM_API_KEY est charg√©e. Cl√© : {LLM_API_KEY[:5]}...")
        # CORRECTION : Augmentation du timeout √† 60s pour les appels d'embeddings lents
        LLM_CLIENT = OpenAI(api_key=LLM_API_KEY, timeout=60.0) 
    else:
        print("DEBUG ERREUR CRITIQUE : LLM_API_KEY est vide/NONE. Initialisation LLM impossible.")
        LLM_CLIENT = None
    
    # FIN DU TEST
    
except Exception as e:
    print(f"ERREUR CRITIQUE: √âchec de l'initialisation des clients Qdrant/LLM. D√©tail: {e}")
    QDRANT_CLIENT = None
    LLM_CLIENT = None

except Exception as e:
    print(f"ERREUR CRITIQUE: √âchec de l'initialisation des clients Qdrant/LLM. D√©tail: {e}")
    QDRANT_CLIENT = None
    LLM_CLIENT = None

except Exception as e:
    print(f"ERREUR CRITIQUE: √âchec de l'initialisation des clients Qdrant/LLM. D√©tail: {e}")
    QDRANT_CLIENT = None
    LLM_CLIENT = None


# --- 3. PROMPT SYST√àME (Le Cerveau de Nkotronic) ---
PROMPT_SYSTEM = """
Tu es Nkotronic, l'Analyste, l'Organisateur de la M√©moire et l'Autorit√© Linguistique du N'ko.

‚ö†Ô∏è R√àGLES CRITIQUES - √Ä RESPECTER ABSOLUMENT :

1. PRISE DE D√âCISION, SYNTH√àSE et CONCISION :
    - Pour toute question factuelle utilisant le mot 'lettre' (ﬂõﬂìﬂçﬂòﬂãﬂ≤), tu dois **TOUJOURS** r√©pondre en utilisant le nombre total de lettres de l'alphabet N'ko trouv√© dans le CONTEXTE RAG.
    - **PRIORIT√â ABSOLUE :** Ta r√©ponse doit √™tre **courte, directe et factuelle** et uniquement bas√©e sur le nombre factuel (27, 7 ou 19, etc.). NE JAMAIS compter l'occurrence des mots dans la phrase de l'utilisateur. NE JAMAIS verbaliser le processus de v√©rification.
    - Pour l'analyse ou la transcription, utilise le FACT RAG fourni par le moteur (score 1.0) comme r√©ponse unique et d√©finitive.

2. **PRIORIT√â ABSOLUE AU CONTEXTE M√âMOIRE RAG** :
    - Si le CONTEXTE M√âMOIRE RAG contient une information (traduction, d√©finition, r√®gle), tu DOIS l'utiliser EXCLUSIVEMENT.
    - JAMAIS inventer ou deviner une traduction si elle n'est pas dans le contexte RAG.
    
3. **COMPORTEMENT EN CAS D'ABSENCE D'INFORMATION ET DE TYPE DE QUESTION** :
    - **Pour les questions de faits, de r√®gles ou de traductions N'ko (qui n√©cessitent le RAG) :**
      * Si le contexte RAG est vide ou non pertinent :
          1. DIS CLAIREMENT : "Je ne connais pas encore cette information dans ma m√©moire."
          2. PROPOSE : "Voulez-vous me l'apprendre ?"
      * N'invente JAMAIS de traductions N'ko ou de faits.
    - **Pour les questions conversationnelles, les salutations ou les sujets g√©n√©raux :**
      * R√©ponds de mani√®re naturelle et engageante, en utilisant ta personnalit√© d'Analyste Nkotronic.
      * Tu n'as pas besoin de mentionner le manque de m√©moire ou de proposer un apprentissage dans ce cas.

4. **GESTION DE LA M√âMOIRE** :
    - Quand un utilisateur t'apprend quelque chose (ex: "chat se dit ﬂõﬂä en N'ko"), tu dois :
      a) Confirmer que tu as enregistr√© l'information
      b) G√©n√©rer le JSON de m√©moire dans les balises <MEMOIRE></MEMOIRE>
    
5. **FORMAT DE SORTIE M√âMOIRE** :
    - Le JSON doit √™tre un tableau d'objets [...]
    - Champs requis :
      * "concept_identifie": un identifiant stable (ex: "traduction_chat_nko")
      * "element_fran√ßais": description compl√®te en fran√ßais
      * "element_nko": traduction ou √©quivalent en N'ko (si applicable)

Exemple de r√©ponse avec apprentissage :
"Merci ! J'ai bien enregistr√© que 'chat' se dit ﬂõﬂä en N'ko. <MEMOIRE>[{"concept_identifie": "traduction_chat_nko", "element_fran√ßais": "Le mot 'chat' se traduit par ﬂõﬂä en √©criture N'ko", "element_nko": "ﬂõﬂä"}]</MEMOIRE>"

Exemple de r√©ponse sans information (pour un fait N'ko) :
"Je ne connais pas encore la traduction de ce mot dans ma m√©moire. Voulez-vous me l'apprendre ?"

Message Utilisateur:
"""

# =================================================================
# 4. FONCTIONS UTILITAIRES SYNCHRONES 
# =================================================================

def separer_texte_et_json(llm_output: str) -> Tuple[str, Optional[List[Dict[str, Any]]]]:
    """Extrait le JSON de m√©moire et retourne le texte de r√©ponse et l'objet JSON."""
    json_data = None
    json_match = re.search(r"<MEMOIRE>(.*?)</MEMOIRE>", llm_output, re.DOTALL)
    
    if json_match:
        json_string = json_match.group(1).strip()
        response_text = llm_output.replace(json_match.group(0), "").strip()
        try:
            parsed_data = json.loads(json_string)
            if isinstance(parsed_data, list):
                json_data = parsed_data
            else:
                print("AVERTISSEMENT: Le JSON extrait n'est pas un tableau (List).")
        except json.JSONDecodeError as e:
            print(f"ERREUR: √âchec du d√©codage JSON de la m√©moire. Erreur: {e}")
    else:
        response_text = llm_output
        
    return response_text, json_data


# Ins√©rer dans la section 4. FONCTIONS UTILITAIRES SYNCHRONES

def transcrire_et_analyser(message: str) -> str:
    """
    Tente de d√©tecter si le message est une requ√™te de transcription phon√©tique 
    (Fran√ßais -> N'ko) ou d'analyse N'ko (N'ko -> IPA/Fran√ßais) et fournit une r√©ponse directe.

    Retourne un fait format√© pour le LLM si la d√©tection est positive, sinon retourne None.
    """
    
    # R√©cup√©rer la map phon√©tique
    PHONETIC_MAP = NKO_STRUCTURED_KNOWLEDGE['PHONETICS']['MAP']
    
    # 1. T√¢che de D√âTECTION DE TRANSCRIPTION (Fran√ßais -> N'ko)
    # Ex: "transcris 'tomate' en n'ko"
    match_transcribe = re.search(r"(transcris|traduis phon√©tiquement|√©cris phon√©tiquement) ['\"]?([a-zA-Z√Ä-√ø\s]+)['\"]? en n[']?ko", message, re.IGNORECASE)
    
    if match_transcribe:
        word_to_transcribe = match_transcribe.group(2).lower().strip()
        
        # Ce n'est qu'une d√©monstration simple de substitution lettre par lettre
        # Dans un vrai syst√®me, il faudrait un mod√®le de prononciation plus complexe.
        nko_output = []
        for char in word_to_transcribe:
            # On cherche une correspondance simple, sinon on garde l'espace
            nko_char = PHONETIC_MAP.get(char)
            if isinstance(nko_char, str):
                nko_output.append(nko_char)
            elif char == ' ':
                nko_output.append(' ')
        
        nko_result = "".join(nko_output)
        
        if nko_result:
            # Formatage du r√©sultat pour qu'il soit inject√© comme un fait RAG (Score de 1.0)
            fact = f"""
CONTEXTE M√âMOIRE RAG (PRIORIT√â ABSOLUE - TRANSCRIPTION):
FACT 1 (Score: 1.00) - transcription_directe: La transcription phon√©tique de '{word_to_transcribe}' est calcul√©e comme √©tant : {nko_result} | N'ko: {nko_result}
            """
            return fact.strip()

    # 2. T√¢che de D√âTECTION D'ANALYSE (N'ko -> Fran√ßais/Phon√©tique)
    # Ex: "lis ﬂìﬂä"
    match_analyse = re.search(r"(lis|prononce|analyse) ['\"]?(\s*[\u07C0-\u07FF]+\s*)['\"]?", message)
    
    if match_analyse:
        nko_word = match_analyse.group(2).strip()
        ipa_output = []
        
        # Inversion de la map pour N'ko -> IPA/Phon√©tique
        IPA_REVERSE_MAP = {}
        for ipa_char, nko_chars in PHONETIC_MAP.items():
            if isinstance(nko_chars, str):
                IPA_REVERSE_MAP[nko_chars] = ipa_char
            elif isinstance(nko_chars, list):
                for nko_char in nko_chars:
                    IPA_REVERSE_MAP[nko_char] = ipa_char
        
        for nko_char in nko_word:
            ipa_char = IPA_REVERSE_MAP.get(nko_char, nko_char) # Garde le caract√®re si pas de map
            ipa_output.append(ipa_char)
            
        ipa_result = "".join(ipa_output)
        
        # Formatage du r√©sultat pour qu'il soit inject√© comme un fait RAG
        fact = f"""
CONTEXTE M√âMOIRE RAG (PRIORIT√â ABSOLUE - ANALYSE):
FACT 1 (Score: 1.00) - analyse_directe: L'analyse phon√©tique du terme N'ko '{nko_word}' est : [{ipa_result}]. | N'ko: {nko_word}
        """
        return fact.strip()
        
    return None


def mettre_a_jour_memoire(json_data: List[Dict[str, Any]]):
    """
    Cr√©e les embeddings et ins√®re les nouveaux points de m√©moire dans Qdrant.
    Utilise le concept_identifie pour cr√©er un ID stable et forcer l'√©crasement.
    """
    if not QDRANT_CLIENT or not LLM_CLIENT:
        print("Mise √† jour de m√©moire ignor√©e: Clients non disponibles.")
        return

    texts_to_embed = []
    facts_to_process = [] # Liste pour garder les faits ordonn√©s

    for fact in json_data:
        if 'concept_identifie' in fact and 'element_fran√ßais' in fact:
            # 1. Cr√©ation de la cl√© stable pour l'overwrite
            concept_key = fact['concept_identifie'].lower().strip()
            # Utilisation de sha256 pour g√©n√©rer un ID entier stable et unique par concept
            stable_id = int(hashlib.sha256(concept_key.encode('utf-8')).hexdigest(), 16) % (2**63)

            text = f"{fact['concept_identifie']} : {fact['element_fran√ßais']} {fact.get('element_nko', '')}"
            texts_to_embed.append(text)
            facts_to_process.append((stable_id, fact)) # Stockage de l'ID et du fait
        else:
            print("AVERTISSEMENT: Fait ignor√© car il manque 'concept_identifie' ou 'element_fran√ßais'.")

    if not texts_to_embed:
        print("Mise √† jour de m√©moire: Aucun fait valide √† ins√©rer.")
        return

    try:
        response = LLM_CLIENT.embeddings.create(input=texts_to_embed, model=EMBEDDING_MODEL)
        
        points_to_insert = []
        for i, (stable_id, fact) in enumerate(facts_to_process):
            vector = response.data[i].embedding
            points_to_insert.append(
                models.PointStruct(
                    # UTILISATION DE L'ID STABLE POUR L'OVERWRITE :
                    id=stable_id, 
                    vector=vector,
                    payload=fact
                )
            )

        if points_to_insert:
            QDRANT_CLIENT.upsert(
                collection_name=COLLECTION_NAME,
                wait=True,
                points=points_to_insert
            )
            print(f"--- {len(points_to_insert)} FAITS DE M√âMOIRE MIS √Ä JOUR (OVERWRITE PAR ID STABLE). ---")

    except Exception as e:
        print(f"ERREUR CRITIQUE lors de la mise √† jour de la m√©moire Qdrant: {e}")

def rechercher_memoire_qdrant(query_vector: List[float], limit: int) -> List[models.ScoredPoint]:
    """
    Fonction utilisant query_points (m√©thode moderne pour Qdrant 1.8+).
    """
    if not QDRANT_CLIENT:
        return []
    
    try:
        # Nouvelle API moderne (remplace .search)
        response = QDRANT_CLIENT.query_points(
            collection_name=COLLECTION_NAME,
            query=query_vector,
            limit=limit,
            with_payload=True,
        )
        
        # query_points retourne un objet avec .points
        return response.points if hasattr(response, 'points') else []
        
    except Exception as e:
        print(f"ERREUR lors de la recherche Qdrant: {e}")
        return []


def pre_traiter_requete(message: str) -> str:
    """
    Reformule la requ√™te utilisateur en substituant les termes N'ko
    m√©moris√©s (vocabulaire) par leurs √©quivalents fran√ßais.
    Ceci am√©liore la pertinence de la recherche RAG.
    """
    processed_message = message.lower()
    
    # Inversion du dictionnaire de vocabulaire pour recherche rapide N'ko -> Fran√ßais
    VOCAB_REVERSE_MAP = {}
    
    for fr_term, nko_terms in NKO_STRUCTURED_KNOWLEDGE['VOCABULARY'].items():
        if isinstance(nko_terms, str):
            VOCAB_REVERSE_MAP[nko_terms] = fr_term
        elif isinstance(nko_terms, list):
            for nko_term in nko_terms:
                VOCAB_REVERSE_MAP[nko_term] = fr_term


    # Substitution : Remplacer les termes N'ko par leur traduction fran√ßaise
    # On it√®re sur les termes N'ko du map invers√©.
    for nko_term, fr_term in VOCAB_REVERSE_MAP.items():
        # Utilisation de regex pour ne remplacer que des mots entiers (ou presque)
        # re.escape est CRITIQUE pour g√©rer les caract√®res N'ko
        processed_message = re.sub(r'\b' + re.escape(nko_term) + r'\b', fr_term, processed_message)

    
    # Deuxi√®me passe : Tenter d'identifier les questions de FAITS PURS (non-RAG direct)
    
    # 1. Requ√™te de type "Combien de [concept] ?"
    match_count = re.search(r"combien de (lettre|voyelle|consonne)(s)?\b", processed_message)
    if match_count:
        concept = match_count.group(1).lower()
        if concept in ['lettre', 'voyelle', 'consonne']:
            # On force la requ√™te RAG pour trouver le fait exact "il y a X lettres..."
            return f"information factuelle: nombre de {concept}s dans l'alphabet nko"

    # 2. Requ√™te de type "Montre-moi les [concept]"
    match_show = re.search(r"montre(s)? moi (les )?(lettre|voyelle|consonne)s?\b", processed_message)
    if match_show:
        concept = match_show.group(3).lower()
        if concept in ['lettre', 'voyelle', 'consonne']:
            # On force la requ√™te RAG pour trouver la liste exacte "les 27 lettres sont: ..."
            return f"liste des {concept}s de l'alphabet nko"

    return processed_message

# --- NOUVELLE FONCTION UTILITAIRE POUR LE BATCHING ---
def chunk_list(data: list, chunk_size: int) -> List[list]:
    """Divise une liste en lots (chunks) de taille maximale sp√©cifi√©e."""
    return [data[i:i + chunk_size] for i in range(0, len(data), chunk_size)]


# Fichier : nkotronic_api.py (Remplacement de la Section 5)

# =================================================================# 5. INITIALISATION ASYNCHRONE DE QDRANT (VERSION FINALE CORRIG√âE)
# =================================================================

# Global pour suivre l'√©tat d'initialisation
QDRANT_INITIALIZED = False

async def initialiser_qdrant(collection_name, dimension):
    global QDRANT_CLIENT, QDRANT_INITIALIZED, QDRANT_LOCK
    
    if not QDRANT_CLIENT or not LLM_CLIENT:
        print("Initialisation Qdrant non d√©marr√©e : Client Qdrant ou LLM non initialis√©.")
        return

    async with QDRANT_LOCK:
        if QDRANT_INITIALIZED:
            print("Qdrant d√©j√† initialis√©.")
            return

        print(f"Tentative de v√©rification/injection Qdrant...")
        
        # 1. Pr√©paration et Fusion des faits (Logique inchang√©e, utilise les variables globales)
        tous_les_faits = BCS_INITIAL_FACTS.copy()
        LEXIQUE_FILE = "bcs_lexique_auto.json"
        try:
            with open(LEXIQUE_FILE, 'r', encoding='utf-8') as f:
                lexique_faits = json.load(f)
                tous_les_faits.extend(lexique_faits)
                print(f"--- FUSION DES FAITS : {len(lexique_faits)} faits du lexique charg√©s. ---")
        except FileNotFoundError:
            print(f"AVERTISSEMENT: Fichier lexique '{LEXIQUE_FILE}' non trouv√©. Utilisation de BCS_INITIAL_FACTS uniquement.")
        except json.JSONDecodeError as e:
            print(f"ERREUR de d√©codage JSON dans '{LEXIQUE_FILE}'. V√©rifiez le format. {e}")
            
        print(f"Total des faits √† traiter : {len(tous_les_faits)}")

        # 2. V√©rification et Cr√©ation de la collection
        try:
            # Tente de r√©cup√©rer les informations de la collection
            collection_exists = True
            try:
                await asyncio.to_thread(
                    QDRANT_CLIENT.get_collection,
                    collection_name=collection_name
                )
            except Exception:
                collection_exists = False

            # Cr√©er la collection seulement si elle n'existe pas
            if not collection_exists:
                print(f"Collection '{collection_name}' non trouv√©e. Cr√©ation...")
                await asyncio.to_thread(
                    QDRANT_CLIENT.recreate_collection,
                    collection_name=collection_name,
                    vectors_config=models.VectorParams(size=dimension, distance=models.Distance.COSINE)
                )
                print(f"Collection '{collection_name}' cr√©√©e.")
            else:
                print(f"Collection '{collection_name}' trouv√©e. D√©marrage de la mise √† jour B.C.S...")
            
            # 3. Injection/Mise √† jour de TOUS les faits (BCS + Lexique)
            if tous_les_faits:
                print(f"D√©marrage de l'injection/mise √† jour B.C.S. : {len(tous_les_faits)} points...")
                
                # R√©cup√©rer les textes √† embarquer
                textes_a_embarquer = [f['element_fran√ßais'] for f in tous_les_faits]
                
                # --- NOUVELLE LOGIQUE DE BATCHING POUR √âVITER LA LIMITE OPENAI ---
                
                # Taille de lot de 100 entr√©es est tr√®s s√ªre pour les embeddings OpenAI.
                CHUNK_SIZE = 100
                
                # Diviser les textes et les faits en lots correspondants
                text_batches = chunk_list(textes_a_embarquer, CHUNK_SIZE)
                fact_batches = chunk_list(tous_les_faits, CHUNK_SIZE)
                
                total_points_injected = 0
                
                print(f"G√©n√©ration de {len(textes_a_embarquer)} embeddings... Traitement par lots de {CHUNK_SIZE}.")
                
                # Boucle d'injection par lots
                MAX_RETRIES = 3
                for i, (text_batch, fact_batch) in enumerate(zip(text_batches, fact_batches)):
    
                    # Messages de d√©bogage nettoy√©s
                    print(f"    -> Traitement du lot {i+1}/{len(text_batches)} ({len(text_batch)} faits)...")
    
                    # 1. G√©n√©ration des embeddings pour le lot 
                    embeddings_response = await asyncio.to_thread(
                        LLM_CLIENT.embeddings.create,
                        input=text_batch,
                        model=EMBEDDING_MODEL
                    )
                    
                    # 2. Cr√©ation des Points avec des ID STABLES pour le lot
                    points_to_upsert = []
                    for j, fact in enumerate(fact_batch):
                        # La logique de l'ID stable reste la m√™me (pour l'overwrite)
                        concept_key = fact.get('concept_identifie', f"bcs_fact_{total_points_injected + j}").lower().strip()
                        stable_id = int(hashlib.sha256(concept_key.encode('utf-8')).hexdigest(), 16) % (2**63)
                        
                        points_to_upsert.append(
                            models.PointStruct(
                                id=stable_id,
                                vector=embeddings_response.data[j].embedding,
                                payload=fact
                            )
                        )
                    
                    # 3. Injecter/Mettre √† jour les points Qdrant pour ce lot (upsert)
                    # üö® LOGIQUE DE R√âESSAI APPLIQU√âE ICI üö®
                    for attempt in range(MAX_RETRIES):
                        try:
                            # On injecte apr√®s chaque lot pour la fiabilit√©.
                            await asyncio.to_thread(
                                QDRANT_CLIENT.upsert,
                                collection_name=collection_name,
                                points=points_to_upsert,
                                wait=True
                            )
                            # Si l'upsert r√©ussit, on sort de la boucle de r√©essai
                            break
                        except Exception as e:
                            if attempt < MAX_RETRIES - 1:
                                print(f"AVERTISSEMENT: √âchec de l'injection Qdrant du lot {i+1} (Tentative {attempt+1}/{MAX_RETRIES}). Erreur: {e}. Nouvelle tentative dans 5 secondes...")
                                await asyncio.sleep(5)  # Attendre 5 secondes avant de r√©essayer
                            else:
                                # Si c'est la derni√®re tentative, on l√®ve l'exception critique
                                print(f"ERREUR CRITIQUE: √âchec de l'injection Qdrant du lot {i+1} apr√®s {MAX_RETRIES} tentatives. D√©tail: {e}")
                                # On l√®ve l'exception pour que le bloc except du point 2 la capture et arr√™te tout.
                                raise 

                    total_points_injected += len(points_to_upsert)
    
                    # Pause augment√©e (3s) pour √©viter le timeout de Qdrant/OpenAI
                    print("    -> Pause de 3 secondes pour respecter les limites de d√©bit et le d√©lai Qdrant...")
                    await asyncio.sleep(3) # Pause asynchrone
                    
                print(f"Injection/Mise √† jour B.C.S. de {total_points_injected} points termin√©e.")
            else:
                print("AVERTISSEMENT: Aucun fait √† injecter.")

        except Exception as e:
            # Ce bloc attrape les erreurs critiques apr√®s les tentatives de r√©essai
            print(f"ERREUR lors de la cr√©ation ou de l'injection Qdrant: {e}")
            QDRANT_CLIENT = None 
            return 

        QDRANT_INITIALIZED = True
        print(f"Initialisation Qdrant termin√©e. Collection '{collection_name}' pr√™te.")


# =================================================================
# 6. D√âCLARATION DE L'APPLICATION FASTAPI ET MIDDLEWARE
# =================================================================

app = FastAPI(
    title="Nkotronic Backend API",
    description="API pour le service RAG (Retrieval-Augmented Generation) N'ko.",
    version="1.0.0",
)

# --- Configuration CORS (ESSENTIEL pour le Frontend React) ---
origins = [
    "http://localhost",
    "http://localhost:3000",
    "http://localhost:8080",
    "*", 
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mod√®les de donn√©es pour les endpoints
class ChatRequest(BaseModel):
    message: str

class ChatResponse(BaseModel):
    response_text: str
    memory_update: Optional[List[Dict[str, Any]]] = None


# =================================================================
# 7. POINTS DE TERMINAISON (ENDPOINTS)
# =================================================================

@app.get("/health")
def health_check():
    """V√©rification de l'√©tat de l'API."""
    status = {
        "api_status": "OK",
        "qdrant_ready": QDRANT_CLIENT is not None,
        "llm_ready": LLM_CLIENT is not None,
    }
    return status


# Fichier : nkotronic_api.py (Fonction gerer_requete_chat corrig√©e)

@app.post("/chat", response_model=ChatResponse)
async def gerer_requete_chat(request: ChatRequest):
    """
    Point de terminaison asynchrone pour g√©rer les requ√™tes de chat, effectuer le RAG
    avec pr√©-traitement et mettre √† jour la m√©moire.
    """
    if not LLM_CLIENT:
        raise HTTPException(status_code=503, detail="Service LLM non initialis√©. Cl√© API manquante ou invalide.")

    # üö® INITIALISATION DES VARIABLES (CORRECTION DES ERREURS PYLANCE)
    rag_enabled = QDRANT_CLIENT is not None 
    user_message_original = request.message
    user_message = user_message_original # Initialisation par d√©faut
    
    # Initialisation d'un contexte par d√©faut
    contexte_rag = "\n\nCONTEXTE M√âMOIRE RAG:\n[Aucun contexte pertinent trouv√© dans la m√©moire utilisateur ou dans la base de connaissances statique. Utiliser la connaissance interne.]\n\n"
    
    # 1. TENTATIVE DE TRANSCRIPTION/ANALYSE DIRECTE
    contexte_direct = transcrire_et_analyser(user_message_original)
    
    if contexte_direct:
        # Si une r√©ponse directe est trouv√©e, on bypass le pr√©-traitement et le RAG normal
        contexte_rag = contexte_direct
        # user_message reste user_message_original (d√©j√† initialis√©)
        print("\n--- D√âBOGAGE RAG : CONTEXTE DIRECT (Transcription/Analyse) INJECT√â ---")
        
    else:
        # Si pas de r√©ponse directe, on proc√®de au Pr√©-Traitement de la requ√™te
        user_message = pre_traiter_requete(user_message_original)
        
        if user_message.lower() != user_message_original.lower():
            print(f"--- PR√â-TRAITEMENT APPLIQU√â : '{user_message_original}' -> '{user_message}' ---")

        # --- A. RAG (Retrieval-Augmented Generation) ---
        if rag_enabled:
            # Note: Le 'contexte_rag' est d√©j√† initialis√© au d√©faut.
            try:
                # 1. Vectorisation du message utilisateur (on utilise le user_message PR√â-TRAIT√â pour le RAG)
                # UTILISATION D'UN BLOC TRY/EXCEPT SP√âCIFIQUE ICI (Pour les erreurs d'Embedding/API)
                try:
                    user_vector_response = await asyncio.to_thread(
                        LLM_CLIENT.embeddings.create,
                        input=[user_message],
                        model=EMBEDDING_MODEL
                    )
                    user_vector = user_vector_response.data[0].embedding
                except Exception as e:
                    # Si l'embedding √©choue, on log l'erreur et on force user_vector √† None.
                    print(f"ERREUR D'EMBEDDING CRITIQUE: {e}. Le RAG est d√©sactiv√© pour cette requ√™te.")
                    user_vector = None
                
                
                if user_vector: # Continuer seulement si l'embedding a r√©ussi
                    # 2. Recherche de contexte pertinent
                    resultats_rag = await asyncio.to_thread(
                        rechercher_memoire_qdrant,
                        user_vector,
                        15 
                    )

                    # 3. Construction du contexte RAG
                    if resultats_rag:
                        contexte_rag = "\n\nCONTEXTE M√âMOIRE RAG (PRIORIT√â ABSOLUE):\n"
                        for i, point in enumerate(resultats_rag):
                            element_fr = point.payload.get('element_fran√ßais', 'Information N/A')
                            element_nko = point.payload.get('element_nko', '')
                            concept = point.payload.get('concept_identifie', 'N/A')
                            
                            contexte_rag += f"FACT {i+1} (Score: {point.score:.2f}) - {concept}: {element_fr} | N'ko: {element_nko}\n"
                        contexte_rag += "\n" 

            except Exception as e:
                # Cette erreur attrape les probl√®mes restants (comme Qdrant)
                print(f"ERREUR RAG G√âN√âRALE : {e}")
                contexte_rag = "\n\nCONTEXTE M√âMOIRE RAG (ERREUR RAG): [Utiliser uniquement la connaissance interne]\n\n"

    # --- B. Ex√©cution du LLM ---
    
    # --- D√âBOGAGE RAG : CONTEXTE ENVOY√â AU LLM ---
    print(f"\n--- D√âBOGAGE RAG : CONTEXTE ENVOY√â AU LLM ---\n{contexte_rag}\n-------------------------------------------------\n")

    # Le prompt final utilise le CONTEXTE RAG et le MESSAGE ORIGINAL
    prompt_final = PROMPT_SYSTEM + contexte_rag + f"Message Utilisateur : {user_message_original}"

    # ... (Reste de la fonction inchang√©e : appel LLM, post-traitement, retour) ...
    try:
        llm_completion = await asyncio.to_thread(
            LLM_CLIENT.chat.completions.create,
            model=LLM_MODEL,
            messages=[{"role": "system", "content": prompt_final}]
        )
        llm_output = llm_completion.choices[0].message.content
        
        # --- D√âBOGAGE : AFFICHER LA R√âPONSE DU LLM ---
        print(f"\n--- R√âPONSE BRUTE DU LLM ---\n{llm_output}\n--------------------------\n")
        
    except APIError as api_err:
        raise HTTPException(status_code=500, detail=f"Erreur de l'API LLM: {api_err.response.status_code} - {api_err.response.text}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur interne lors de l'appel LLM: {e}")


    # --- C. Post-Traitement, S√©paration et Mise √† Jour (D) ---
    response_text, json_data = separer_texte_et_json(llm_output)

    if json_data and rag_enabled:
        # Ex√©cution de la mise √† jour de m√©moire en arri√®re-plan
        asyncio.create_task(asyncio.to_thread(mettre_a_jour_memoire, json_data))
    elif json_data:
        print("AVERTISSEMENT: JSON de m√©moire g√©n√©r√© mais non trait√© car Qdrant est d√©sactiv√©.")

    # --- E. R√©ponse Finale (Inclus memory_update) ---
    return ChatResponse(
        response_text=response_text,
        memory_update=json_data 
    )


# =================================================================
# 8. T√¢che de D√©marrage
# =================================================================

@app.on_event("startup")
async def startup_event():
    """
    S'ex√©cute au d√©marrage de l'application pour garantir que la B.C.S. est en place.
    """
    print("D√©marrage de l'application Nkotronic API...")

    if QDRANT_URL and EMBEDDING_MODEL and LLM_CLIENT:
        # CORRECTION : Appel de la fonction initialiser_qdrant avec les bons arguments
        await initialiser_qdrant(
            COLLECTION_NAME, # La collection est COLLECTION_NAME
            VECTOR_SIZE      # La dimension est VECTOR_SIZE
        )
    else:
        print("Initialisation Qdrant ignor√©e : Clients ou cl√©s manquantes.")

# =================================================================
# FIN DU FICHIER
# =================================================================