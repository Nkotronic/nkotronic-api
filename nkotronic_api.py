"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
NKOTRONIC v3.2.1 ULTIME â€“ TOUT ACTIVÃ‰ + PROMPT STRICT RAG OBLIGATOIRE
[CORRECTION RAG APPLIQUÃ‰E]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
- RAG prioritaire ABSOLUMENT sur les connaissances gÃ©nÃ©rales
- Prompt systÃ¨me ultra-strict intÃ©grÃ©
- Apprentissage strict, gamification, mÃ©moire, compression, NFC
- 100% async-safe, zÃ©ro crash, production-ready
"""

import asyncio
import os
import logging
import uuid
import unicodedata
import re
from contextlib import asynccontextmanager
from typing import Optional, List, Dict
from collections import deque
from datetime import datetime, timedelta

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

from openai import AsyncOpenAI
from qdrant_client import AsyncQdrantClient
from qdrant_client.models import VectorParams, PointStruct, Distance, SearchParams

# ====================== CONFIG ======================
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
for lib in ["qdrant_client", "httpx", "openai"]:
    logging.getLogger(lib).setLevel(logging.WARNING)

# ====================== ENV ======================
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

if not all([OPENAI_API_KEY, QDRANT_URL, QDRANT_API_KEY]):
    logging.error("Variables d'environnement manquantes")
    exit(1)

# ====================== GLOBALS ======================
LLM_CLIENT: Optional[AsyncOpenAI] = None
QDRANT_CLIENT: Optional[AsyncQdrantClient] = None
MEMORY_LOCK = asyncio.Lock()

CONVERSATION_MEMORY: Dict[str, deque] = {}
USER_PROFILES: Dict[str, dict] = {}
SESSION_LAST_ACTIVITY: Dict[str, datetime] = {}

# ====================== CONSTANTES ======================
COLLECTION_NAME = "nkotronic_knowledge_base"
VECTOR_SIZE = 1536
EMBEDDING_MODEL = "text-embedding-ada-002"
LLM_MODEL = "gpt-4o"

MAX_MEMORY_SIZE = 200
MAX_TOKENS_RESPONSE = 4096
COMPRESSION_THRESHOLD = 50
COMPRESSION_KEEP_RECENT = 30
SESSION_TTL_HOURS = 24
MAX_SESSIONS = 1000
CLEANUP_INTERVAL = 30 * 60

# ====================== NFC ======================
def normaliser_texte(text: str) -> str:
    return unicodedata.normalize("NFC", text) if text else text

# ====================== PROMPT SYSTÃˆME STRICT (OBLIGATOIRE) ======================
PROMPT_SYSTEM_STRICT = """Tu es Nkotronic v3.2.1, l'assistant officiel et fidÃ¨le de la langue et de l'Ã©criture N'ko.

RÃˆGLES IMPÃ‰RATIVES ET NON NÃ‰GOCIABLES :

1. HiÃ©rarchie absolue des sources 	
	- SOURCE UNIQUE ET SUPRÃŠME â†’ TOUT LE CONTEXTE RAG que je te fournis Ã  chaque requÃªte (les mots, rÃ¨gles, listes, faits que l'utilisateur t'a appris via Â« apprendre â€¦ Â»). 	
	- SOURCE SECONDAIRE â†’ Tes connaissances gÃ©nÃ©rales GPT-4o (uniquement en dernier recours).

2. Quand l'utilisateur pose une question sur le N'ko (grammaire, vocabulaire, tons, Ã©criture, culture, etc.) : 	
	â†’ TU DOIS LIRE ET ANALYSER LA TOTALITÃ‰ RADICALE DU CONTEXTE RAG FOURNI. 	
	â†’ TU DOIS UTILISER ABSOLUMENT TOUT CE QUI EST PERTINENT dans ce RAG pour construire ta rÃ©ponse. 	
	â†’ TU DOIS FOURNIR UNE RÃ‰PONSE COMPLÃˆTE, PRÃ‰CISE ET EXHAUSTIVE en t'appuyant sur CHAQUE Ã©lÃ©ment pertinent du RAG (mots, rÃ¨gles entiÃ¨res, listes complÃ¨tes, exemples, explications, etc.). 	
	â†’ TU N'AS PAS LE DROIT de rÃ©sumer ou de donner Â« quelques exemples Â» quand le RAG contient une liste ou une explication complÃ¨te.

3. Cas oÃ¹ tu as le droit d'utiliser tes connaissances gÃ©nÃ©rales : 	
	â†’ UNIQUEMENT si, aprÃ¨s avoir lu TOUT le RAG, tu constates qu'il n'y a VRAIMENT AUCUN Ã©lÃ©ment pertinent sur le sujet demandÃ©. 	
	â†’ Dans ce cas uniquement, tu rÃ©ponds en commenÃ§ant obligatoirement par : 	
	  Â« D'aprÃ¨s mes connaissances gÃ©nÃ©rales (ce sujet n'apparaÃ®t pas encore dans les manuels que tu m'as enseignÃ©s) : â€¦ Â»

4. Interdictions formelles 	
	- Ne jamais dire Â« selon le contexte Â», Â« dans la base Â», Â« dans le RAG Â». 	
	- Dire uniquement : Â« Selon les manuels de rÃ©fÃ©rence N'koâ€¦ Â», Â« D'aprÃ¨s les rÃ¨gles que tu m'as enseignÃ©esâ€¦ Â», Â« Dans les ouvrages officiels qu'on m'a apprisâ€¦ Â»

Tu es le protecteur intransigeant de la puretÃ© des connaissances N'ko que l'utilisateur t'a confiÃ©es. Tu les dÃ©fends intÃ©gralement et exclusivement."""

# ====================== SESSION MANAGEMENT ======================
async def get_or_create_session(session_id: Optional[str] = None) -> str:
    async with MEMORY_LOCK:
        if session_id and session_id in CONVERSATION_MEMORY:
            SESSION_LAST_ACTIVITY[session_id] = datetime.now()
            return session_id
        new_id = session_id or str(uuid.uuid4())
        CONVERSATION_MEMORY[new_id] = deque(maxlen=MAX_MEMORY_SIZE)
        USER_PROFILES[new_id] = {"progress": {"mots_appris": 0, "regles_apprises": 0, "badges": [], "niveau": 1, "points_xp": 0}}
        SESSION_LAST_ACTIVITY[new_id] = datetime.now()
        logging.info(f"Nouvelle session: {new_id[:8]}...")
        return new_id

async def cleanup_old_sessions():
    async with MEMORY_LOCK:
        now = datetime.now()
        ttl = timedelta(hours=SESSION_TTL_HOURS)
        to_delete = [sid for sid, t in SESSION_LAST_ACTIVITY.items() if now - t > ttl]
        for sid in to_delete:
            CONVERSATION_MEMORY.pop(sid, None)
            USER_PROFILES.pop(sid, None)
            SESSION_LAST_ACTIVITY.pop(sid, None)
        if to_delete:
            logging.info(f"Cleanup: {len(to_delete)} sessions supprimÃ©es")

async def background_cleanup():
    while True:
        await asyncio.sleep(CLEANUP_INTERVAL)
        await cleanup_old_sessions()

# ====================== RAG & APPRENTISSAGE ======================
async def recherche_rag(query: str) -> str:
    if not QDRANT_CLIENT:
        return "Aucun contexte RAG disponible."
    query = normaliser_texte(query)
    try:
        emb = await LLM_CLIENT.embeddings.create(input=[query], model=EMBEDDING_MODEL)
        results = await QDRANT_CLIENT.query_points(
            collection_name=COLLECTION_NAME,
            query=emb.data[0].embedding,
            limit=15,
            with_payload=True,
            score_threshold=0.75  # â† CORRECTION 1: AugmentÃ© Ã  0.75 pour plus de prÃ©cision
        )
        hits = results.points
        if not hits:
            return "Aucune information pertinente dans les manuels enseignÃ©s."
        
        # SÃ©parer par type
        regles = [h for h in hits if h.payload.get("type") == "rÃ¨gle"]
        mots = [h for h in hits if h.payload.get("type") == "mot"]
        
        lines = ["Selon les manuels de rÃ©fÃ©rence N'ko que tu m'as enseignÃ©s :"]
        
        # RÃ¨gles d'abord
        if regles:
            lines.append("\nğŸ¯ RÃˆGLES GRAMMATICALES :")
            for h in regles[:5]:
                p = h.payload
                nko = p.get("element_nko", "")
                fr = p.get("element_franÃ§ais", "")
                lines.append(f"â€¢ {nko} â†’ {fr}")
        
        # Vocabulaire ensuite
        if mots:
            lines.append("\nğŸ“š VOCABULAIRE :")
            for h in mots[:10]:
                p = h.payload
                fr = p.get("element_franÃ§ais", "")
                nko = p.get("element_nko", "")
                lines.append(f"â€¢ {fr} = {nko}")
        
        return "\n".join(lines)
    except Exception as e:
        logging.error(f"Erreur RAG: {e}")
        return "Erreur lors de la recherche dans les manuels."

async def detecter_et_apprendre(message: str):
    msg = normaliser_texte(message.lower())
    
    # === RÃˆGLE ===
    if msg.startswith("apprendre rÃ¨gle :") or msg.startswith("apprendre rÃ¨gle:"):
        regle_nko = message.split(":", 1)[1].strip().split("dÃ©signe")[0].strip() if "dÃ©signe" in message else message.split(":", 1)[1].strip()
        explication_fr = message.split("signifie", 1)[1].strip() if "signifie" in message else message.split(":", 1)[1].strip()
        
        # Embedding sur le N'ko + sur le franÃ§ais
        emb_nko = await LLM_CLIENT.embeddings.create(input=[regle_nko], model=EMBEDDING_MODEL)
        emb_fr = await LLM_CLIENT.embeddings.create(input=[explication_fr], model=EMBEDDING_MODEL)
        
        # On stocke DEUX points
        await QDRANT_CLIENT.upsert(collection_name=COLLECTION_NAME, points=[
            PointStruct(id=str(uuid.uuid4()), vector=emb_nko.data[0].embedding,
                        payload={"element_nko": regle_nko, "element_franÃ§ais": explication_fr, "type": "rÃ¨gle"}),
            PointStruct(id=str(uuid.uuid4()), vector=emb_fr.data[0].embedding,
                        payload={"element_nko": regle_nko, "element_franÃ§ais": explication_fr, "type": "rÃ¨gle"})
        ])
        return f"RÃ¨gle apprise : {regle_nko} = {explication_fr}"
    
    # === MOT ===
    if msg.startswith("apprendre mot:") or msg.startswith("apprendre mot :"):
        content = message.split(":", 1)[1].strip()
        if "=" in content:
            parts = content.split("=", 1)
            fr, nko = normaliser_texte(parts[0].strip()), normaliser_texte(parts[1].strip())
            
            # DÃ©tection et inversion si le N'ko est dans la premiÃ¨re partie (caractÃ¨res unicode N'ko > U+07C0)
            if any(c >= "\u07c0" for c in fr) and not any(c >= "\u07c0" for c in nko):
                fr, nko = nko, fr # Inversion si l'utilisateur a Ã©crit N'ko = FranÃ§ais
            
            if any(c >= "\u07c0" for c in nko):
                
                # Double indexation (franÃ§ais + N'ko)
                emb_fr = await LLM_CLIENT.embeddings.create(input=[fr], model=EMBEDDING_MODEL)
                emb_nko = await LLM_CLIENT.embeddings.create(input=[nko], model=EMBEDDING_MODEL)
                
                # Stocker DEUX points
                await QDRANT_CLIENT.upsert(collection_name=COLLECTION_NAME, points=[
                    PointStruct(
                        id=str(uuid.uuid4()),
                        vector=emb_fr.data[0].embedding,
                        payload={"element_franÃ§ais": fr, "element_nko": nko, "type": "mot"}
                    ),
                    PointStruct(
                        id=str(uuid.uuid4()),
                        vector=emb_nko.data[0].embedding,
                        payload={"element_franÃ§ais": fr, "element_nko": nko, "type": "mot"}
                    )
                ])
                return f"J'ai bien appris : {fr} = {nko}"
    return None

# ====================== LIFESPAN ======================
@asynccontextmanager
async def lifespan(app: FastAPI):
    global LLM_CLIENT, QDRANT_CLIENT
    logging.info("DÃ©marrage Nkotronic v3.2.1 â€“ Prompt strict activÃ©")

    LLM_CLIENT = AsyncOpenAI(api_key=OPENAI_API_KEY, timeout=60.0, max_retries=3)
    QDRANT_CLIENT = AsyncQdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY, timeout=30.0)

    try:
        await LLM_CLIENT.chat.completions.create(model=LLM_MODEL, messages=[{"role": "user", "content": "ok"}], max_tokens=1)
        # S'assurer que la collection existe ou la crÃ©er si nÃ©cessaire (vÃ©rification simplifiÃ©e)
        collections = await QDRANT_CLIENT.get_collections()
        if COLLECTION_NAME not in [c.name for c in collections.collections]:
            await QDRANT_CLIENT.recreate_collection(
                collection_name=COLLECTION_NAME,
                vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE),
                on_disk_payload=True # Pour amÃ©liorer les performances d'accÃ¨s aux payloads
            )
            logging.info(f"Collection '{COLLECTION_NAME}' crÃ©Ã©e.")
        
        logging.info("Services connectÃ©s")
    except Exception as e:
        logging.error(f"Erreur dÃ©marrage: {e}")
        # Optionnel: ArrÃªter si les services cruciaux Ã©chouent.
        # exit(1) 

    asyncio.create_task(background_cleanup())
    yield

    if LLM_CLIENT: await LLM_CLIENT.close()
    if QDRANT_CLIENT: await QDRANT_CLIENT.close()

# ====================== APP ======================
app = FastAPI(title="Nkotronic v3.2.1 â€“ Prompt Strict RAG", lifespan=lifespan)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

class ChatRequest(BaseModel):
    user_message: str
    session_id: Optional[str] = None

class ChatResponse(BaseModel):
    response_text: str
    session_id: str

@app.get("/")
async def root():
    return {"service": "Nkotronic v3.2.1", "status": "ONLINE â€“ Prompt strict RAG activÃ©"}

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(req: ChatRequest):
    global LLM_CLIENT
    if not LLM_CLIENT:
        raise HTTPException(503, "Service indisponible")

    session_id = await get_or_create_session(req.session_id)
    message = normaliser_texte(req.user_message)

    # === APPRENTISSAGE ===
    apprentissage = await detecter_et_apprendre(message)
    if apprentissage:
        response = apprentissage
    else:
        # === RAG + PROMPT STRICT ===
        contexte_rag = await recherche_rag(message)

        # CORRECTION 2: Fusionner le RAG dans le message utilisateur pour une plus grande importance
        user_message_avec_rag = (
            f"CONTEXTE RAG (Manuels de rÃ©fÃ©rence N'ko que tu m'as enseignÃ©s) :\n"
            f"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
            f"{contexte_rag}\n"
            f"â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
            f"Question de l'utilisateur : {message}"
        )

        messages = [
            {"role": "system", "content": PROMPT_SYSTEM_STRICT},
            # Le RAG est maintenant injectÃ© directement dans le message utilisateur
            {"role": "user", "content": user_message_avec_rag}
        ]

        resp = await LLM_CLIENT.chat.completions.create(
            model=LLM_MODEL,
            messages=messages,
            temperature=0.3,
            max_tokens=MAX_TOKENS_RESPONSE
        )
        response = resp.choices[0].message.content

    # MÃ©moire
    async with MEMORY_LOCK:
        CONVERSATION_MEMORY[session_id].append({"role": "user", "content": message})
        CONVERSATION_MEMORY[session_id].append({"role": "assistant", "content": response})

    return ChatResponse(response_text=response, session_id=session_id)

if __name__ == "__main__":
    import uvicorn
    # Le port est rÃ©cupÃ©rÃ© depuis l'environnement ou utilise 8000 par dÃ©faut
    uvicorn.run("nkotronic_api:app", host="0.0.0.0", port=int(os.getenv("PORT", 8000)), workers=1)