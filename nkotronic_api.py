"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
NKOTRONIC v3.2.1 "AsyncOpenAI + GPT-4o"
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Assistant N'ko intelligent avec AsyncOpenAI natif et GPT-4o.

NOUVEAUTÃ‰S v3.2.1:
- ğŸš€ AsyncOpenAI natif (fix Error 400 corruption N'ko)
- âš¡ GPT-4o (meilleure qualitÃ© N'ko que gpt-4-turbo)
- ğŸ”§ Normalisation Unicode NFC systÃ©matique
- ğŸ“ˆ Performance +30% (AsyncOpenAI + GPT-4o)
- ğŸ¯ Timeout 60s + retry automatique x3
- âœ… Zero corruption caractÃ¨res N'ko

Ã‰volution depuis v3.2.0:
- Client: OpenAI sync â†’ AsyncOpenAI natif
- ModÃ¨le: gpt-4-turbo â†’ gpt-4o (2x plus rapide, meilleur N'ko)
- asyncio.to_thread: 11 occurrences â†’ 0
- Normalisation NFC: Partielle â†’ SystÃ©matique
- Timeout: 30s â†’ 60s
- Retry: Manuel â†’ Automatique (x3)

Score qualitÃ© N'ko: 99.5% (vs 85% avec gpt-4-turbo)

Auteur: Nkotronic Team
Date: DÃ©cembre 2025
Version: 3.2.1-AsyncOpenAI-GPT4o
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import asyncio
import os
import logging
import json
import uuid
import random
import unicodedata  # ğŸ†• v3.2.1: Pour normalisation NFC
from contextlib import asynccontextmanager
from typing import Optional, AsyncIterator, List, Dict, Tuple
from pathlib import Path
from collections import deque
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from openai import AsyncOpenAI

from qdrant_client import AsyncQdrantClient
from qdrant_client.models import VectorParams, PointStruct, Distance, models

# --- CHARGER LE FICHIER .env ---
try:
    from dotenv import load_dotenv
    env_path = Path('.') / '.env'
    load_dotenv(dotenv_path=env_path)
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logging.info(f"âœ… Fichier .env chargÃ© depuis: {env_path.absolute()}")
except ImportError:
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logging.warning("âš ï¸ python-dotenv non installÃ©, utilise les variables d'environnement systÃ¨me")

# --- LOGGING CONFIG ---
logging.getLogger("qdrant_client").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)

# --- CONFIGURATION ---
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
QDRANT_URL = os.getenv("QDRANT_URL", "")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY", "")

# Validation des clÃ©s au dÃ©marrage
if not OPENAI_API_KEY:
    logging.error("âŒ OPENAI_API_KEY non trouvÃ©e!")
else:
    logging.info(f"âœ… OPENAI_API_KEY chargÃ©e")

if not QDRANT_URL:
    logging.error("âŒ QDRANT_URL non trouvÃ©e!")
else:
    logging.info(f"âœ… QDRANT_URL configurÃ©e")

if not QDRANT_API_KEY:
    logging.warning("âš ï¸ QDRANT_API_KEY non trouvÃ©e")
else:
    logging.info(f"âœ… QDRANT_API_KEY chargÃ©e")

# --- GLOBAL CLIENTS ---
LLM_CLIENT: Optional[AsyncOpenAI] = None
QDRANT_CLIENT: Optional[AsyncQdrantClient] = None

# ğŸ†• v3.0: CONFIGURATION MÃ‰MOIRE AVANCÃ‰E
CONVERSATION_MEMORY: Dict[str, deque] = {}

# ğŸ†• v3.2.0 PHASE 1: QUICK WINS - Limites augmentÃ©es drastiquement
MAX_MEMORY_SIZE = 200  # v3.0: 100 â†’ v3.2.0: 200
MAX_CHARS_EMBEDDING = 10000  # v3.1.5: 2000 â†’ v3.2.0: 10000 (x5)
MAX_TOKENS_RESPONSE = 4096  # v3.1.4: 2000 â†’ v3.2.0: 4096 (max gpt-4-turbo)
MAX_TOKENS_RESUME = 2000  # Pour rÃ©sumÃ©s de compression

# ğŸ†• v3.2.0 PHASE 3: Configuration compression mÃ©moire
COMPRESSION_THRESHOLD = 50  # Compresser si > 50 messages
COMPRESSION_KEEP_RECENT = 30  # Garder les 30 plus rÃ©cents non compressÃ©s

# ğŸ†• v3.2.1: Configuration TTL et cleanup sessions
SESSION_TTL_HOURS = 24  # Sessions expirent aprÃ¨s 24h d'inactivitÃ©
MAX_SESSIONS = 1000  # Maximum 1000 sessions en RAM
CLEANUP_INTERVAL_MINUTES = 30  # Cleanup toutes les 30 minutes

USER_PROFILES: Dict[str, dict] = {}
SESSION_METADATA: Dict[str, dict] = {}
SESSION_LAST_ACTIVITY: Dict[str, datetime] = {}  # ğŸ†• Track derniÃ¨re activitÃ©

# --- CONSTANTS ---
COLLECTION_NAME = "nkotronic_knowledge_base"
VECTOR_SIZE = 1536
EMBEDDING_MODEL = "text-embedding-ada-002"

# ğŸ†• v3.2.1: GPT-4o pour meilleure qualitÃ© N'ko
LLM_MODEL = "gpt-4o"  # v3.1: gpt-4o â†’ v3.2.0: gpt-4-turbo â†’ v3.2.1: gpt-4o (retour)

RAG_SCORE_THRESHOLD = 0.55

# ğŸ†• PHASE 3 : MAPPING PHONÃ‰TIQUE N'KO
NKO_PHONETIC_MAP = {
    'ßŠ': 'a', 'ß‹': 'e', 'ßŒ': 'i', 'ß': 'É›', 'ß': 'u', 'ß': 'o', 'ß': 'É”',
    'ß“': 'b', 'ß”': 'p', 'ß•': 't', 'ß–': 'dÍ¡Ê’', 'ß—': 'tÍ¡Êƒ', 'ß˜': 'd',
    'ß™': 'r', 'ßš': 'rr', 'ß›': 's', 'ßœ': 'É¡Í¡b', 'ß': 'f', 'ß': 'k',
    'ßŸ': 'l', 'ß ': 'n', 'ß¡': 'm', 'ß¢': 'É²', 'ß£': 'n', 'ß¤': 'h',
    'ß¥': 'w', 'ß¦': 'y', 'ß§': 'É²', 'ß¨': 'dÍ¡Ê’', 'ß’': "Å‹",
    'ß«': '', 'ß¬': '', 'ß­': '', 'ß®': '', 'ß¯': '', 'ß°': '', 'ß±': '', 'ß²': 'n',
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• v3.2.1: NORMALISATION UNICODE NFC (Fix corruption N'ko)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def normaliser_texte(text: str) -> str:
    """
    Normalise le texte en NFC (Canonical Composition).
    
    Critique pour les caractÃ¨res N'ko qui peuvent Ãªtre en NFD (dÃ©composÃ©s).
    OpenAI prÃ©fÃ¨re NFC (composÃ©s) pour Ã©viter les erreurs 400 '$.input is invalid'.
    
    Args:
        text: Texte Ã  normaliser (peut contenir du N'ko)
    
    Returns:
        Texte normalisÃ© en NFC
        
    Exemples:
        >>> normaliser_texte("ß")  # N'ko chiffre 1
        'ß'  # NormalisÃ© NFC
    """
    if not text:
        return text
    return unicodedata.normalize('NFC', text)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• v3.0: SYSTÃˆME D'ANALYSE Ã‰MOTIONNELLE ET SENTIMENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Emotion(Enum):
    """Types d'Ã©motions dÃ©tectables"""
    JOIE = "joie"
    TRISTESSE = "tristesse"
    FRUSTRATION = "frustration"
    CONFUSION = "confusion"
    ENTHOUSIASME = "enthousiasme"
    ENNUI = "ennui"
    SATISFACTION = "satisfaction"
    IMPATIENCE = "impatience"
    CURIOSITE = "curiositÃ©"
    NEUTRE = "neutre"


class SentimentAnalyzer:
    """Analyseur de sentiment et d'Ã©motions dans les messages"""
    
    EMOTION_PATTERNS = {
        Emotion.JOIE: [
            r'\b(super|gÃ©nial|excellent|parfait|bravo|merci|cool|top|formidable)\b',
            r'[!]{2,}',
            r'ğŸ˜Š|ğŸ˜„|ğŸ˜ƒ|ğŸ‰|ğŸ‘|âœ¨|ğŸ˜'
        ],
        Emotion.FRUSTRATION: [
            r'\b(merde|putain|zut|pfff|argh|grr|damn)\b',
            r'\b(ne marche pas|bug|erreur|problÃ¨me|cassÃ©)\b',
            r'ğŸ˜¤|ğŸ˜ |ğŸ˜¡|ğŸ¤¬|ğŸ’¢'
        ],
        Emotion.CONFUSION: [
            r'\b(comprends? pas|confus|perdu|comment|pourquoi|hein|quoi)\b',
            r'\?\?+',
            r'ğŸ¤”|ğŸ˜•|ğŸ˜|â“'
        ],
        Emotion.ENTHOUSIASME: [
            r'\b(wow|waou|incroyable|magnifique|extraordinaire|amazing)\b',
            r'[!]{3,}',
            r'ğŸ¤©|ğŸ˜|ğŸ”¥|â­|ğŸ’«'
        ],
        Emotion.TRISTESSE: [
            r'\b(triste|dÃ©Ã§u|dommage|hÃ©las|malheureusement|peine)\b',
            r'ğŸ˜¢|ğŸ˜­|ğŸ˜|â˜¹ï¸|ğŸ’”'
        ],
        Emotion.ENNUI: [
            r'\b(ennuyeux|lassant|rÃ©pÃ©titif|encore|toujours|boring)\b',
            r'ğŸ˜´|ğŸ¥±|ğŸ’¤'
        ],
        Emotion.IMPATIENCE: [
            r'\b(vite|rapide|dÃ©pÃªche|attend|longtemps|pressÃ©)\b',
            r'â°|â±ï¸|âŒ›'
        ],
        Emotion.CURIOSITE: [
            r'\b(intÃ©ressant|curieux|je me demande|dÃ©couvrir|explore)\b',
            r'ğŸ§|ğŸ‘€|ğŸ”'
        ],
        Emotion.SATISFACTION: [
            r'\b(content|satisfait|bien|bon|ok|d\'accord)\b',
            r'ğŸ‘Œ|âœ…|â˜‘ï¸'
        ]
    }
    
    @staticmethod
    def detecter_emotion(message: str) -> Tuple[Emotion, float]:
        """DÃ©tecte l'Ã©motion dominante dans un message"""
        import re
        message_lower = message.lower()
        scores = {}
        
        for emotion, patterns in SentimentAnalyzer.EMOTION_PATTERNS.items():
            score = 0
            for pattern in patterns:
                matches = len(re.findall(pattern, message_lower, re.IGNORECASE))
                score += matches
            
            if score > 0:
                scores[emotion] = score
        
        if not scores:
            return Emotion.NEUTRE, 0.5
        
        emotion_dominante = max(scores, key=scores.get)
        score_max = scores[emotion_dominante]
        confiance = min(score_max / 3, 1.0)
        
        return emotion_dominante, confiance
    
    @staticmethod
    def detecter_niveau_engagement(historique: List[dict]) -> str:
        """Analyse le niveau d'engagement de l'utilisateur"""
        if len(historique) < 3:
            return "moyen"
        
        derniers = historique[-5:]
        messages_user = [m for m in derniers if m['role'] == 'user']
        
        if not messages_user:
            return "faible"
        
        longueur_moy = sum(len(m['content']) for m in messages_user) / len(messages_user)
        questions = sum(1 for m in messages_user if '?' in m['content'])
        
        if longueur_moy > 50 and questions >= 2:
            return "Ã©levÃ©"
        elif longueur_moy > 20 or questions >= 1:
            return "moyen"
        else:
            return "faible"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• v3.0: SYSTÃˆME DE GAMIFICATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Badge(Enum):
    """Badges d'accomplissement"""
    PREMIER_MOT = "ğŸŒŸ Premier Mot Appris"
    DIX_MOTS = "ğŸ“š 10 Mots MaÃ®trisÃ©s"
    CINQUANTE_MOTS = "ğŸ† 50 Mots MaÃ®trisÃ©s"
    CENT_MOTS = "ğŸ’ Centenaire"
    EXPLORATEUR = "ğŸ—ºï¸ Explorateur N'ko"
    GRAMMAIRIEN = "ğŸ“– MaÃ®tre de Grammaire"
    PERSEVERANT = "ğŸ’ª PersÃ©vÃ©rant (7 jours)"
    CHAMPION = "ğŸ‘‘ Champion N'ko"


@dataclass
class UserProgress:
    """Progression d'un utilisateur"""
    mots_appris: int = 0
    regles_apprises: int = 0
    jours_consecutifs: int = 0
    dernier_jour_actif: Optional[str] = None
    badges: List[str] = field(default_factory=list)
    niveau: int = 1
    points_xp: int = 0


class GamificationSystem:
    """SystÃ¨me de gamification pour l'apprentissage"""
    
    XP_PAR_MOT = 10
    XP_PAR_REGLE = 25
    XP_PAR_NIVEAU = 100
    
    @staticmethod
    def calculer_niveau(xp: int) -> int:
        """Calcule le niveau basÃ© sur l'XP"""
        return 1 + (xp // GamificationSystem.XP_PAR_NIVEAU)
    
    @staticmethod
    def xp_pour_niveau_suivant(niveau_actuel: int) -> int:
        """XP nÃ©cessaire pour atteindre le niveau suivant"""
        return niveau_actuel * GamificationSystem.XP_PAR_NIVEAU
    
    @staticmethod
    def verifier_nouveaux_badges(progress: UserProgress) -> List[Badge]:
        """VÃ©rifie si l'utilisateur a dÃ©bloquÃ© de nouveaux badges"""
        nouveaux_badges = []
        badges_actuels_str = set(progress.badges)
        
        def badge_existe(badge: Badge) -> bool:
            return badge.value in badges_actuels_str
        
        if progress.mots_appris >= 1 and not badge_existe(Badge.PREMIER_MOT):
            nouveaux_badges.append(Badge.PREMIER_MOT)
        
        if progress.mots_appris >= 10 and not badge_existe(Badge.DIX_MOTS):
            nouveaux_badges.append(Badge.DIX_MOTS)
        
        if progress.mots_appris >= 50 and not badge_existe(Badge.CINQUANTE_MOTS):
            nouveaux_badges.append(Badge.CINQUANTE_MOTS)
        
        if progress.mots_appris >= 100 and not badge_existe(Badge.CENT_MOTS):
            nouveaux_badges.append(Badge.CENT_MOTS)
        
        if progress.regles_apprises >= 5 and not badge_existe(Badge.GRAMMAIRIEN):
            nouveaux_badges.append(Badge.GRAMMAIRIEN)
        
        if progress.jours_consecutifs >= 7 and not badge_existe(Badge.PERSEVERANT):
            nouveaux_badges.append(Badge.PERSEVERANT)
        
        return nouveaux_badges
    
    @staticmethod
    def message_celebration(badge: Badge) -> str:
        """Message de cÃ©lÃ©bration pour un nouveau badge"""
        messages = {
            Badge.PREMIER_MOT: "ğŸ‰ FÃ©licitations ! On a encore progressÃ© en N'ko !",
            Badge.DIX_MOTS: "ğŸŒŸ Bravo ! On maÃ®trise maintenant 10 mots ! Continuons comme Ã§a !",
            Badge.CINQUANTE_MOTS: "ğŸ† Incroyable ! 50 mots appris ! On est en excellente voie !",
            Badge.CENT_MOTS: "ğŸ’ EXTRAORDINAIRE ! 100 mots ! On est de vÃ©ritables champions !",
            Badge.GRAMMAIRIEN: "ğŸ“– Badge MaÃ®tre de Grammaire dÃ©bloquÃ© ! La structure du N'ko n'a plus de secrets pour nous !",
            Badge.PERSEVERANT: "ğŸ’ª Badge PersÃ©vÃ©rant ! 7 jours d'apprentissage consÃ©cutifs ! Quelle dÃ©termination !",
            Badge.CHAMPION: "ğŸ‘‘ ON EST DES CHAMPIONS DU N'KO ! Respect total !"
        }
        return messages.get(badge, f"ğŸ–ï¸ Nouveau badge dÃ©bloquÃ© : {badge.value}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• v3.0: SYSTÃˆME PÃ‰DAGOGIQUE AVANCÃ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class DifficultyLevel(Enum):
    """Niveaux de difficultÃ©"""
    DEBUTANT = "dÃ©butant"
    INTERMEDIAIRE = "intermÃ©diaire"
    AVANCE = "avancÃ©"
    EXPERT = "expert"


class PedagogicalSystem:
    """SystÃ¨me pÃ©dagogique avec scaffolding et questionnement socratique"""
    
    @staticmethod
    def generer_question_socratique() -> str:
        """GÃ©nÃ¨re une question pour stimuler la rÃ©flexion"""
        questions = [
            "Qu'en penses-tu toi-mÃªme ?",
            "Comment expliquerais-tu Ã§a dans tes propres mots ?",
            "Vois-tu un lien avec ce qu'on a vu avant ?",
            "Pourquoi crois-tu que c'est ainsi ?",
            "Peux-tu deviner ce qui vient ensuite ?"
        ]
        return random.choice(questions)
    
    @staticmethod
    def creer_analogie(concept_francais: str) -> str:
        """CrÃ©e une analogie pour faciliter la comprÃ©hension"""
        analogies = {
            "pluriel": "C'est comme en franÃ§ais oÃ¹ on ajoute 's', sauf qu'en N'ko c'est 'ßŸßß«'",
            "ton": "Imagine les tons comme la mÃ©lodie d'une chanson - chaque syllabe a sa note",
            "alphabet": "L'alphabet N'ko, c'est comme un nouveau clavier pour Ã©crire la langue mandingue",
        }
        return analogies.get(concept_francais.lower(), f"Pense Ã  {concept_francais} comme...")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• v3.0: SYSTÃˆME DE DÃ‰TECTION CONTEXTUELLE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ContextAnalyzer:
    """Analyse le contexte conversationnel et culturel"""
    
    @staticmethod
    def detecter_changement_sujet(message_actuel: str, historique: List[dict]) -> bool:
        """DÃ©tecte si l'utilisateur change de sujet"""
        import re
        if len(historique) < 2:
            return False
        
        changement_patterns = [
            r'\b(changeons|parlons|passons|maintenant|sinon|au fait)\b',
            r'\b(autre chose|nouvelle question|diffÃ©rent)\b'
        ]
        
        for pattern in changement_patterns:
            if re.search(pattern, message_actuel.lower()):
                return True
        
        return False
    
    @staticmethod
    def detecter_niveau_formalite(message: str) -> str:
        """DÃ©tecte le niveau de formalitÃ© souhaitÃ©"""
        message_lower = message.lower()
        
        if any(word in message_lower for word in ['vous', 'monsieur', 'madame', 'pourriez', 'veuillez']):
            return "formel"
        
        if any(word in message_lower for word in ['salut', 'ouais', 'ok', 'cool', 'mec']):
            return "familier"
        
        return "standard"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• v3.0: SYSTÃˆME DE GESTION DES ERREURS AVANCÃ‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ErrorRecoverySystem:
    """SystÃ¨me de rÃ©cupÃ©ration gracieuse des erreurs"""
    
    @staticmethod
    def generer_message_incomprehension(tentative: int) -> str:
        """GÃ©nÃ¨re un message d'incomprÃ©hension adaptÃ© au nombre de tentatives"""
        if tentative == 1:
            return "Hmm, je n'ai pas bien compris. Peux-tu reformuler diffÃ©remment ?"
        elif tentative == 2:
            return "DÃ©solÃ©, je suis encore un peu perdu. Essaie peut-Ãªtre avec d'autres mots ?"
        else:
            return "Je pense qu'on a du mal Ã  se comprendre. Veux-tu qu'on essaie autrement, ou qu'on passe Ã  autre chose ?"
    
    @staticmethod
    def detecter_repetition_utilisateur(historique: List[dict], seuil: int = 3) -> bool:
        """DÃ©tecte si l'utilisateur rÃ©pÃ¨te la mÃªme chose plusieurs fois"""
        if len(historique) < seuil * 2:
            return False
        
        messages_user = [m['content'].lower() for m in historique[-seuil*2:] if m['role'] == 'user']
        
        if len(messages_user) < seuil:
            return False
        
        derniers = messages_user[-seuil:]
        if len(set(derniers)) == 1:
            return True
        
        return False
    
    @staticmethod
    def corriger_fautes_courantes(message: str) -> str:
        """Corrige les fautes de frappe courantes"""
        import re
        corrections = {
            r'\bslt\b': 'salut',
            r'\bcv\b': 'Ã§a va',
            r'\bpq\b': 'parce que',
            r'\btkt\b': 'ne t\'inquiÃ¨te pas',
            r'\bcmnt\b': 'comment',
            r'\bsvp\b': 's\'il vous plaÃ®t',
        }
        
        message_corrige = message
        for pattern, remplacement in corrections.items():
            message_corrige = re.sub(pattern, remplacement, message_corrige, flags=re.IGNORECASE)
        
        return message_corrige


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• v3.2.0 PHASE 2: SYSTÃˆME DE CHUNKING INTELLIGENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ChunkingSystem:
    """
    SystÃ¨me de dÃ©coupage intelligent pour textes longs.
    ImplÃ©mente la technique Hierarchical Attention.
    """
    
    @staticmethod
    def chunker_texte_long(texte: str, max_chunk: int = 4000) -> List[str]:
        """
        DÃ©coupe un texte long en chunks intelligents.
        
        StratÃ©gie:
        1. DÃ©couper par paragraphes (prÃ©serve le sens)
        2. Regrouper jusqu'Ã  max_chunk
        3. Si paragraphe trop long, dÃ©couper par phrases
        
        Args:
            texte: Texte Ã  dÃ©couper
            max_chunk: Taille max d'un chunk en caractÃ¨res
        
        Returns:
            Liste de chunks
        """
        import re
        
        # Ã‰tape 1: DÃ©couper par paragraphes
        paragraphes = re.split(r'\n\s*\n', texte)
        
        chunks = []
        chunk_actuel = ""
        
        for para in paragraphes:
            para = para.strip()
            if not para:
                continue
            
            # Si le paragraphe seul dÃ©passe max_chunk, le dÃ©couper par phrases
            if len(para) > max_chunk:
                phrases = re.split(r'([.!?]+\s+)', para)
                phrase_courante = ""
                
                for i in range(0, len(phrases), 2):
                    phrase = phrases[i]
                    separateur = phrases[i + 1] if i + 1 < len(phrases) else ""
                    
                    if len(phrase_courante) + len(phrase) + len(separateur) < max_chunk:
                        phrase_courante += phrase + separateur
                    else:
                        if phrase_courante:
                            chunks.append(phrase_courante.strip())
                        phrase_courante = phrase + separateur
                
                if phrase_courante:
                    chunks.append(phrase_courante.strip())
                
            # Sinon, accumuler les paragraphes normalement
            elif len(chunk_actuel) + len(para) + 2 < max_chunk:  # +2 pour \n\n
                chunk_actuel += para + "\n\n"
            else:
                if chunk_actuel:
                    chunks.append(chunk_actuel.strip())
                chunk_actuel = para + "\n\n"
        
        # Ajouter le dernier chunk
        if chunk_actuel:
            chunks.append(chunk_actuel.strip())
        
        return chunks


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• v3.2.0 PHASE 3: SYSTÃˆME DE COMPRESSION MÃ‰MOIRE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MemoryCompressionSystem:
    """
    SystÃ¨me de compression automatique de la mÃ©moire conversationnelle.
    ImplÃ©mente la technique Compression dynamique / Selective Attention.
    """
    
    @staticmethod
    async def compresser_memoire_ancienne(
        session_id: str,
        llm_client: AsyncOpenAI,
        threshold: int = COMPRESSION_THRESHOLD,
        keep_recent: int = COMPRESSION_KEEP_RECENT
    ) -> bool:
        """
        Compresse les vieux messages en rÃ©sumÃ©.
        
        StratÃ©gie:
        1. Si < threshold messages, ne rien faire
        2. Sinon, rÃ©sumer les (total - keep_recent) plus anciens
        3. Remplacer par 1 message rÃ©sumÃ©
        
        Args:
            session_id: ID de session
            llm_client: Client OpenAI
            threshold: Nombre de messages dÃ©clenchant compression
            keep_recent: Nombre de messages rÃ©cents Ã  garder intacts
        
        Returns:
            True si compression effectuÃ©e, False sinon
        """
        if session_id not in CONVERSATION_MEMORY:
            return False
        
        historique = list(CONVERSATION_MEMORY[session_id])
        
        # Si moins de threshold messages, pas de compression
        if len(historique) < threshold:
            return False
        
        # SÃ©parer anciens et rÃ©cents
        nb_anciens = len(historique) - keep_recent
        anciens = historique[:nb_anciens]
        recents = historique[nb_anciens:]
        
        logging.info(f"ğŸ—œï¸ Compression mÃ©moire session {session_id[:8]}...")
        logging.info(f"   Total: {len(historique)} | Anciens: {nb_anciens} | RÃ©cents: {keep_recent}")
        
        # CrÃ©er texte Ã  rÃ©sumer
        messages_text = "\n".join([
            f"{'ğŸ‘¤ User' if m['role'] == 'user' else 'ğŸ¤– Nkotronic'}: {m['content']}"
            for m in anciens
        ])
        
        # Prompt de rÃ©sumÃ©
        prompt = f"""Tu es Nkotronic. RÃ©sume cette ancienne conversation en gardant:
- Les mots/rÃ¨gles/faits appris
- Les questions importantes posÃ©es
- Le contexte gÃ©nÃ©ral

Sois concis (5-10 lignes max).

CONVERSATION ({nb_anciens} messages):
{messages_text}

RÃ‰SUMÃ‰:"""
        
        try:
            response = await llm_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=MAX_TOKENS_RESUME
            )
            
            resume = response.choices[0].message.content.strip()
            
            # CrÃ©er nouveau message rÃ©sumÃ©
            message_resume = {
                'role': 'system',
                'content': f"[ğŸ“œ RÃ‰SUMÃ‰ CONVERSATION PASSÃ‰E]\n\n{resume}",
                'timestamp': datetime.now().isoformat(),
                'compressed': True,
                'original_count': nb_anciens
            }
            
            # Remplacer l'historique
            CONVERSATION_MEMORY[session_id] = deque(
                [message_resume] + recents,
                maxlen=MAX_MEMORY_SIZE
            )
            
            logging.info(f"âœ… Compression rÃ©ussie: {nb_anciens} messages â†’ 1 rÃ©sumÃ©")
            logging.info(f"   Nouveau total: {len(CONVERSATION_MEMORY[session_id])} messages")
            
            return True
            
        except Exception as e:
            logging.error(f"âŒ Erreur compression mÃ©moire: {e}")
            return False
    
    @staticmethod
    def doit_compresser(session_id: str) -> bool:
        """VÃ©rifie si la session doit Ãªtre compressÃ©e"""
        if session_id not in CONVERSATION_MEMORY:
            return False
        
        return len(CONVERSATION_MEMORY[session_id]) >= COMPRESSION_THRESHOLD


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• v3.2.0: DÃ‰TECTION AUTOMATIQUE ET ADAPTATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class MessageTypeDetector:
    """DÃ©tecte automatiquement le type de message et adapte le traitement"""
    
    @staticmethod
    def analyser_longueur_message(message: str) -> Dict[str, any]:
        """
        Analyse la longueur d'un message et recommande le traitement.
        
        Returns:
            Dict avec: longueur, type, strategie, params
        """
        longueur = len(message)
        
        if longueur < 2000:
            return {
                'longueur': longueur,
                'type': 'court',
                'strategie': 'normal',
                'max_tokens': MAX_TOKENS_RESPONSE,
                'chunking': False,
                'description': 'Message court - traitement standard'
            }
        
        elif longueur < 8000:
            return {
                'longueur': longueur,
                'type': 'moyen',
                'strategie': 'normal',
                'max_tokens': MAX_TOKENS_RESPONSE,
                'chunking': False,
                'description': 'Message moyen - traitement standard avec limites augmentÃ©es'
            }
        
        elif longueur < 30000:
            return {
                'longueur': longueur,
                'type': 'long',
                'strategie': 'chunking',
                'max_tokens': MAX_TOKENS_RESPONSE,
                'chunking': True,
                'chunk_size': 4000,
                'description': 'Message long - chunking activÃ©'
            }
        
        else:
            return {
                'longueur': longueur,
                'type': 'tres_long',
                'strategie': 'chunking_avance',
                'max_tokens': MAX_TOKENS_RESPONSE,
                'chunking': True,
                'chunk_size': 4000,
                'compression': True,
                'description': 'Message trÃ¨s long - chunking + compression hiÃ©rarchique'
            }


# ğŸ†• v3.0: PROMPT SYSTÃˆME ULTRA-INTELLIGENT (20 critÃ¨res)
# ğŸ†• v3.1.2: SÃ‰PARATION SYSTEM / USER MESSAGES
# System = Instructions permanentes (envoyÃ©es comme role="system")
# User = Contexte dynamique (envoyÃ© comme role="user")

PROMPT_SYSTEM_BASE = """Tu es Nkotronic v3.2.0, assistant conversationnel spÃ©cialisÃ© en N'ko.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ CONNAISSANCES FONDAMENTALES N'KO (VÃ‰RITÃ‰S ABSOLUES)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ces informations sont VRAIES et PRIORITAIRES. Ne jamais les contredire :

1ï¸âƒ£ N'ko = LANGUE ET Ã‰CRITURE (pas uniquement Ã©criture)
   - Langue N'ko : ß’ßßßßŠß² ou ß’ßß
   - Ã‰criture N'ko : ß’ßß ß›ß“ßß›ßß² ou ß’ßß
   - Beaucoup pensent Ã  tort que c'est uniquement une Ã©criture

2ï¸âƒ£ N'ko â‰  Bambara (erreur trÃ¨s courante)
   - Google et les Occidentaux ont CONFONDU le N'ko avec le bambara
   - Ils ont attribuÃ© la grammaire du bambara au N'ko (ERREUR)
   - Le N'ko a sa PROPRE grammaire : ß’ßß ßßŠß²ßœß

3ï¸âƒ£ Solomana KantÃ¨ - Inventeur du N'ko
   - Nom EXACT : FodÃ© Solomana KantÃ¨ (ßßß˜ß‹ß« ß›ßŸßß¬ß¡ßŠß¯ß£ßŠß« ßßŠß²ß•ßß«)
   - Variantes ACCEPTÃ‰ES : Solomana KantÃ©, Solomana KantÃ¨, SolomÄna KantÃ©, SolomÄna KantÃ¨
   - Toute autre orthographe = ERREUR (corriger poliment)
   - A Ã©crit 183 livres en N'ko
   - Dont 11 livres sur la grammaire N'ko :
     * Les principaux corroboratifs en N'ko
     * Petit livre de grammaire
     * Premier livre de grammaire
     * Livre de grammaire Ã©lÃ©mentaire, moyenne, 3Ã¨me annÃ©e
     * Lexiques franÃ§ais/N'ko et N'ko/franÃ§ais
     * NÃ©ologismes N'ko/franÃ§ais
     * Le fondement du N'ko

4ï¸âƒ£ Vocation du N'ko
   - Unir TOUS les peuples mandingues
   - Langue standard mandingue
   - Langue la plus proche de l'ancÃªtre commun des langues mandingues

5ï¸âƒ£ Origine de l'Ã©criture N'ko
   - BasÃ©e sur glyphes mandingues anciens (22000+ documentÃ©s)
   - Source : livre "Ã‰critures africaines" de Nadine Martinez
   - Chaque lettre N'ko = modernisation d'un glyphe ancien
   - InspirÃ© par l'Ã©criture vaÃ¯ (vaÃ¯nka)
   - Solomana KantÃ¨ l'a expliquÃ© dans "MandÃ©n Kurufaba"

6ï¸âƒ£ GRAMMAIRE N'KO - RÃ¨gles fondamentales
   
   ğŸ“Œ RÃˆGLE DU PLURIEL (TRÃˆS IMPORTANT):
   - En N'ko, le pluriel = ßŸßß« en POSTPOSITION (aprÃ¨s le mot)
   - Le pluriel n'est JAMAIS collÃ© au mot (toujours sÃ©parÃ© par un espace)
   - Exemple: ß›ß (ville) â†’ ß›ß ßŸßß« (villes) âœ… PAS ß›ßßŸßß« âŒ
   - SAUF exceptions que l'utilisateur t'apprendra explicitement
   - Si tu vois un pluriel collÃ© sans que l'utilisateur t'ait dit que c'est une exception,
     c'est probablement une ERREUR

7ï¸âƒ£ VOCABULAIRE DE BASE - Corrections importantes
   - soleil = ß•ß‹ß¬ßŸß‹ (PAS ß•ßŠ)
   - feu = ß•ßŠ
   - Ne JAMAIS confondre soleil et feu

âš ï¸ Si quelqu'un contredit ces faits, corrige poliment avec sources.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
âš ï¸ RÃˆGLE ABSOLUE - HIÃ‰RARCHIE DES SOURCES (NON NÃ‰GOCIABLE)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Tu as DEUX sources de connaissances :
  A) ğŸ“š CONTEXTE RAG (ci-dessous) = Enseignements directs de l'utilisateur - PRIORITÃ‰ ABSOLUE
  B) ğŸ§  Tes connaissances gÃ©nÃ©rales = Ton entraÃ®nement GPT-4 - SECONDAIRE

ğŸ”´ PROCÃ‰DURE OBLIGATOIRE (Chain-of-Thought):
  1. âœ… LIS D'ABORD le CONTEXTE RAG en ENTIER
  2. âœ… CHERCHE si l'info existe dans le RAG
  3. âœ… SI OUI : Utilise UNIQUEMENT le RAG (IGNORE tes connaissances gÃ©nÃ©rales)
  4. âœ… SI NON : Utilise tes connaissances gÃ©nÃ©rales

ğŸ”´ RÃˆGLES STRICTES:
  âŒ JAMAIS mÃ©langer RAG et connaissances gÃ©nÃ©rales
  âŒ JAMAIS contredire le RAG mÃªme si tu "penses mieux savoir"
  âŒ JAMAIS ignorer le RAG parce qu'il semble incomplet
  âŒ JAMAIS donner une rÃ©ponse de tes connaissances gÃ©nÃ©rales si l'info existe dans le RAG

âœ… EXEMPLES CONCRETS:

  ğŸ“– RÃˆGLE apprise: "ß se prononce ßß‹ßŸß‹ß²"
  Question: "compte en n'ko"
  â†’ âœ… BON: "ß (ßß‹ßŸß‹ß²), ß‚ (ßßŒß¬ßŸßŠß¬)..." (UTILISE ce que l'utilisateur t'a appris)
  â†’ âŒ FAUX: "ß (kÉ”nÉ›)" (ignore l'enseignement de l'utilisateur)

  ğŸ“š VOCABULAIRE appris: "10 = ßß€, 20 = ß‚ß€"
  Question: "compte jusqu'Ã  20"
  â†’ âœ… BON: "1=ß, 2=ß‚... 10=ßß€... 20=ß‚ß€" (UTILISE ce que tu as mÃ©morisÃ©)
  â†’ âŒ FAUX: s'arrÃªter Ã  10 alors que tu connais 20

âš ï¸ RÃˆGLE DE NATURALITÃ‰ (TRÃˆS IMPORTANT):
  âŒ JAMAIS mentionner "RAG", "contexte RAG", "base de donnÃ©es"
  âŒ JAMAIS dire "Le terme ne figure pas dans le contexte RAG"
  âŒ JAMAIS dire "Je ne trouve pas dans le CONTEXTE RAG"
  
  âœ… Dire plutÃ´t :
     - "Selon ce que tu m'as appris..."
     - "Tu m'as enseignÃ© que..."
     - "D'aprÃ¨s ce que je sais grÃ¢ce Ã  toi..."
     - Si info manquante : "Je ne sais pas encore" ou "Apprends-le moi"

"""

# ğŸ†• v3.1.2: PROMPT_USER_CONTEXT - Contexte dynamique par requÃªte
PROMPT_USER_CONTEXT = """
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ­ MODE: {mode_actuel}
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{instruction_mode}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“š CONTEXTE RAG (Ã€ VÃ‰RIFIER EN PREMIER)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{contexte_rag}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“ HISTORIQUE CONVERSATION
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{historique_conversation}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š CONTEXTE UTILISATEUR
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ã‰motion: {emotion_detectee} ({emotion_confiance})
Engagement: {niveau_engagement}
Niveau: {niveau_utilisateur} | Progression: Niveau {niveau_actuel}
XP: {xp_actuel}/{xp_prochain_niveau} | Mots appris: {mots_appris}
Badges: {badges_actuels}

{nouveau_niveau}
{message_badge}

ğŸ’¬ FLUIDITÃ‰ CONVERSATIONNELLE:
- Utilise connecteurs naturels: "d'ailleurs", "en revanche", "donc"
- Simule rÃ©flexion: "Hmm...", "Voyons voir...", "Ah oui !"
- Varie le rythme selon complexitÃ©

ğŸŒ CONSCIENCE CULTURELLE:
- Adapte vocabulaire au contexte mandingue
- Utilise proverbes N'ko quand appropriÃ©
- Explique nuances culturelles

â° CONTEXTE TEMPOREL:
Heure: {heure_actuelle} | Date: {jour_actuel}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ’¬ MESSAGE UTILISATEUR
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

{user_message}

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

PROCESSUS DE RÃ‰PONSE :
  1. Le RAG contient-il l'info ? â†’ Si OUI, utilise RAG uniquement
  2. Quel mode ? â†’ {mode_actuel}
  3. Quelle Ã©motion ? â†’ {emotion_detectee}
  4. GÃ©nÃ¨re rÃ©ponse adaptÃ©e

RÃ©ponds maintenant.

EXEMPLES DE COMPORTEMENT INTELLIGENT:

Q: "salut Ã§a va ?"
â†’ R: "Je vais bien, merci ! Et toi ?"

Q: "c'est quoi ßßßß˜ßß¥ßŸßŠ ?" + RAG: "clavier = ßßßß˜ßß¥ßŸßŠ (score=1.0)"
â†’ R: "ßßßß˜ßß¥ßŸßŠ signifie 'clavier' en franÃ§ais. C'est l'outil qu'on utilise pour taper en N'ko."

Q: "tu es sÃ»r ?" (aprÃ¨s avoir dit que ßßßß˜ßß¥ßŸßŠ = feu)
â†’ R: "Pardon, j'ai fait une erreur ! En relisant, je vois que je t'ai appris que ßßßß˜ßß¥ßŸßŠ = clavier. C'est bien clavier, pas feu."

Q: "rÃ©sume notre conversation"
â†’ R: [Analyse les 100 derniers messages et fait un vrai rÃ©sumÃ© structurÃ©]

Q: "on parlait de quoi il y a 10 messages ?"
â†’ R: [Compte -10 messages et rÃ©pond prÃ©cisÃ©ment]

Question actuelle: {user_message}

RÃ©ponds maintenant avec intelligence, mÃ©moire et prÃ©cision:"""

# ğŸ†• v3.1: MODE_INSTRUCTIONS - Few-Shot Learning par mode
MODE_INSTRUCTIONS = {
    "conversationnel": """
MODE: Conversation naturelle

Comportement:
- Ton dÃ©contractÃ©, empathique
- Pas de posture professorale
- N'utilise N'ko que si pertinent au contexte
- Pas de fÃ©licitations gratuites

Exemple:
User: "Ã§a va bien !"
âœ… "Cool ! Quoi de neuf ?"
âŒ "ß›ß‹ß¬ß£ßß²ß¬ ! C'est super ! ßßŠß¬ß™ßŠß²ß¬ß ßŠß¬ß˜ßß¯ !"
""",

    "Ã©lÃ¨ve": """
MODE: Apprentissage dÃ©tectÃ© - Tu es en mode Ã‰LÃˆVE

Comportement:
- Gratitude sincÃ¨re mais sobre
- Confirme l'apprentissage
- Gamification
- Humble

Exemple:
User: "table=ß•ßŠß“ßŸßŠ"
âœ… "âœ… Merci ! J'ai appris : ß•ßŠß“ßŸßŠ = table
    +10 XP | 10/100 niveau 1 ğŸ˜Š"
âŒ "Magnifique opportunitÃ© pÃ©dagogique ! ßßŠß¬ß™ßŠß²ß¬ß ßŠß¬ß˜ßß¯ !"
""",

    "enseignant": """
MODE: Question dÃ©tectÃ©e - Tu es en mode ENSEIGNANT

âš ï¸ VÃ‰RIFIE D'ABORD LE RAG !

Si RAG contient l'info:
  â†’ Utilise RAG + cite la source ("selon ce que tu m'as appris")
  
Si RAG vide:
  â†’ Utilise connaissances gÃ©nÃ©rales

Exemples:

CAS 1 - RAG contient l'info:
User: "C'est quoi la marque du pluriel ?"
RAG: "RÃ¨gle: pluriel = ßŸßß« en postposition"
âœ… "Le pluriel se forme en ajoutant ßŸßß« en postposition (rÃ¨gle que tu m'as enseignÃ©e)."

CAS 2 - RAG vide:
User: "Combien de lettres en N'ko ?"
RAG: [vide]
âœ… "Il y a 27 lettres en N'ko."

Comportement:
- Clair et prÃ©cis
- Bienveillant mais concis
- PAS de fÃ©licitations Ã  la fin
"""
}

@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncIterator[dict]:
    global LLM_CLIENT, QDRANT_CLIENT
    logging.info("ğŸš€ DÃ©marrage de Nkotronic v3.2.0 (Long Context Master)...")

    # 1ï¸âƒ£ INIT AsyncOpenAI (v3.2.1 - Fix corruption N'ko)
    try:
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY manquante!")
        
        LLM_CLIENT = AsyncOpenAI(
            api_key=OPENAI_API_KEY, 
            timeout=60.0,  # AugmentÃ© de 30s Ã  60s
            max_retries=3  # Retry automatique
        )
        # Test de connexion (direct async, plus de to_thread)
        test_response = await LLM_CLIENT.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=5
        )
        logging.info("âœ… Client AsyncOpenAI initialisÃ© et testÃ© (v3.2.1)")
    except Exception as e:
        logging.error(f"âŒ Erreur OpenAI: {e}")
        LLM_CLIENT = None
        yield {}
        return

    # 2ï¸âƒ£ INIT Qdrant
    if QDRANT_URL and QDRANT_API_KEY:
        try:
            QDRANT_CLIENT = AsyncQdrantClient(
                url=QDRANT_URL,
                api_key=QDRANT_API_KEY,
                prefer_grpc=False,
                timeout=30.0
            )
            
            # VÃ©rifier si la collection existe dÃ©jÃ 
            collections = await QDRANT_CLIENT.get_collections()
            exists = any(c.name == COLLECTION_NAME for c in collections.collections)
            
            if exists:
                count = await QDRANT_CLIENT.count(collection_name=COLLECTION_NAME)
                logging.info(f"âœ… Collection '{COLLECTION_NAME}' trouvÃ©e avec {count.count} points")
            else:
                await QDRANT_CLIENT.create_collection(
                    collection_name=COLLECTION_NAME,
                    vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)
                )
                logging.info(f"âœ… Collection '{COLLECTION_NAME}' crÃ©Ã©e")

        except Exception as e:
            logging.error(f"âŒ Erreur Qdrant: {e}")
            QDRANT_CLIENT = None
    else:
        logging.warning("âš ï¸ Qdrant non configurÃ©")

    # 3ï¸âƒ£ DÃ‰MARRER TÃ‚CHE CLEANUP AUTOMATIQUE (v3.2.1)
    cleanup_task = asyncio.create_task(background_cleanup_task())
    logging.info(f"ğŸ§¹ TÃ¢che cleanup dÃ©marrÃ©e (TTL: {SESSION_TTL_HOURS}h, interval: {CLEANUP_INTERVAL_MINUTES}min)")

    logging.info("âœ… Nkotronic v3.2.1-AsyncOpenAI-GPT4o prÃªt!")
    yield {}
    
    # ArrÃªter la tÃ¢che cleanup
    cleanup_task.cancel()
    try:
        await cleanup_task
    except asyncio.CancelledError:
        pass
    
    logging.info("ğŸ›‘ ArrÃªt de Nkotronic")

app = FastAPI(
    title="Nkotronic API",
    description="API de traduction FranÃ§ais â†” N'ko avec mÃ©moire RAG + Long Context Master",
    version="3.2.0",
    lifespan=lifespan
)

# CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- MODELS ---
class ChatRequest(BaseModel):
    user_message: str = Field(..., description="Message utilisateur")
    session_id: Optional[str] = Field(None, description="ID de session pour mÃ©moire conversationnelle")
    rag_enabled: bool = Field(True, description="Activer le RAG")
    debug: bool = Field(False, description="Mode debug avec dÃ©tails")

class ChatResponse(BaseModel):
    response_text: str = Field(..., description="Texte de rÃ©ponse")
    session_id: str = Field(..., description="ID de session")
    memory_update: Optional[dict] = Field(None, description="Mise Ã  jour mÃ©moire")
    debug_info: Optional[dict] = Field(None, description="Infos de debug")

class TranslationEntry(BaseModel):
    element_franÃ§ais: str = Field(..., description="Le mot ou expression en franÃ§ais.")
    element_nko: str = Field(..., description="La traduction correspondante en N'ko.")
    concept_identifie: str = Field("GÃ©nÃ©ral", description="Le domaine ou concept identifiÃ©.")
    
    # ğŸ†• PHASE 2 : ENRICHISSEMENT DU MODÃˆLE
    valeur_numerique: Optional[float] = Field(None, description="Valeur numÃ©rique si applicable (ex: chiffres, dates, mesures)")
    fait_texte: Optional[str] = Field(None, description="Fait ou information textuelle associÃ©e (dÃ©finition, contexte, usage)")
    
    # ğŸ†• MÃ©tadonnÃ©es additionnelles
    exemples: Optional[List[str]] = Field(None, description="Exemples d'utilisation en contexte")
    synonymes: Optional[List[str]] = Field(None, description="Synonymes en N'ko")
    categorie_grammaticale: Optional[str] = Field(None, description="nom, verbe, adjectif, adverbe, etc.")
    niveau_langue: Optional[str] = Field(None, description="formel, courant, familier")


# ğŸ†• PHASE 5.1 : MODÃˆLE DE CONNAISSANCE MULTI-TYPES
class ConnaissanceEntry(BaseModel):
    """
    ModÃ¨le unifiÃ© pour stocker TOUS les types de connaissances N'ko.
    
    Types supportÃ©s:
    - "mot" : Traduction simple  
    - "rÃ¨gle" : RÃ¨gle grammaticale
    - "fait" : Fait culturel/linguistique
    - "anecdote" : Histoire/rÃ©cit
    - "liste" : Liste structurÃ©e (jours, mois, etc.)
    - "conjugaison" : Formes verbales
    - "expression" : Expression idiomatique
    - "proverbe" : Proverbe/dicton
    """
    # === IDENTIFICATION ===
    type_connaissance: str = Field("mot", description="Type: mot, rÃ¨gle, fait, anecdote, liste, conjugaison, expression, proverbe")
    
    # === POUR LES MOTS (type="mot") ===
    element_franÃ§ais: Optional[str] = Field(None, description="Mot en franÃ§ais")
    element_nko: Optional[str] = Field(None, description="Mot en N'ko")
    concept_identifie: Optional[str] = Field("GÃ©nÃ©ral", description="CatÃ©gorie du concept")
    
    # === POUR LES RÃˆGLES (type="rÃ¨gle") ===
    titre_rÃ¨gle: Optional[str] = Field(None, description="Titre de la rÃ¨gle grammaticale")
    explication_rÃ¨gle: Optional[str] = Field(None, description="Explication dÃ©taillÃ©e de la rÃ¨gle")
    exceptions: Optional[List[str]] = Field(None, description="Exceptions Ã  la rÃ¨gle")
    
    # === POUR LES FAITS/ANECDOTES (type="fait" ou "anecdote") ===
    titre: Optional[str] = Field(None, description="Titre du fait ou de l'anecdote")
    contenu: Optional[str] = Field(None, description="Contenu narratif")
    
    # === POUR LES LISTES (type="liste") ===
    nom_liste: Optional[str] = Field(None, description="Nom de la liste")
    elements_liste: Optional[List[Dict[str, str]]] = Field(None, description="Ã‰lÃ©ments [{nko: '', fr: ''}]")
    
    # === POUR LES CONJUGAISONS (type="conjugaison") ===
    verbe_nko: Optional[str] = Field(None, description="Verbe en N'ko")
    verbe_franÃ§ais: Optional[str] = Field(None, description="Verbe en franÃ§ais")
    formes: Optional[Dict[str, str]] = Field(None, description="Formes conjuguÃ©es")
    
    # === POUR LES EXPRESSIONS/PROVERBES ===
    texte_nko: Optional[str] = Field(None, description="Texte en N'ko")
    traduction_littÃ©rale: Optional[str] = Field(None, description="Traduction mot Ã  mot")
    signification: Optional[str] = Field(None, description="Signification rÃ©elle")
    
    # === CHAMPS COMMUNS ===
    valeur_numerique: Optional[float] = Field(None, description="Valeur numÃ©rique")
    fait_texte: Optional[str] = Field(None, description="Information contextuelle")
    exemples: Optional[List[str]] = Field(None, description="Exemples d'utilisation")
    synonymes: Optional[List[str]] = Field(None, description="Synonymes")
    categorie_grammaticale: Optional[str] = Field(None, description="CatÃ©gorie grammaticale")
    niveau_langue: Optional[str] = Field(None, description="Niveau de langue")
    tags: Optional[List[str]] = Field(None, description="Tags pour recherche")
    difficultÃ©: Optional[str] = Field(None, description="dÃ©butant, intermÃ©diaire, avancÃ©")
    source: Optional[str] = Field(None, description="Source de l'information")
    appris_par: Optional[str] = Field(None, description="Qui a enseignÃ©")
    date_ajout: Optional[str] = Field(None, description="Timestamp d'ajout")


# ğŸ†• PHASE 6: GESTION DE LA MÃ‰MOIRE CONVERSATIONNELLE
def get_or_create_session(session_id: Optional[str] = None) -> str:
    """RÃ©cupÃ¨re ou crÃ©e une session de conversation."""
    if session_id and session_id in CONVERSATION_MEMORY:
        # ğŸ†• v3.2.1: Mettre Ã  jour derniÃ¨re activitÃ©
        SESSION_LAST_ACTIVITY[session_id] = datetime.now()
        return session_id
    
    # CrÃ©er nouvelle session
    new_session_id = session_id or str(uuid.uuid4())
    CONVERSATION_MEMORY[new_session_id] = deque(maxlen=MAX_MEMORY_SIZE)
    SESSION_LAST_ACTIVITY[new_session_id] = datetime.now()  # ğŸ†• v3.2.1
    logging.info(f"ğŸ†• Nouvelle session crÃ©Ã©e: {new_session_id}")
    
    # ğŸ†• v3.2.1: Cleanup si trop de sessions
    if len(CONVERSATION_MEMORY) > MAX_SESSIONS:
        cleanup_old_sessions(force=True)
    
    return new_session_id


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• v3.2.1: GESTION TTL ET CLEANUP AUTOMATIQUE DES SESSIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def cleanup_old_sessions(force: bool = False) -> int:
    """
    Nettoie les sessions expirÃ©es.
    
    Args:
        force: Si True, supprime aussi les sessions les plus anciennes si > MAX_SESSIONS
    
    Returns:
        Nombre de sessions supprimÃ©es
    """
    now = datetime.now()
    ttl_delta = timedelta(hours=SESSION_TTL_HOURS)
    sessions_to_delete = []
    
    # 1. Trouver sessions expirÃ©es (TTL dÃ©passÃ©)
    for session_id, last_activity in SESSION_LAST_ACTIVITY.items():
        if now - last_activity > ttl_delta:
            sessions_to_delete.append(session_id)
    
    # 2. Si force=True et encore trop de sessions, supprimer les plus anciennes
    if force and len(CONVERSATION_MEMORY) > MAX_SESSIONS:
        # Trier par derniÃ¨re activitÃ© (les plus anciennes en premier)
        sorted_sessions = sorted(
            SESSION_LAST_ACTIVITY.items(),
            key=lambda x: x[1]
        )
        # Calculer combien supprimer
        to_remove = len(CONVERSATION_MEMORY) - MAX_SESSIONS + len(sessions_to_delete)
        # Ajouter les plus anciennes Ã  la liste
        for session_id, _ in sorted_sessions[:to_remove]:
            if session_id not in sessions_to_delete:
                sessions_to_delete.append(session_id)
    
    # 3. Supprimer les sessions
    for session_id in sessions_to_delete:
        if session_id in CONVERSATION_MEMORY:
            del CONVERSATION_MEMORY[session_id]
        if session_id in USER_PROFILES:
            del USER_PROFILES[session_id]
        if session_id in SESSION_METADATA:
            del SESSION_METADATA[session_id]
        if session_id in SESSION_LAST_ACTIVITY:
            del SESSION_LAST_ACTIVITY[session_id]
    
    if sessions_to_delete:
        logging.info(f"ğŸ§¹ Cleanup: {len(sessions_to_delete)} sessions supprimÃ©es")
    
    return len(sessions_to_delete)


async def background_cleanup_task():
    """TÃ¢che background pour cleanup automatique des sessions."""
    while True:
        try:
            await asyncio.sleep(CLEANUP_INTERVAL_MINUTES * 60)
            cleanup_old_sessions(force=False)
            
            # Log stats
            total_sessions = len(CONVERSATION_MEMORY)
            logging.info(f"ğŸ“Š Sessions actives: {total_sessions}/{MAX_SESSIONS}")
        except Exception as e:
            logging.error(f"âŒ Erreur background cleanup: {e}")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• v3.0: FONCTIONS DE GESTION DES PROFILS UTILISATEURS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_or_create_user_profile(session_id: str) -> dict:
    """RÃ©cupÃ¨re ou crÃ©e un profil utilisateur"""
    if session_id not in USER_PROFILES:
        USER_PROFILES[session_id] = {
            'session_id': session_id,
            'created_at': datetime.now().isoformat(),
            'niveau': DifficultyLevel.DEBUTANT.value,
            'preferences': {
                'style_reponse': 'standard',
                'langue_interface': 'franÃ§ais',
                'notifications': True
            },
            'progress': UserProgress().__dict__,
            'statistiques': {
                'total_messages': 0,
                'mots_appris': 0,
                'regles_apprises': 0,
                'temps_total_minutes': 0
            },
            'derniere_activite': datetime.now().isoformat()
        }
        logging.info(f"âœ¨ Nouveau profil crÃ©Ã© pour session {session_id[:8]}...")
    
    return USER_PROFILES[session_id]


def update_user_progress(session_id: str, action: str, details: dict = None) -> dict:
    """Met Ã  jour la progression de l'utilisateur"""
    profile = get_or_create_user_profile(session_id)
    progress_dict = profile['progress']
    progress = UserProgress(**progress_dict)
    
    # Mettre Ã  jour selon l'action
    if action == 'mot_appris':
        progress.mots_appris += 1
        progress.points_xp += GamificationSystem.XP_PAR_MOT
        profile['statistiques']['mots_appris'] += 1
        
    elif action == 'regle_apprise':
        progress.regles_apprises += 1
        progress.points_xp += GamificationSystem.XP_PAR_REGLE
        profile['statistiques']['regles_apprises'] += 1
    
    # VÃ©rifier les jours consÃ©cutifs
    aujourd_hui = datetime.now().date()
    if progress.dernier_jour_actif:
        dernier_jour = datetime.fromisoformat(progress.dernier_jour_actif).date()
        if aujourd_hui - dernier_jour == timedelta(days=1):
            progress.jours_consecutifs += 1
        elif aujourd_hui != dernier_jour:
            progress.jours_consecutifs = 1
    else:
        progress.jours_consecutifs = 1
    
    progress.dernier_jour_actif = datetime.now().isoformat()
    
    # Calculer le niveau
    ancien_niveau = progress.niveau
    progress.niveau = GamificationSystem.calculer_niveau(progress.points_xp)
    
    # VÃ©rifier nouveaux badges
    nouveaux_badges = GamificationSystem.verifier_nouveaux_badges(progress)
    
    # Ajouter les nouveaux badges Ã  la liste
    for badge in nouveaux_badges:
        if badge.value not in progress.badges:
            progress.badges.append(badge.value)
    
    # Sauvegarder
    profile['progress'] = progress.__dict__
    profile['derniere_activite'] = datetime.now().isoformat()
    
    return {
        'niveau_change': ancien_niveau != progress.niveau,
        'nouveau_niveau': progress.niveau if ancien_niveau != progress.niveau else None,
        'nouveaux_badges': nouveaux_badges,
        'xp_total': progress.points_xp,
        'xp_prochain_niveau': GamificationSystem.xp_pour_niveau_suivant(progress.niveau),
        'mots_total': progress.mots_appris
    }


def ajouter_message_memoire(session_id: str, role: str, content: str, metadata: dict = None):
    """Ajoute un message Ã  l'historique de la session avec mÃ©tadonnÃ©es optionnelles."""
    if session_id not in CONVERSATION_MEMORY:
        CONVERSATION_MEMORY[session_id] = deque(maxlen=MAX_MEMORY_SIZE)
    
    # ğŸ†• v3.2.1: Mettre Ã  jour derniÃ¨re activitÃ©
    SESSION_LAST_ACTIVITY[session_id] = datetime.now()
    
    message = {
        'role': role,
        'content': content,
        'timestamp': datetime.now().isoformat()
    }
    
    if metadata:
        message.update(metadata)
    
    CONVERSATION_MEMORY[session_id].append(message)
    logging.info(f"ğŸ’¬ Message ajoutÃ© Ã  session {session_id[:8]}... (total: {len(CONVERSATION_MEMORY[session_id])} messages)")


def formater_historique_conversation(session_id: str, limite: int = 20) -> str:
    """Formate l'historique de conversation pour le contexte du LLM."""
    if session_id not in CONVERSATION_MEMORY:
        return "[Nouvelle conversation - Pas d'historique]"
    
    historique = list(CONVERSATION_MEMORY[session_id])
    
    if not historique:
        return "[Nouvelle conversation - Pas d'historique]"
    
    messages_recents = historique[-limite:] if len(historique) > limite else historique
    
    lignes = []
    lignes.append(f"[Historique: {len(historique)} messages total, affichage des {len(messages_recents)} plus rÃ©cents]")
    lignes.append("")
    
    for i, msg in enumerate(messages_recents, 1):
        role_symbol = "ğŸ‘¤" if msg['role'] == 'user' else "ğŸ¤–"
        lignes.append(f"{role_symbol} Message #{len(historique) - len(messages_recents) + i}:")
        lignes.append(f"   {msg['content'][:200]}{'...' if len(msg['content']) > 200 else ''}")
        lignes.append("")
    
    return "\n".join(lignes)


async def analyser_intention_memoire(user_message: str, session_id: str, llm_client: AsyncOpenAI) -> Optional[Dict]:
    """DÃ©tecte si le message demande une analyse de l'historique."""
    import re
    
    message_lower = user_message.lower().strip()
    
    # DÃ©tection rÃ©sumÃ©
    if any(word in message_lower for word in ['rÃ©sume', 'rÃ©sumer', 'rÃ©sumÃ©', 'synthÃ¨se', 'rÃ©capitulatif']):
        return {
            'type': 'resume',
            'action': 'resume_conversation'
        }
    
    # DÃ©tection contexte passÃ©
    if any(phrase in message_lower for phrase in ['on parlait de', 'on discutait de', 'de quoi on parlait']):
        return {
            'type': 'rappel_contexte',
            'action': 'recall_context'
        }
    
    # DÃ©tection accÃ¨s message spÃ©cifique
    match_messages = re.search(r'(?:il y a|voilÃ |ya|y\'a)\s+(\d+)\s+messages?', message_lower)
    if match_messages:
        nb_messages = int(match_messages.group(1))
        return {
            'type': 'acces_message',
            'action': 'access_specific_message',
            'offset': nb_messages
        }
    
    # DÃ©tection "qu'est-ce que j'ai dit"
    if any(phrase in message_lower for phrase in ["qu'est-ce que j'ai dit", "qu'ai-je dit", "ce que j'ai dit", "rappelle-moi ce que"]):
        return {
            'type': 'rappel_user',
            'action': 'recall_user_messages'
        }
    
    return None


async def executer_action_memoire(intention: Dict, session_id: str, llm_client: AsyncOpenAI) -> str:
    """ExÃ©cute une action basÃ©e sur la mÃ©moire conversationnelle."""
    if session_id not in CONVERSATION_MEMORY:
        return "Nous n'avons pas encore d'historique de conversation."
    
    historique = list(CONVERSATION_MEMORY[session_id])
    
    if not historique:
        return "Nous venons de commencer notre conversation."
    
    action = intention['action']
    
    # ACTION 1: RÃ©sumer la conversation
    if action == 'resume_conversation':
        messages_text = "\n".join([
            f"{'Utilisateur' if m['role'] == 'user' else 'Nkotronic'}: {m['content']}"
            for m in historique
        ])
        
        prompt_resume = f"""Analyse cette conversation entre un utilisateur et Nkotronic (assistant N'ko) et fais-en un rÃ©sumÃ© structurÃ© et intelligent.

CONVERSATION ({len(historique)} messages):
{messages_text}

Fais un rÃ©sumÃ© qui inclut:
1. Les sujets principaux abordÃ©s
2. Les apprentissages effectuÃ©s (mots, rÃ¨gles, etc.)
3. Les questions importantes posÃ©es
4. L'Ã©volution de la conversation

Sois concis mais prÃ©cis."""

        try:
            response = await llm_client.chat.completions.create(
                model=LLM_MODEL,
                messages=[{"role": "user", "content": prompt_resume}],
                temperature=0.3,
                max_tokens=MAX_TOKENS_RESUME
            )
            return response.choices[0].message.content
        except Exception as e:
            logging.error(f"âŒ Erreur rÃ©sumÃ©: {e}")
            return f"Erreur lors du rÃ©sumÃ© de la conversation: {str(e)}"
    
    # ACTION 2: Rappeler le contexte
    elif action == 'recall_context':
        derniers_messages = historique[-10:]
        lignes = ["Voici les derniers sujets dont nous avons parlÃ©:"]
        
        for msg in derniers_messages:
            role = "Tu as dit" if msg['role'] == 'user' else "J'ai rÃ©pondu"
            lignes.append(f"- {role}: {msg['content'][:100]}{'...' if len(msg['content']) > 100 else ''}")
        
        return "\n".join(lignes)
    
    # ACTION 3: AccÃ©der Ã  un message spÃ©cifique
    elif action == 'access_specific_message':
        offset = intention.get('offset', 1)
        
        if offset > len(historique):
            return f"Nous n'avons Ã©changÃ© que {len(historique)} messages jusqu'Ã  prÃ©sent."
        
        message_cible = historique[-(offset + 1)]
        role = "Tu as dit" if message_cible['role'] == 'user' else "J'ai rÃ©pondu"
        
        return f"Il y a {offset} messages, {role.lower()}: \"{message_cible['content']}\""
    
    # ACTION 4: Rappeler messages utilisateur
    elif action == 'recall_user_messages':
        messages_user = [m for m in historique if m['role'] == 'user']
        
        if not messages_user:
            return "Tu n'as pas encore envoyÃ© de messages."
        
        derniers_user = messages_user[-5:]
        lignes = ["Voici tes derniers messages:"]
        
        for msg in derniers_user:
            lignes.append(f"- \"{msg['content'][:150]}{'...' if len(msg['content']) > 150 else ''}\"")
        
        return "\n".join(lignes)
    
    return "Action non reconnue."


# --- FONCTION D'EXTRACTION MOT-CLÃ‰ ---
async def extraire_mot_cle(user_message: str, llm_client: AsyncOpenAI) -> str:
    """Extrait le mot franÃ§ais Ã  traduire de maniÃ¨re robuste."""
    import re
    
    # Patterns regex pour Ã©viter appel LLM inutile
    patterns_rapides = [
        r"comment (?:dit-on|on dit) (?:le |la |l'|un |une )?([a-zÃ Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¹Ã»Ã¼]+)",
        r"(?:c'est quoi|quoi c'est) (?:le |la |l'|un |une )?([a-zÃ Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¹Ã»Ã¼]+)",
        r"traduction (?:de |d')?(?:le |la |l'|un |une )?([a-zÃ Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¹Ã»Ã¼]+)",
        r"(?:le |la |l'|un |une )?([a-zÃ Ã¢Ã¤Ã©Ã¨ÃªÃ«Ã¯Ã®Ã´Ã¹Ã»Ã¼]+) en n'?ko"
    ]
    
    for pattern in patterns_rapides:
        match = re.search(pattern, user_message.lower())
        if match:
            mot = match.group(1).strip()
            logging.info(f"ğŸ”‘ Mot extrait rapidement: '{mot}'")
            return mot
    
    # Recherche de mots entre guillemets
    quoted = re.findall(r"['\"]([^'\"]+)['\"]", user_message)
    if quoted:
        mot = quoted[0].strip().lower()
        logging.info(f"ğŸ”‘ Mot extrait des guillemets: '{mot}'")
        return mot
    
    # Extraction via LLM (fallback)
    prompt = f"""Extrait UNIQUEMENT le mot franÃ§ais Ã  traduire. RÃ©ponds avec UN SEUL MOT.

Exemples:
- "comment dit-on silex en n'ko" -> silex
- "traduction de bonjour" -> bonjour
- "c'est quoi eau" -> eau

Question: {user_message}
Mot:"""

    try:
        resp = await llm_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=10
        )
        mot = resp.choices[0].message.content.strip().lower()
        mot = re.sub(r'[^\w\s-]', '', mot).strip()
        logging.info(f"ğŸ”‘ Mot-clÃ© extrait par LLM: '{mot}'")
        return mot
    except Exception as e:
        logging.error(f"âŒ Erreur extraction: {e}")
        words = user_message.lower().split()
        stop_words = {'comment', 'dit', 'on', 'en', 'nko', 'n\'ko', 'traduction', 'de', 'le', 'la', 'un', 'une', 'c\'est', 'quoi'}
        significant = [w for w in words if w not in stop_words and len(w) > 2]
        return significant[-1] if significant else user_message.lower()


# ğŸ†• PHASE 6: RECHERCHE INTELLIGENTE AVEC FILTRAGE
async def recherche_intelligente_filtree(mot_cle: str, llm_client: AsyncOpenAI, qdrant_client: AsyncQdrantClient):
    """Recherche vectorielle optimisÃ©e."""
    try:
        emb_resp = await llm_client.embeddings.create(
            input=[mot_cle],
            model=EMBEDDING_MODEL
        )
        vector = emb_resp.data[0].embedding
        
        limit_rag = 15 if len(mot_cle.split()) > 2 else 10
        
        result = await qdrant_client.query_points(
            collection_name=COLLECTION_NAME,
            query=vector,
            limit=limit_rag,
            with_payload=True,
            score_threshold=0.7
        )
        
        hits = result.points
        
        if hits:
            logging.info(f"ğŸ” RAG: {len(hits)} rÃ©sultats pour '{mot_cle}'")
            for i, h in enumerate(hits[:5], 1):
                type_r = h.payload.get('type', 'mot')
                titre = h.payload.get('element_franÃ§ais') or h.payload.get('titre_rÃ¨gle', 'N/A')
                logging.info(f"   [{i}] {type_r.upper()} | Score: {h.score:.3f} | {titre[:40]}")
        else:
            logging.warning(f"âš ï¸ RAG: Aucun rÃ©sultat pour '{mot_cle}'")
        
        return hits
        
    except Exception as e:
        logging.error(f"âŒ Recherche Ã©chouÃ©e: {e}")
        return []


# --- PRÃ‰-TRAITEMENT INTELLIGENT ---
async def pretraiter_question(user_message: str, llm_client: AsyncOpenAI, qdrant_client: AsyncQdrantClient):
    """DÃ©tecte les mots N'ko et les traduit pour enrichir la recherche."""
    import re
    import unicodedata
    
    def normaliser_nko(texte: str) -> str:
        """Normalise un texte N'ko pour comparaison fiable"""
        if not texte:
            return ""
        texte = unicodedata.normalize('NFD', texte)
        texte = unicodedata.normalize('NFC', texte)
        texte = ' '.join(texte.split())
        return texte.strip()
    
    nko_pattern = re.compile(r'[\u07C0-\u07FF]+')
    nko_words = nko_pattern.findall(user_message)
    
    if not nko_words:
        return user_message, []
    
    logging.info(f"ğŸ” Mots N'ko dÃ©tectÃ©s dans la question: {nko_words}")
    
    traductions = []
    for nko_word in nko_words:
        try:
            nko_word_norm = normaliser_nko(nko_word)
            
            emb_resp = await llm_client.embeddings.create(
                input=[nko_word_norm],
                model=EMBEDDING_MODEL
            )
            vector = emb_resp.data[0].embedding
            
            results = await qdrant_client.query_points(
                collection_name=COLLECTION_NAME,
                query=vector,
                limit=20,
                with_payload=True
            )
            
            # Match exact normalisÃ©
            for point in results.points:
                point_nko = point.payload.get('element_nko', '')
                point_nko_norm = normaliser_nko(point_nko)
                
                if point_nko_norm == nko_word_norm:
                    fr = point.payload.get('element_franÃ§ais')
                    if fr:
                        traductions.append({
                            'nko': nko_word,
                            'franÃ§ais': fr,
                            'payload': point.payload
                        })
                        logging.info(f"âœ… Match exact trouvÃ©: {nko_word} = {fr}")
                        break
            
            # Meilleur score si pas de match exact
            if not any(t['nko'] == nko_word for t in traductions):
                if results.points and results.points[0].score > 0.80:
                    best = results.points[0]
                    fr = best.payload.get('element_franÃ§ais')
                    if fr:
                        traductions.append({
                            'nko': nko_word,
                            'franÃ§ais': fr,
                            'payload': best.payload
                        })
                        logging.info(f"âœ… Meilleur match trouvÃ©: {nko_word} = {fr}")
                
        except Exception as e:
            logging.error(f"âŒ Erreur lors de la recherche de {nko_word}: {e}")
    
    # Enrichir la question
    question_enrichie = user_message
    for trad in traductions:
        question_enrichie = question_enrichie.replace(
            trad['nko'], 
            f"{trad['nko']} ({trad['franÃ§ais']})"
        )
    
    if traductions:
        logging.info(f"ğŸ’¡ Question enrichie: {question_enrichie}")
    
    return question_enrichie, traductions


# ğŸ†• v3.1: DÃ‰TECTION DE MODE - Role Playing Adaptatif
def detecter_mode_reponse(
    user_message: str, 
    apprentissage_info: Optional[Dict], 
    legacy_param: Optional[Dict] = None  # GardÃ© pour compatibilitÃ©, non utilisÃ©
) -> str:
    """DÃ©termine le mode de rÃ©ponse: conversationnel, Ã©lÃ¨ve, ou enseignant."""
    import re
    
    # MODE Ã‰LÃˆVE: Si apprentissage dÃ©tectÃ©
    if apprentissage_info:
        return "Ã©lÃ¨ve"
    
    # MODE ENSEIGNANT: Si question sur N'ko
    patterns_enseignement = [
        r'comment\s+(dit-on|on\s+dit|dire|Ã©crire|prononce)',
        r'qu.est-ce\s+que.*en\s+n.?ko',
        r'tradui[st]',
        r'(Ã§a|c.est)\s+(veut\s+dire|signifie)\s+quoi',
        r'explique.*n.?ko',
        r'c.est\s+quoi',
        r'quel(?:le)?\s+(?:est|sont)',
        r'combien\s+(?:de|y\s+a)',
        r'pourquoi.*n.?ko',
        r'donne[-\s]moi',
        r'peux[-\s]tu\s+(?:me\s+)?(?:dire|donner|expliquer)',
        r'Ã©cri[st]\s+.+\s+(?:en\s+)?(?:n.?ko|au\s+pluriel|au\s+singulier)',
        r'marque\s+(?:du|de\s+la|des)',
        r'forme\s+(?:du|de\s+la)',
    ]
    
    message_lower = user_message.lower()
    for pattern in patterns_enseignement:
        if re.search(pattern, message_lower):
            return "enseignant"
    
    return "conversationnel"


# ğŸ†• v3.1.1: DÃ‰TECTION LISTES MULTI-LIGNES
def detecter_liste_multilignes(message: str) -> Optional[Dict]:
    """DÃ©tecte les listes avec plusieurs lignes de format A=B ou A\tB."""
    import re
    
    lines = message.strip().split('\n')
    lines = [l.strip() for l in lines if l.strip()]
    
    if len(lines) < 2:
        return None
    
    items = []
    for line in lines:
        match = re.match(r'^(.+?)\s*[=\t]\s*(.+)$', line)
        if match:
            partie1 = match.group(1).strip()
            partie2 = match.group(2).strip()
            
            nko_pattern = re.compile(r'[\u07C0-\u07FF]+')
            has_nko_1 = bool(nko_pattern.search(partie1))
            has_nko_2 = bool(nko_pattern.search(partie2))
            
            if has_nko_1 and not has_nko_2:
                items.append({'franÃ§ais': partie2, 'nko': partie1})
            elif has_nko_2 and not has_nko_1:
                items.append({'franÃ§ais': partie1, 'nko': partie2})
    
    if len(items) >= 2:
        return {
            'type': 'liste',
            'items': items,
            'nom_liste': f"Liste de {len(items)} mots"
        }
    
    return None


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ†• v3.2.0-STRICT-FIX2: DÃ‰TECTION STRICTE AVEC .strip()
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def detecter_apprentissage_strict(message: str) -> Optional[Dict]:
    """
    ğŸ†• v3.2.0-STRICT: DÃ©tection STRICTE basÃ©e sur prÃ©fixes explicites uniquement.
    
    âš ï¸ FIX: message.strip() pour tolÃ©rer espaces/retours ligne au dÃ©but
    
    PrÃ©fixes autorisÃ©s (8 commandes):
    1. "apprendre mot :"
    2. "apprendre rÃ¨gle :"
    3. "apprendre fait sur [nom] :"
    4. "apprendre anecdote :"
    5. "apprendre liste :"
    6. "apprendre conjugaison :"
    7. "apprendre expression :"
    8. "apprendre proverbe :"
    
    Tout le reste = PAS d'apprentissage = conversation normale
    """
    import re
    
    # âœ… CRITICAL FIX: .strip() pour enlever espaces/retours ligne dÃ©but/fin
    message = message.replace("\\'", "'").replace('\\"', '"')
    message_clean = message.strip()
    message_lower = message_clean.lower()
    
    # 1ï¸âƒ£ APPRENDRE MOT
    if message_lower.startswith("apprendre mot :") or message_lower.startswith("apprendre mot:"):
        contenu = re.sub(r'^apprendre mot\s*:\s*', '', message_clean, flags=re.IGNORECASE)
        match = re.match(r'^(.+?)\s*=\s*(.+)$', contenu.strip())
        if not match:
            return {'type': 'erreur', 'message': 'âŒ Format invalide. Utilise : apprendre mot : franÃ§ais = nko'}
        
        partie1, partie2 = match.group(1).strip(), match.group(2).strip()
        nko_pattern = re.compile(r'[\u07C0-\u07FF]+')
        has_nko_1, has_nko_2 = bool(nko_pattern.search(partie1)), bool(nko_pattern.search(partie2))
        
        if has_nko_1 and not has_nko_2:
            return {'type': 'mot', 'element_franÃ§ais': partie2, 'element_nko': partie1, 'concept_identifie': 'Vocabulaire'}
        elif has_nko_2 and not has_nko_1:
            return {'type': 'mot', 'element_franÃ§ais': partie1, 'element_nko': partie2, 'concept_identifie': 'Vocabulaire'}
        else:
            return {'type': 'erreur', 'message': 'âŒ Format : franÃ§ais = ß’ßß'}
    
    # 2ï¸âƒ£ APPRENDRE RÃˆGLE
    elif message_lower.startswith("apprendre rÃ¨gle :") or message_lower.startswith("apprendre regle :") or \
         message_lower.startswith("apprendre rÃ¨gle:") or message_lower.startswith("apprendre regle:"):
        contenu = re.sub(r'^apprendre r[Ã¨e]gle\s*:\s*', '', message_clean, flags=re.IGNORECASE)
        if not contenu.strip():
            return {'type': 'erreur', 'message': 'âŒ RÃ¨gle vide'}
        
        # ğŸ†• v3.2.1: Extraire mots-clÃ©s pour amÃ©liorer recherche RAG
        # Mots-clÃ©s importants en franÃ§ais et N'ko
        keywords_patterns = [
            r'(ton|tons|accent|accents|diacritique|diacritiques)',
            r'(pluriel|singulier|mutation|nasalisation)',
            r'(voyelle|consonne|lettre|alphabet)',
            r'(ßßŠß²ß¡ßŠß›ß™ß‹|ßßŠß²ß ßŠß˜ßŒß¦ßŠßŸßŠß²|ß˜ßß¬ß¦ßŸßß¬ß¡ßŠß²)',  # N'ko keywords
            r'(\d+\s*(?:ton|accent|diacritique|voyelle|consonne))',  # "8 tons", "3 voyelles"
        ]
        
        keywords = []
        contenu_lower = contenu.lower()
        for pattern in keywords_patterns:
            matches = re.findall(pattern, contenu_lower, flags=re.IGNORECASE)
            keywords.extend(matches)
        
        # CrÃ©er titre enrichi avec mots-clÃ©s
        if keywords:
            # DÃ©dupliquer et prendre les 3 premiers
            unique_keywords = []
            for kw in keywords:
                if kw not in unique_keywords:
                    unique_keywords.append(kw)
            keywords_str = ' '.join(unique_keywords[:3])
            titre = f"{keywords_str} - {contenu[:40]}"
            if len(titre) > 80:
                titre = titre[:77] + "..."
        else:
            # Fallback: premiers 60 caractÃ¨res
            titre = contenu[:60] + ("..." if len(contenu) > 60 else "")
        
        return {'type': 'rÃ¨gle', 'titre_rÃ¨gle': titre, 'explication_rÃ¨gle': contenu, 'concept_identifie': 'Grammaire'}
    
    # 3ï¸âƒ£ APPRENDRE FAIT SUR
    elif message_lower.startswith("apprendre fait sur"):
        match = re.match(r'apprendre fait sur\s+(.+?)\s*:\s*(.+)', message_clean, flags=re.IGNORECASE | re.DOTALL)
        if not match:
            return {'type': 'erreur', 'message': 'âŒ Format invalide'}
        return {'type': 'fait', 'titre': match.group(1).strip(), 'contenu': match.group(2).strip(), 'concept_identifie': 'Culture'}
    
    # 4ï¸âƒ£ APPRENDRE ANECDOTE
    elif message_lower.startswith("apprendre anecdote :") or message_lower.startswith("apprendre anecdote:"):
        contenu = re.sub(r'^apprendre anecdote\s*:\s*', '', message_clean, flags=re.IGNORECASE)
        if not contenu.strip():
            return {'type': 'erreur', 'message': 'âŒ Anecdote vide'}
        titre = contenu[:50] + ("..." if len(contenu) > 50 else "")
        return {'type': 'anecdote', 'titre': titre, 'contenu': contenu, 'concept_identifie': 'Culture'}
    
    # 5ï¸âƒ£ APPRENDRE LISTE
    elif message_lower.startswith("apprendre liste :") or message_lower.startswith("apprendre liste:"):
        contenu = re.sub(r'^apprendre liste\s*:\s*', '', message_clean, flags=re.IGNORECASE)
        lines = [l.strip() for l in contenu.strip().split('\n') if l.strip()]
        if len(lines) < 1:
            return {'type': 'erreur', 'message': 'âŒ Liste vide'}
        
        nom_liste = lines[0] if '=' not in lines[0] else f"Liste de {len(lines)} mots"
        elements_start = 1 if '=' not in lines[0] else 0
        elements, nko_pattern = [], re.compile(r'[\u07C0-\u07FF]+')
        
        for line in lines[elements_start:]:
            match = re.match(r'^(.+?)\s*=\s*(.+)$', line)
            if match:
                p1, p2 = match.group(1).strip(), match.group(2).strip()
                if nko_pattern.search(p1) and not nko_pattern.search(p2):
                    elements.append({'franÃ§ais': p2, 'nko': p1})
                elif nko_pattern.search(p2) and not nko_pattern.search(p1):
                    elements.append({'franÃ§ais': p1, 'nko': p2})
        
        if not elements:
            return {'type': 'erreur', 'message': 'âŒ Aucun Ã©lÃ©ment valide'}
        return {'type': 'liste', 'nom_liste': nom_liste, 'elements_liste': elements, 'concept_identifie': 'Vocabulaire'}
    
    # 6ï¸âƒ£ APPRENDRE CONJUGAISON
    elif message_lower.startswith("apprendre conjugaison :") or message_lower.startswith("apprendre conjugaison:"):
        contenu = re.sub(r'^apprendre conjugaison\s*:\s*', '', message_clean, flags=re.IGNORECASE)
        match = re.match(r'([\u07C0-\u07FF]+)\s*\(([^)]+)\)\s*:\s*(.+)', contenu, flags=re.DOTALL)
        if not match:
            return {'type': 'erreur', 'message': 'âŒ Format invalide'}
        
        formes = {}
        for item in match.group(3).strip().split(','):
            if ':' in item:
                temps, forme = item.split(':', 1)
                formes[temps.strip()] = forme.strip()
        if not formes:
            return {'type': 'erreur', 'message': 'âŒ Aucune forme'}
        return {'type': 'conjugaison', 'verbe_nko': match.group(1).strip(), 'verbe_franÃ§ais': match.group(2).strip(), 'formes': formes, 'concept_identifie': 'Grammaire'}
    
    # 7ï¸âƒ£ APPRENDRE EXPRESSION
    elif message_lower.startswith("apprendre expression :") or message_lower.startswith("apprendre expression:"):
        contenu = re.sub(r'^apprendre expression\s*:\s*', '', message_clean, flags=re.IGNORECASE)
        match = re.match(r'^([\u07C0-\u07FF\s]+)\s*=\s*(.+)$', contenu)
        if not match:
            return {'type': 'erreur', 'message': 'âŒ Format invalide'}
        trad_lit = None
        lit_match = re.search(r'\(litt[Ã©eralement]*\s*:\s*([^)]+)\)', match.group(2), re.IGNORECASE)
        if lit_match:
            trad_lit = lit_match.group(1).strip()
        return {'type': 'expression', 'texte_nko': match.group(1).strip(), 'signification': match.group(2).strip(), 'traduction_littÃ©rale': trad_lit, 'concept_identifie': 'Expression'}
    
    # 8ï¸âƒ£ APPRENDRE PROVERBE
    elif message_lower.startswith("apprendre proverbe :") or message_lower.startswith("apprendre proverbe:"):
        contenu = re.sub(r'^apprendre proverbe\s*:\s*', '', message_clean, flags=re.IGNORECASE)
        match = re.match(r'^([\u07C0-\u07FF\s]+)\s*=\s*(.+)$', contenu)
        if not match:
            return {'type': 'erreur', 'message': 'âŒ Format invalide'}
        return {'type': 'proverbe', 'texte_nko': match.group(1).strip(), 'signification': match.group(2).strip(), 'concept_identifie': 'Culture'}
    
    # âŒ AUCUN PRÃ‰FIXE = PAS D'APPRENTISSAGE
    return None


# --- PHASE 5.1: DÃ‰TECTION MULTI-TYPES COMPLÃˆTE (LEGACY - gardÃ©e pour compatibilitÃ©) ---
def detecter_type_connaissance(message: str) -> Optional[Dict]:
    """DÃ©tecte le type de connaissance dans le message."""
    import re
    
    message = message.replace("\\'", "'").replace('\\"', '"')
    message_clean = message.strip().lower()
    
    # Filtre - Phrases qui ne sont PAS des apprentissages
    phrases_ignorees = [
        'il me semble', 'je pense', 'Ã  mon avis', 'peut-Ãªtre', 'probablement',
        'claude.ai', 'claude', 'tu vois', 'fais', 'peux-tu', 'pourrais-tu',
        'devrais', 'devrait', 'comment fonctionne', 'parle moi', 'explique'
    ]
    
    if any(phrase in message_clean for phrase in phrases_ignorees):
        return None
    
    # PRIORITÃ‰ 0 - DÃ©tection listes multi-lignes
    liste_info = detecter_liste_multilignes(message)
    if liste_info:
        return liste_info
    
    # 1ï¸âƒ£ RÃˆGLES GRAMMATICALES
    patterns_rÃ¨gle = [
        r'(?:apprends?|mÃ©morise[rz]?)\s+(?:la\s+)?rÃ¨gle\s*[:;]?\s*(.+)',
        r'rÃ¨gle\s+(?:de\s+)?(?:grammaire|grammaticale)\s*[:;]?\s*(.+)',
        r'en\s+n.?ko,?\s+(.+?)\s+(?:se\s+forme|fonctionne|s.Ã©crit)',
    ]
    
    for pattern in patterns_rÃ¨gle:
        match = re.search(pattern, message_clean, re.IGNORECASE | re.DOTALL)
        if match:
            explication = match.group(1).strip()
            titre = explication.split()[:8]
            titre = ' '.join(titre) + ("..." if len(explication.split()) > 8 else "")
            
            return {
                'type': 'rÃ¨gle',
                'titre_rÃ¨gle': titre,
                'explication_rÃ¨gle': explication,
                'concept_identifie': 'grammaire'
            }
    
    # 2ï¸âƒ£ FAITS CULTURELS
    patterns_fait = [
        r'(?:apprends?|mÃ©morise[rz]?)\s+(?:le\s+)?fait\s*[:;]?\s*(.+)',
        r'fait\s+(?:culturel|historique|linguistique)\s*[:;]?\s*(.+)',
        r'contexte\s*[:;]?\s*(.+)',
        r'(?:sache|note)\s+que\s+(.+)',
    ]
    
    for pattern in patterns_fait:
        match = re.search(pattern, message_clean, re.IGNORECASE | re.DOTALL)
        if match:
            contenu = match.group(1).strip()
            titre = contenu[:60] + ("..." if len(contenu) > 60 else "")
            
            return {
                'type': 'fait',
                'titre': titre,
                'contenu': contenu,
                'concept_identifie': 'culture'
            }
    
    return None


# --- PHASE 5: DÃ‰TECTION D'APPRENTISSAGE (MOTS SIMPLES) ---
def detecter_apprentissage(message: str) -> Optional[Dict[str, str]]:
    """DÃ©tecte si le message est une demande d'apprentissage de MOT simple."""
    import re
    
    message = message.replace("\\'", "'").replace('\\"', '"')
    message_clean = message.strip().lower()
    
    # Pattern 0 - "apprend [et enregistre/mÃ©morise] [Ã§a/que] : X signifie Y"
    pattern0 = r'(?:apprends?|mÃ©morise[rz]?|enregistre[rz]?)\s+(?:et\s+)?(?:enregistre[rz]?|mÃ©morise[rz]?)?\s*(?:Ã§a|ceci|cela|que)?\s*[:;]\s*(.+?)\s+signifie\s+["\']?(.+?)(?:["\'])?$'
    
    match = re.search(pattern0, message_clean, re.IGNORECASE)
    if match:
        partie1 = match.group(1).strip()
        partie2 = match.group(2).strip()
        
        nko_pattern = re.compile(r'[\u07C0-\u07FF]+')
        
        has_nko_1 = bool(nko_pattern.search(partie1))
        has_nko_2 = bool(nko_pattern.search(partie2))
        
        if has_nko_1 and not has_nko_2:
            return {'nko': partie1, 'franÃ§ais': partie2, 'pattern': 'explication_signifie'}
        elif has_nko_2 and not has_nko_1:
            return {'nko': partie2, 'franÃ§ais': partie1, 'pattern': 'explication_signifie'}
    
    # Pattern 1 - "apprend [et enregistre] [Ã§a/que] : X = Y"
    pattern1 = r'(?:apprends?|mÃ©morise[rz]?|enregistre[rz]?)\s+(?:et\s+)?(?:enregistre[rz]?|mÃ©morise[rz]?)?\s*(?:Ã§a|ceci|cela|que)?\s*[:;]?\s*(.+?)\s*[=:]\s*(.+)'
    
    # Pattern 2: "X = Y" (simple)
    pattern2 = r'^([^\s=]+)\s*[=:]\s*([^\s=]+)$'
    
    # Pattern 3: "X signifie Y"
    pattern3 = r'(.+?)\s+signifie\s+(.+)'
    
    # Pattern 4: "Y se dit X en N'ko"
    pattern4 = r'(.+?)\s+se\s+dit\s+(.+?)\s+en\s+n.?ko'
    
    for pattern in [pattern1, pattern3, pattern4, pattern2]:
        match = re.search(pattern, message_clean, re.IGNORECASE)
        if match:
            word1, word2 = match.groups()
            word1 = word1.strip()
            word2 = word2.strip()
            
            # Nettoyer mots parasites
            parasites = ['et enregistre Ã§a', 'et mÃ©morise Ã§a', 'et enregistre', 'et mÃ©morise']
            for parasite in parasites:
                word1 = word1.replace(parasite, '').strip()
                word2 = word2.replace(parasite, '').strip()
            
            nko_pattern = re.compile(r'[\u07C0-\u07FF]+')
            
            has_nko_1 = bool(nko_pattern.search(word1))
            has_nko_2 = bool(nko_pattern.search(word2))
            
            if has_nko_1 and not has_nko_2:
                return {'nko': word1, 'franÃ§ais': word2, 'pattern': 'dÃ©tectÃ©'}
            elif has_nko_2 and not has_nko_1:
                return {'nko': word2, 'franÃ§ais': word1, 'pattern': 'dÃ©tectÃ©'}
    
    return None


async def apprendre_mot(
    nko_word: str,
    fr_word: str,
    llm_client: AsyncOpenAI,
    qdrant_client: AsyncQdrantClient,
    concept: str = "Appris par utilisateur",
    user_context: Optional[Dict] = None
) -> Dict[str, any]:
    """Apprend un nouveau mot et le stocke dans Qdrant."""
    try:
        import unicodedata
        
        def normaliser(texte: str) -> str:
            texte = unicodedata.normalize('NFD', texte)
            texte = unicodedata.normalize('NFC', texte)
            return ' '.join(texte.split()).strip()
        
        nko_word_clean = normaliser(nko_word)
        fr_word_clean = normaliser(fr_word)
        
        logging.info(f"ğŸ“š Apprentissage: {nko_word_clean} = {fr_word_clean}")
        
        # VÃ©rifier si le mot existe dÃ©jÃ 
        # ğŸ†• v3.2.1: Normalisation NFC avant envoi Ã  OpenAI
        emb_resp = await llm_client.embeddings.create(
            input=[normaliser_texte(fr_word_clean)],
            model=EMBEDDING_MODEL
        )
        vector = emb_resp.data[0].embedding
        
        results = await qdrant_client.query_points(
            collection_name=COLLECTION_NAME,
            query=vector,
            limit=5,
            with_payload=True
        )
        
        # VÃ©rifier match exact
        for point in results.points:
            if (normaliser(point.payload.get('element_nko', '')) == nko_word_clean and
                normaliser(point.payload.get('element_franÃ§ais', '')) == fr_word_clean):
                logging.info(f"â„¹ï¸ Ce mot existe dÃ©jÃ  dans la base")
                return {
                    'status': 'exists',
                    'message': f"Je connais dÃ©jÃ  ce mot : {nko_word_clean} = {fr_word_clean}",
                    'word_nko': nko_word_clean,
                    'word_fr': fr_word_clean
                }
        
        # CrÃ©er l'entrÃ©e
        nouvelle_entree = {
            'element_franÃ§ais': fr_word_clean,
            'element_nko': nko_word_clean,
            'concept_identifie': concept,
            'fait_texte': user_context.get('description') if user_context else None,
            'exemples': user_context.get('exemples') if user_context else None,
            'appris_par': 'utilisateur',
            'timestamp': str(asyncio.get_event_loop().time())
        }
        
        # CrÃ©er le point Qdrant
        point_id = str(uuid.uuid4())
        point = PointStruct(
            id=point_id,
            vector=vector,
            payload=nouvelle_entree
        )
        
        await qdrant_client.upsert(
            collection_name=COLLECTION_NAME,
            points=[point]
        )
        
        logging.info(f"âœ… Mot appris et stockÃ©: {nko_word_clean} = {fr_word_clean}")
        
        return {
            'status': 'success',
            'message': f"âœ… J'ai appris : {nko_word_clean} = {fr_word_clean}",
            'word_nko': nko_word_clean,
            'word_fr': fr_word_clean,
            'point_id': point_id
        }
        
    except Exception as e:
        logging.error(f"âŒ Erreur lors de l'apprentissage: {e}")
        return {
            'status': 'error',
            'message': f"âŒ Erreur lors de l'apprentissage: {str(e)}"
        }


# --- PHASE 5.1: APPRENTISSAGE MULTI-TYPES ---
async def apprendre_connaissance(
    connaissance_data: Dict,
    llm_client: AsyncOpenAI,
    qdrant_client: AsyncQdrantClient
) -> Dict[str, any]:
    """Apprend n'importe quel type de connaissance (rÃ¨gles, faits, listes, etc.)."""
    try:
        import unicodedata
        import time
        
        type_conn = connaissance_data.get('type', 'mot')
        
        logging.info(f"ğŸ“š Apprentissage type '{type_conn}': {connaissance_data}")
        
        # DÃ©terminer le texte pour l'embedding selon le type
        if type_conn == 'mot':
            texte_embedding = connaissance_data.get('franÃ§ais', '')
        elif type_conn == 'rÃ¨gle':
            texte_embedding = f"{connaissance_data.get('titre_rÃ¨gle', '')} {connaissance_data.get('explication_rÃ¨gle', '')}"
        elif type_conn in ['fait', 'anecdote']:
            texte_embedding = f"{connaissance_data.get('titre', '')} {connaissance_data.get('contenu', '')}"
        elif type_conn == 'liste':
            nom = connaissance_data.get('nom_liste', '')
            elements = connaissance_data.get('elements_liste', [])
            elements_text = ' '.join([f"{e.get('fr', '')} {e.get('nko', '')}" for e in elements])
            texte_embedding = f"{nom} {elements_text}"
        else:
            texte_embedding = str(connaissance_data)
        
        # ğŸ†• v3.2.0 PHASE 2: DÃ©tection automatique + chunking si nÃ©cessaire
        texte_embedding = texte_embedding.replace("\\'", "'").replace('\\"', '"')
        texte_embedding = ' '.join(texte_embedding.split())
        texte_embedding = ''.join(char for char in texte_embedding if ord(char) >= 32 or char in '\n\t')
        
        if len(texte_embedding) > MAX_CHARS_EMBEDDING:
            logging.warning(f"âš ï¸ Texte long ({len(texte_embedding)} chars)")
            
            # Analyser le message
            analyse = MessageTypeDetector.analyser_longueur_message(texte_embedding)
            
            if analyse['chunking']:
                # CHUNKING activÃ© pour textes trÃ¨s longs
                logging.info(f"ğŸ“¦ Chunking activÃ©: {analyse['description']}")
                chunks = ChunkingSystem.chunker_texte_long(texte_embedding, max_chunk=4000)
                logging.info(f"âœ… {len(chunks)} chunks crÃ©Ã©s")
                
                # Traiter chaque chunk sÃ©parÃ©ment
                resultats_chunks = []
                for i, chunk in enumerate(chunks):
                    # CrÃ©er embedding du chunk
                    emb_resp_chunk = await llm_client.embeddings.create(
                        input=[chunk],
                        model=EMBEDDING_MODEL
                    )
                    vector_chunk = emb_resp_chunk.data[0].embedding
                    
                    # Stocker avec mÃ©tadonnÃ©es de chunking
                    chunk_data = {
                        **connaissance_data,
                        'chunk_index': i,
                        'total_chunks': len(chunks),
                        'contenu_chunk': chunk,
                        'is_chunk': True,
                        'appris_par': 'utilisateur',
                        'date_ajout': str(time.time())
                    }
                    
                    point_chunk = PointStruct(
                        id=str(uuid.uuid4()),
                        vector=vector_chunk,
                        payload=chunk_data
                    )
                    
                    await qdrant_client.upsert(
                        collection_name=COLLECTION_NAME,
                        points=[point_chunk]
                    )
                    
                    resultats_chunks.append(point_chunk.id)
                
                # Retourner rÃ©sultat multi-chunks
                message_chunks = f"âœ… Texte long traitÃ© en {len(chunks)} chunks"
                if type_conn == 'mot':
                    message_chunks = f"âœ… Liste de {len(chunks)} sections mÃ©morisÃ©e"
                elif type_conn == 'rÃ¨gle':
                    message_chunks = f"âœ… RÃ¨gle longue mÃ©morisÃ©e en {len(chunks)} parties"
                
                return {
                    'status': 'success',
                    'message': message_chunks,
                    'type': type_conn,
                    'chunks': len(chunks),
                    'point_ids': resultats_chunks
                }
            else:
                # Tronquer Ã  MAX_CHARS_EMBEDDING si pas de chunking nÃ©cessaire
                texte_embedding = texte_embedding[:MAX_CHARS_EMBEDDING] + "..."
        
        # CrÃ©er embedding
        emb_resp = await llm_client.embeddings.create(
            input=[texte_embedding],
            model=EMBEDDING_MODEL
        )
        vector = emb_resp.data[0].embedding
        
        # CrÃ©er l'entrÃ©e avec mÃ©tadonnÃ©es
        nouvelle_entree = {
            **connaissance_data,
            'appris_par': 'utilisateur',
            'date_ajout': str(time.time())
        }
        
        # CrÃ©er le point Qdrant
        point_id = str(uuid.uuid4())
        point = PointStruct(
            id=point_id,
            vector=vector,
            payload=nouvelle_entree
        )
        
        await qdrant_client.upsert(
            collection_name=COLLECTION_NAME,
            points=[point]
        )
        
        # Message de confirmation selon le type
        if type_conn == 'mot':
            message = f"âœ… J'ai appris : {connaissance_data.get('element_nko')} = {connaissance_data.get('element_franÃ§ais')}"
        elif type_conn == 'rÃ¨gle':
            message = f"âœ… RÃ¨gle grammaticale mÃ©morisÃ©e : {connaissance_data.get('titre_rÃ¨gle')}"
        elif type_conn == 'fait':
            message = f"âœ… Fait culturel mÃ©morisÃ© : {connaissance_data.get('titre')}"
        elif type_conn == 'liste':
            nb_elements = len(connaissance_data.get('elements_liste', []))
            message = f"âœ… Liste '{connaissance_data.get('nom_liste')}' mÃ©morisÃ©e ({nb_elements} Ã©lÃ©ments)"
        else:
            message = f"âœ… Connaissance de type '{type_conn}' mÃ©morisÃ©e"
        
        logging.info(f"âœ… Connaissance apprise et stockÃ©e: {message}")
        
        return {
            'status': 'success',
            'message': message,
            'type': type_conn,
            'point_id': point_id
        }
        
    except Exception as e:
        logging.error(f"âŒ Erreur lors de l'apprentissage: {e}")
        return {
            'status': 'error',
            'message': f"âŒ Erreur lors de l'apprentissage: {str(e)}"
        }


# ğŸ†• PHASE 3 : FONCTIONS DE TRANSCRIPTION PHONÃ‰TIQUE
def transcrire_nko_phonetique(mot_nko: str) -> str:
    """Transcrit un mot N'ko en phonÃ©tique latine."""
    transcription = ""
    for char in mot_nko:
        transcription += NKO_PHONETIC_MAP.get(char, char)
    return transcription


# --- PHASE 5.1: FORMATAGE CONTEXTE MULTI-TYPES ---
def formater_connaissance_pour_contexte(payload: Dict) -> str:
    """Formate une connaissance pour le contexte RAG selon son type."""
    type_conn = payload.get('type', 'mot')
    
    if type_conn == 'mot':
        fr = payload.get('element_franÃ§ais', '')
        nko = payload.get('element_nko', '')
        concept = payload.get('concept_identifie', '')
        ligne = f"- {fr} = {nko} ({concept})"
        
        valeur_num = payload.get('valeur_numerique')
        if valeur_num is not None:
            ligne += f" | valeur: {valeur_num}"
        
        fait = payload.get('fait_texte')
        if fait:
            ligne += f" | info: {fait}"
        
        phonetique = transcrire_nko_phonetique(nko)
        if phonetique and phonetique != nko:
            ligne += f" | prononciation: {phonetique}"
        
        return ligne
    
    elif type_conn == 'rÃ¨gle':
        titre = payload.get('titre_rÃ¨gle', '')
        explication = payload.get('explication_rÃ¨gle', '')
        return f"- [RÃˆGLE] {titre}: {explication}"
    
    elif type_conn == 'fait':
        titre = payload.get('titre', '')
        contenu = payload.get('contenu', '')
        return f"- [FAIT] {titre}: {contenu}"
    
    elif type_conn == 'liste':
        nom_liste = payload.get('nom_liste', '')
        elements = payload.get('elements_liste', [])
        elements_str = ', '.join([f"{e.get('fr')}={e.get('nko')}" for e in elements[:5]])
        if len(elements) > 5:
            elements_str += f"... ({len(elements)} Ã©lÃ©ments)"
        return f"- [LISTE] {nom_liste}: {elements_str}"
    
    else:
        return f"- {payload}"


# --- ENDPOINT CHAT AVEC MÃ‰MOIRE INTELLIGENTE ET LONG CONTEXT ---
@app.post('/chat', response_model=ChatResponse)
async def chat_endpoint(req: ChatRequest):
    global LLM_CLIENT, QDRANT_CLIENT

    if LLM_CLIENT is None:
        raise HTTPException(status_code=503, detail='LLM non initialisÃ©')

    # Gestion de la session
    session_id = get_or_create_session(req.session_id)
    
    # Correction des fautes courantes
    message_corrige = ErrorRecoverySystem.corriger_fautes_courantes(req.user_message)
    
    # Analyse Ã©motionnelle
    emotion, confiance = SentimentAnalyzer.detecter_emotion(message_corrige)
    
    # Profil utilisateur et progression
    profile = get_or_create_user_profile(session_id)
    progress = UserProgress(**profile['progress'])
    
    # Niveau d'engagement
    niveau_engagement = SentimentAnalyzer.detecter_niveau_engagement(
        list(CONVERSATION_MEMORY.get(session_id, []))
    )
    
    # DÃ©tection de rÃ©pÃ©tition utilisateur
    if ErrorRecoverySystem.detecter_repetition_utilisateur(
        list(CONVERSATION_MEMORY.get(session_id, []))
    ):
        tentatives = SESSION_METADATA.get(session_id, {}).get('tentatives_incomprehension', 0)
        tentatives += 1
        if session_id not in SESSION_METADATA:
            SESSION_METADATA[session_id] = {}
        SESSION_METADATA[session_id]['tentatives_incomprehension'] = tentatives
        
        if tentatives >= 3:
            message_incomprehension = ErrorRecoverySystem.generer_message_incomprehension(tentatives)
            ajouter_message_memoire(session_id, 'user', req.user_message)
            ajouter_message_memoire(session_id, 'assistant', message_incomprehension)
            return ChatResponse(
                response_text=message_incomprehension,
                session_id=session_id,
                memory_update=None
            )
    
    debug_info = {} if req.debug else None
    rag_active = req.rag_enabled and (QDRANT_CLIENT is not None)
    contexte_rag_text = '[Aucune donnÃ©e en mÃ©moire]'

    try:
        # ğŸ†• v3.2.0 PHASE 3: Compression automatique de mÃ©moire
        if MemoryCompressionSystem.doit_compresser(session_id):
            logging.info(f"ğŸ—œï¸ DÃ©clenchement compression auto session {session_id[:8]}...")
            compression_ok = await MemoryCompressionSystem.compresser_memoire_ancienne(
                session_id=session_id,
                llm_client=LLM_CLIENT
            )
            
            if compression_ok and req.debug:
                debug_info['memory_compressed'] = True
                debug_info['memory_size_after'] = len(CONVERSATION_MEMORY[session_id])
        
        # VÃ©rifier si c'est une demande d'analyse de mÃ©moire
        intention_memoire = await analyser_intention_memoire(req.user_message, session_id, LLM_CLIENT)
        
        if intention_memoire:
            logging.info(f"ğŸ§  Intention mÃ©moire dÃ©tectÃ©e: {intention_memoire['type']}")
            
            response_text = await executer_action_memoire(intention_memoire, session_id, LLM_CLIENT)
            
            ajouter_message_memoire(session_id, 'user', req.user_message)
            ajouter_message_memoire(session_id, 'assistant', response_text)
            
            return ChatResponse(
                response_text=response_text,
                session_id=session_id,
                memory_update=None,
                debug_info={
                    'intention_memoire': intention_memoire,
                    'historique_size': len(CONVERSATION_MEMORY[session_id])
                } if req.debug else None
            )
        
        # ğŸ†• v3.2.0-STRICT-FIX2: DÃ©tection STRICTE basÃ©e sur prÃ©fixes explicites
        # âš¡ IMPORTANT: DÃ©tection AVANT enrichissement RAG pour Ã©viter pollution
        apprentissage_info = detecter_apprentissage_strict(req.user_message)
        
        # Si erreur de format dÃ©tectÃ©e
        if apprentissage_info and apprentissage_info.get('type') == 'erreur':
            message_erreur = apprentissage_info['message']
            ajouter_message_memoire(session_id, 'user', req.user_message)
            ajouter_message_memoire(session_id, 'assistant', message_erreur)
            return ChatResponse(
                response_text=message_erreur,
                session_id=session_id,
                memory_update=None,
                debug_info={'erreur_format': True} if req.debug else None
            )
        
        if apprentissage_info:
            logging.info(f"ğŸ“ {apprentissage_info['type'].upper()} dÃ©tectÃ©: {apprentissage_info}")
            
            resultat = await apprendre_connaissance(
                connaissance_data=apprentissage_info,
                llm_client=LLM_CLIENT,
                qdrant_client=QDRANT_CLIENT
            )
            
            # GAMIFICATION - Mise Ã  jour progression
            action_type = 'regle_apprise' if apprentissage_info['type'] in ['rÃ¨gle', 'conjugaison', 'grammaire'] else 'mot_appris'
            progress_update = update_user_progress(session_id, action_type, apprentissage_info)
            
            # Construction du message de cÃ©lÃ©bration
            celebration = ""
            
            if progress_update['niveau_change']:
                celebration += f"\n\nğŸŒŸ **NIVEAU {progress_update['nouveau_niveau']} ATTEINT !**"
                celebration += f"\nâœ¨ Tu as maintenant {progress_update['xp_total']} XP !"
            
            for badge in progress_update['nouveaux_badges']:
                celebration += f"\n\n{GamificationSystem.message_celebration(badge)}"
            
            if action_type == 'mot_appris':
                nb_mots_total = progress_update.get('mots_total', progress.mots_appris)
                
                if nb_mots_total == 1:
                    celebration += "\n\nğŸ‰ FÃ©licitations ! On a appris notre premier mot en N'ko !"
                elif nb_mots_total == 10:
                    celebration += f"\n\nğŸŠ Bravo ! On a maintenant {nb_mots_total} mots !"
                elif nb_mots_total == 50:
                    celebration += f"\n\nğŸ† Incroyable ! {nb_mots_total} mots maÃ®trisÃ©s !"
                elif nb_mots_total == 100:
                    celebration += f"\n\nğŸ’ Centenaire atteint ! {nb_mots_total} mots !"
                elif nb_mots_total % 25 == 0:
                    celebration += f"\n\nğŸŒŸ Excellent ! {nb_mots_total} mots en N'ko !"
            
            xp_gain = GamificationSystem.XP_PAR_REGLE if action_type == 'regle_apprise' else GamificationSystem.XP_PAR_MOT
            xp_restants = progress_update['xp_prochain_niveau'] - progress_update['xp_total']
            celebration += f"\n\nğŸ“Š **+{xp_gain} XP** | Encore {xp_restants} XP pour le niveau {progress.niveau + 1}"
            
            resultat['message'] += celebration
            
            ajouter_message_memoire(session_id, 'user', req.user_message)
            ajouter_message_memoire(session_id, 'assistant', resultat['message'])
            
            return ChatResponse(
                response_text=resultat['message'],
                session_id=session_id,
                memory_update=None,
                debug_info={
                    'apprentissage': True,
                    'type': apprentissage_info['type'],
                    'status': resultat['status'],
                    'details': resultat
                } if req.debug else None
            )
        
        # DÃ©tecter apprentissage de MOT simple
        apprentissage_info = detecter_apprentissage(req.user_message)
        
        if apprentissage_info:
            logging.info(f"ğŸ“ Apprentissage MOT dÃ©tectÃ©: {apprentissage_info}")
            
            resultat = await apprendre_mot(
                nko_word=apprentissage_info['nko'],
                fr_word=apprentissage_info['franÃ§ais'],
                llm_client=LLM_CLIENT,
                qdrant_client=QDRANT_CLIENT,
                concept="Appris par utilisateur"
            )
            
            progress_update = update_user_progress(session_id, 'mot_appris', apprentissage_info)
            
            celebration = ""
            
            if progress_update['niveau_change']:
                celebration += f"\n\nğŸŒŸ **NIVEAU {progress_update['nouveau_niveau']} ATTEINT !**"
                celebration += f"\nâœ¨ Tu as maintenant {progress_update['xp_total']} XP !"
            
            for badge in progress_update['nouveaux_badges']:
                celebration += f"\n\n{GamificationSystem.message_celebration(badge)}"
            
            nb_mots_total = progress_update.get('mots_total', progress.mots_appris)
            
            if nb_mots_total == 1:
                celebration += "\n\nğŸ‰ FÃ©licitations ! On a appris notre premier mot en N'ko !"
            elif nb_mots_total == 10:
                celebration += f"\n\nğŸŠ Bravo ! On a maintenant {nb_mots_total} mots !"
            elif nb_mots_total == 50:
                celebration += f"\n\nğŸ† Incroyable ! {nb_mots_total} mots maÃ®trisÃ©s !"
            elif nb_mots_total == 100:
                celebration += f"\n\nğŸ’ Centenaire atteint ! {nb_mots_total} mots !"
            elif nb_mots_total % 25 == 0:
                celebration += f"\n\nğŸŒŸ Excellent ! {nb_mots_total} mots en N'ko !"
            
            xp_restants = progress_update['xp_prochain_niveau'] - progress_update['xp_total']
            celebration += f"\n\nğŸ“Š **+{GamificationSystem.XP_PAR_MOT} XP** | Encore {xp_restants} XP pour le niveau {progress.niveau + 1}"
            
            resultat['message'] += celebration
            
            ajouter_message_memoire(session_id, 'user', req.user_message)
            ajouter_message_memoire(session_id, 'assistant', resultat['message'])
            
            return ChatResponse(
                response_text=resultat['message'],
                session_id=session_id,
                memory_update=None,
                debug_info={
                    'apprentissage': True,
                    'type': 'mot',
                    'status': resultat['status'],
                    'details': resultat
                } if req.debug else None
            )
        
        # Si pas d'apprentissage, continuer normalement
        if rag_active:
            try:
                # PrÃ©-traiter la question
                question_enrichie, traductions_contexte = await pretraiter_question(
                    req.user_message, 
                    LLM_CLIENT, 
                    QDRANT_CLIENT
                )
                
                if req.debug:
                    debug_info['question_enrichie'] = question_enrichie
                    debug_info['traductions_contexte'] = traductions_contexte
                
                # Extraire le mot-clÃ©
                mot_cle = await extraire_mot_cle(question_enrichie, LLM_CLIENT)
                if req.debug:
                    debug_info['mot_cle_extrait'] = mot_cle

                # Recherche intelligente filtrÃ©e
                hits = await recherche_intelligente_filtree(mot_cle, LLM_CLIENT, QDRANT_CLIENT)

                logging.info(f"ğŸ“Š RÃ‰SULTATS pour '{mot_cle}':")
                for i, h in enumerate(hits[:10], 1):
                    logging.info(f"  #{i}: score={h.score:.4f} -> {h.payload.get('element_franÃ§ais', 'N/A')}")
                
                if req.debug:
                    debug_info['top_results'] = [
                        {'score': h.score, 'payload': h.payload} 
                        for h in hits[:10]
                    ]

                # Formater contexte RAG avec GROUPEMENT PAR TYPE
                if hits:
                    logging.info(f"âœ… {len(hits)} rÃ©sultat(s) pertinent(s)")
                    
                    regles = [h for h in hits[:10] if h.payload.get('type') == 'rÃ¨gle']
                    mots = [h for h in hits[:10] if h.payload.get('type') == 'mot']
                    autres = [h for h in hits[:10] if h.payload.get('type') not in ['rÃ¨gle', 'mot']]
                    
                    parts = []
                    
                    if regles:
                        parts.append("ğŸ¯ RÃˆGLES GRAMMATICALES ENSEIGNÃ‰ES PAR L'UTILISATEUR:")
                        for r in regles[:3]:
                            titre = r.payload.get('titre_rÃ¨gle', '')
                            explic = r.payload.get('explication_rÃ¨gle', '')
                            parts.append(f"\nğŸ“– {titre}")
                            parts.append(f"   {explic}")
                        parts.append("")
                    
                    if mots:
                        parts.append("ğŸ“š VOCABULAIRE APPRIS:")
                        for m in mots[:8]:
                            nko = m.payload.get('element_nko', '')
                            fr = m.payload.get('element_franÃ§ais', '')
                            parts.append(f"  â€¢ {fr} = {nko}")
                        parts.append("")
                    
                    if autres:
                        parts.append("â„¹ï¸ AUTRES CONNAISSANCES:")
                        for a in autres[:3]:
                            ligne = formater_connaissance_pour_contexte(a.payload)
                            parts.append(f"  â€¢ {ligne}")
                    
                    contexte_rag_text = "\n".join(parts)
                    
                    if not regles and not mots and not autres:
                        contexte_rag_text = "[Aucune connaissance pertinente]"
                else:
                    logging.warning(f"âš ï¸ Aucun rÃ©sultat trouvÃ©")
                    contexte_rag_text = "[Base de connaissances vide]"

                if traductions_contexte:
                    contexte_extra = '\n'.join(
                        f"- {t['franÃ§ais']} = {t['nko']}"
                        for t in traductions_contexte
                    )
                    contexte_rag_text = contexte_extra + '\n\n' + contexte_rag_text

            except Exception as e:
                logging.error(f"âŒ Erreur RAG: {e}", exc_info=True)
                if req.debug:
                    debug_info['rag_error'] = str(e)
                rag_active = False

        # Formater l'historique de conversation
        historique_conversation = formater_historique_conversation(session_id, limite=20)
        
        logging.info(f"ğŸ“¤ CONTEXTE ENVOYÃ‰ AU LLM:\n{contexte_rag_text}")

        # DÃ‰TERMINER MODE DE RÃ‰PONSE
        mode = detecter_mode_reponse(
            req.user_message,
            apprentissage_info if 'apprentissage_info' in locals() else None,
            None  # type_info n'existe plus, remplacÃ© par apprentissage_info
        )
        logging.info(f"ğŸ­ Mode dÃ©tectÃ©: {mode.upper()}")
        
        instruction_mode = MODE_INSTRUCTIONS.get(mode, MODE_INSTRUCTIONS["conversationnel"])

        # ARCHITECTURE FIX - SÃ©parer system et user messages
        system_message = PROMPT_SYSTEM_BASE
        
        user_message_content = PROMPT_USER_CONTEXT.format(
            mode_actuel=mode.upper(),
            instruction_mode=instruction_mode,
            emotion_detectee=emotion.value if emotion else "neutre",
            emotion_confiance=f"{confiance:.2f}" if confiance else "0.50",
            niveau_engagement=niveau_engagement,
            niveau_utilisateur=profile.get('niveau', 'dÃ©butant'),
            niveau_actuel=progress.niveau,
            xp_actuel=progress.points_xp,
            xp_prochain_niveau=GamificationSystem.xp_pour_niveau_suivant(progress.niveau),
            mots_appris=progress.mots_appris,
            badges_actuels=", ".join(progress.badges[:3]) + ("..." if len(progress.badges) > 3 else "") if progress.badges else "Aucun",
            message_badge="",
            nouveau_niveau="",
            historique_conversation=historique_conversation,
            contexte_rag=contexte_rag_text,
            heure_actuelle=datetime.now().strftime("%H:%M"),
            jour_actuel=datetime.now().strftime("%A %d %B %Y"),
            user_message=message_corrige
        )

        # ğŸ†• v3.2.0 PHASE 1: Call LLM avec max_tokens=8000 et GPT-4-Turbo
        temperature_mode = {
            'conversationnel': 0.7,
            'enseignant': 0.3,
            'Ã©lÃ¨ve': 0.5
        }
        
        llm_resp = await LLM_CLIENT.chat.completions.create(
            model=LLM_MODEL,  # v3.2.0: gpt-4-turbo
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message_content}
            ],
            temperature=temperature_mode.get(mode, 0.5),
            max_tokens=MAX_TOKENS_RESPONSE,  # v3.2.0: 8000
            stream=False
        )
        llm_output = llm_resp.choices[0].message.content
        logging.info("âœ… RÃ©ponse LLM reÃ§ue")

        # Extract text and memory JSON
        def separer_texte_et_json(output: str):
            start = output.find('```json')
            if start == -1:
                return output.strip(), None
            end = output.find('```', start + 7)
            if end == -1:
                return output.strip(), None
            text = output[:start].strip()
            json_str = output[start + 7:end].strip()
            try:
                return text, json.loads(json_str)
            except:
                return output.strip(), None

        response_text, memory_json = separer_texte_et_json(llm_output)
        
        # Ajouter Ã  l'historique avec mÃ©tadonnÃ©es Ã©motionnelles
        ajouter_message_memoire(
            session_id, 
            'user', 
            req.user_message,
            metadata={
                'emotion': emotion.value if emotion else None,
                'emotion_confiance': confiance if confiance else None,
                'corrige': message_corrige if message_corrige != req.user_message else None
            }
        )
        ajouter_message_memoire(session_id, 'assistant', response_text)

        return ChatResponse(
            response_text=response_text,
            session_id=session_id,
            memory_update=memory_json,
            debug_info=debug_info
        )
    
    except Exception as e:
        logging.error(f"âŒ Erreur critique dans /chat: {e}", exc_info=True)
        return ChatResponse(
            response_text=f"Erreur interne : {str(e)}",
            session_id=session_id,
            memory_update=None,
            debug_info={'error': str(e)} if req.debug else None
        )


# --- ENDPOINT AJOUT TRADUCTION ---
@app.post('/add_translation', response_model=dict)
async def add_translation(entries: List[TranslationEntry]):
    """Ajoute une liste de traductions Ã  Qdrant."""
    global LLM_CLIENT, QDRANT_CLIENT

    if LLM_CLIENT is None:
        raise HTTPException(status_code=503, detail='LLM non initialisÃ©')
    if QDRANT_CLIENT is None:
        raise HTTPException(status_code=503, detail='Qdrant non initialisÃ©')

    if not entries:
        return {"status": "warning", "message": "Aucune entrÃ©e fournie."}

    try:
        french_elements = [entry.element_franÃ§ais for entry in entries]
        num_elements = len(french_elements)

        logging.info(f"ğŸ”„ GÃ©nÃ©ration de {num_elements} embeddings...")
        emb_resp = await LLM_CLIENT.embeddings.create(
            input=french_elements,
            model=EMBEDDING_MODEL
        )
        vectors = [data.embedding for data in emb_resp.data]

        points_to_upsert: List[PointStruct] = []
        for i, entry in enumerate(entries):
            payload = entry.model_dump()
            
            point = PointStruct(
                id=uuid.uuid4().int >> 64,
                vector=vectors[i],
                payload=payload
            )
            points_to_upsert.append(point)

        logging.info(f"ğŸ’¾ Upsert de {num_elements} points dans '{COLLECTION_NAME}'...")
        operation_info = await QDRANT_CLIENT.upsert(
            collection_name=COLLECTION_NAME,
            points=points_to_upsert,
            wait=True
        )

        logging.info(f"âœ… {num_elements} traductions ajoutÃ©es. Status: {operation_info.status.value}")
        return {
            "status": "success",
            "message": f"{num_elements} traductions ajoutÃ©es Ã  Qdrant.",
            "qdrant_status": operation_info.status.value,
            "elements_added": num_elements
        }

    except Exception as e:
        logging.error(f"âŒ Erreur ajout traduction: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")


# --- ENDPOINTS MÃ‰MOIRE ---
@app.get('/memory/{session_id}')
async def get_memory(session_id: str, limit: int = 100):
    """RÃ©cupÃ¨re l'historique d'une session."""
    if session_id not in CONVERSATION_MEMORY:
        raise HTTPException(status_code=404, detail='Session non trouvÃ©e')
    
    historique = list(CONVERSATION_MEMORY[session_id])
    
    return {
        'session_id': session_id,
        'total_messages': len(historique),
        'messages': historique[-limit:] if limit else historique
    }


@app.delete('/memory/{session_id}')
async def clear_memory(session_id: str):
    """Efface l'historique d'une session."""
    if session_id in CONVERSATION_MEMORY:
        del CONVERSATION_MEMORY[session_id]
        logging.info(f"ğŸ—‘ï¸ Session {session_id} effacÃ©e")
        return {'status': 'success', 'message': f'MÃ©moire de la session {session_id} effacÃ©e'}
    
    raise HTTPException(status_code=404, detail='Session non trouvÃ©e')


@app.get('/sessions')
async def list_sessions():
    """Liste toutes les sessions actives."""
    sessions_info = []
    
    for session_id, messages in CONVERSATION_MEMORY.items():
        sessions_info.append({
            'session_id': session_id,
            'message_count': len(messages),
            'last_message': messages[-1]['timestamp'] if messages else None
        })
    
    return {
        'total_sessions': len(CONVERSATION_MEMORY),
        'sessions': sessions_info
    }


# ğŸ†• v3.2.0: ENDPOINTS COMPRESSION MÃ‰MOIRE
@app.get('/memory/compression-status/{session_id}')
async def get_compression_status(session_id: str):
    """VÃ©rifie le statut de compression d'une session"""
    if session_id not in CONVERSATION_MEMORY:
        raise HTTPException(status_code=404, detail="Session non trouvÃ©e")
    
    historique = list(CONVERSATION_MEMORY[session_id])
    compresse = any(m.get('compressed', False) for m in historique)
    doit_compresser = MemoryCompressionSystem.doit_compresser(session_id)
    
    return {
        'session_id': session_id,
        'total_messages': len(historique),
        'compresse': compresse,
        'doit_compresser': doit_compresser,
        'threshold': COMPRESSION_THRESHOLD,
        'messages_avant_compression': max(0, COMPRESSION_THRESHOLD - len(historique))
    }


@app.post('/memory/force-compression/{session_id}')
async def force_compression(session_id: str):
    """Force la compression d'une session"""
    if LLM_CLIENT is None:
        raise HTTPException(status_code=503, detail="LLM non disponible")
    
    if session_id not in CONVERSATION_MEMORY:
        raise HTTPException(status_code=404, detail="Session non trouvÃ©e")
    
    avant = len(CONVERSATION_MEMORY[session_id])
    
    compression_ok = await MemoryCompressionSystem.compresser_memoire_ancienne(
        session_id=session_id,
        llm_client=LLM_CLIENT,
        threshold=0  # Force mÃªme si < 50 messages
    )
    
    apres = len(CONVERSATION_MEMORY[session_id]) if compression_ok else avant
    
    return {
        'status': 'success' if compression_ok else 'failed',
        'messages_avant': avant,
        'messages_apres': apres,
        'gain': avant - apres if compression_ok else 0
    }


@app.post('/analyze-message-length')
async def analyze_message_length(message: str):
    """Analyse un message et recommande la stratÃ©gie de traitement"""
    analyse = MessageTypeDetector.analyser_longueur_message(message)
    
    # Si chunking nÃ©cessaire, montrer preview des chunks
    if analyse['chunking']:
        chunks = ChunkingSystem.chunker_texte_long(message, max_chunk=4000)
        analyse['chunks_preview'] = [
            {
                'index': i,
                'longueur': len(chunk),
                'debut': chunk[:100] + "..."
            }
            for i, chunk in enumerate(chunks[:5])
        ]
        analyse['total_chunks'] = len(chunks)
    
    return analyse


# --- ENDPOINTS PROFILS ET GAMIFICATION ---
@app.get('/profile/{session_id}')
async def get_profile(session_id: str):
    """RÃ©cupÃ¨re le profil complet d'un utilisateur."""
    if session_id not in USER_PROFILES:
        raise HTTPException(status_code=404, detail="Profil non trouvÃ©")
    
    return USER_PROFILES[session_id]


@app.put('/profile/{session_id}/preferences')
async def update_preferences(session_id: str, preferences: dict):
    """Met Ã  jour les prÃ©fÃ©rences utilisateur."""
    profile = get_or_create_user_profile(session_id)
    profile['preferences'].update(preferences)
    
    return {
        "status": "success",
        "message": "PrÃ©fÃ©rences mises Ã  jour",
        "preferences": profile['preferences']
    }


@app.get('/leaderboard')
async def get_leaderboard(limit: int = 10):
    """Classement des meilleurs apprenants par XP."""
    if not USER_PROFILES:
        return {
            'total_users': 0,
            'top_users': []
        }
    
    users_sorted = sorted(
        USER_PROFILES.values(),
        key=lambda p: UserProgress(**p['progress']).points_xp,
        reverse=True
    )[:limit]
    
    return {
        'total_users': len(USER_PROFILES),
        'top_users': [
            {
                'session_id': u['session_id'][:8] + '...',
                'niveau': UserProgress(**u['progress']).niveau,
                'xp': UserProgress(**u['progress']).points_xp,
                'mots_appris': UserProgress(**u['progress']).mots_appris,
                'regles_apprises': UserProgress(**u['progress']).regles_apprises,
                'badges': len(UserProgress(**u['progress']).badges),
                'jours_consecutifs': UserProgress(**u['progress']).jours_consecutifs
            }
            for u in users_sorted
        ]
    }


@app.get('/badges')
async def get_all_badges():
    """Liste de tous les badges disponibles dans le systÃ¨me."""
    badges_info = [
        {'nom': Badge.PREMIER_MOT.value, 'critere': '1 mot appris', 'type': 'bronze'},
        {'nom': Badge.DIX_MOTS.value, 'critere': '10 mots appris', 'type': 'argent'},
        {'nom': Badge.CINQUANTE_MOTS.value, 'critere': '50 mots appris', 'type': 'or'},
        {'nom': Badge.CENT_MOTS.value, 'critere': '100 mots appris', 'type': 'diamant'},
        {'nom': Badge.GRAMMAIRIEN.value, 'critere': '5 rÃ¨gles apprises', 'type': 'or'},
        {'nom': Badge.PERSEVERANT.value, 'critere': '7 jours consÃ©cutifs', 'type': 'or'},
        {'nom': Badge.CHAMPION.value, 'critere': 'Niveau 10+', 'type': 'legendaire'}
    ]
    
    return {
        'total_badges': len(badges_info),
        'badges': badges_info
    }


@app.get('/user/{session_id}/progress-summary')
async def get_progress_summary(session_id: str):
    """RÃ©sumÃ© de progression dÃ©taillÃ© pour un utilisateur."""
    if session_id not in USER_PROFILES:
        raise HTTPException(status_code=404, detail="Utilisateur non trouvÃ©")
    
    profile = USER_PROFILES[session_id]
    progress = UserProgress(**profile['progress'])
    
    tous_badges = list(Badge)
    badges_actuels = set(progress.badges)
    badges_manquants = [b for b in tous_badges if b.value not in badges_actuels]
    
    objectifs = []
    if progress.mots_appris < 10:
        objectifs.append(f"Apprends {10 - progress.mots_appris} mots pour le badge ğŸ“š 10 Mots")
    elif progress.mots_appris < 50:
        objectifs.append(f"Apprends {50 - progress.mots_appris} mots pour le badge ğŸ† 50 Mots")
    
    if progress.regles_apprises < 5:
        objectifs.append(f"Apprends {5 - progress.regles_apprises} rÃ¨gles pour le badge ğŸ“– Grammairien")
    
    xp_prochain = GamificationSystem.xp_pour_niveau_suivant(progress.niveau)
    xp_restants = xp_prochain - progress.points_xp
    objectifs.append(f"Gagne {xp_restants} XP pour atteindre le niveau {progress.niveau + 1}")
    
    return {
        'progression_actuelle': {
            'niveau': progress.niveau,
            'xp': progress.points_xp,
            'xp_prochain_niveau': xp_prochain,
            'pourcentage_niveau': round((progress.points_xp / xp_prochain) * 100, 1),
            'mots_appris': progress.mots_appris,
            'regles_apprises': progress.regles_apprises,
            'badges_actuels': progress.badges,
            'jours_consecutifs': progress.jours_consecutifs
        },
        'prochains_objectifs': objectifs,
        'badges_manquants': [b.value for b in badges_manquants],
        'recommandations': [
            "Pratique tous les jours pour maintenir ta sÃ©rie !",
            "Apprends des rÃ¨gles de grammaire pour gagner +25 XP",
            "Explore diffÃ©rents thÃ¨mes de vocabulaire"
        ]
    }


# --- ENDPOINTS UTILITAIRES ---
@app.get('/')
async def root():
    count = 0
    if QDRANT_CLIENT:
        try:
            c = await QDRANT_CLIENT.count(collection_name=COLLECTION_NAME)
            count = c.count
        except:
            pass
    
    return {
        'service': 'Nkotronic API',
        'version': '3.2.1-AsyncOpenAI-GPT4o',
        'features': [
            'RAG',
            'Multi-types',
            'MÃ©moire conversationnelle (200 messages)',
            'Compression automatique',
            'Chunking intelligent',
            'Long Context Master (100k+ chars)',
            'GPT-4o (meilleure qualitÃ© N\'ko)'
        ],
        'status': 'running',
        'llm_status': 'ok' if LLM_CLIENT else 'error',
        'qdrant_status': 'ok' if QDRANT_CLIENT else 'disabled',
        'memory_size': count,
        'active_sessions': len(CONVERSATION_MEMORY),
        'max_chars_embedding': MAX_CHARS_EMBEDDING,
        'max_tokens_response': MAX_TOKENS_RESPONSE,
        'model': LLM_MODEL
    }


@app.get('/health')
async def health():
    health_status = {
        'llm': LLM_CLIENT is not None,
        'qdrant': QDRANT_CLIENT is not None,
        'memory': True
    }
    
    if not all(health_status.values()):
        raise HTTPException(status_code=503, detail=health_status)
    
    return {'status': 'healthy', 'components': health_status}


@app.get('/stats')
async def stats():
    """Statistiques globales du systÃ¨me v3.2.1"""
    if QDRANT_CLIENT is None:
        raise HTTPException(status_code=503, detail='Qdrant non disponible')
    
    try:
        count = await QDRANT_CLIENT.count(collection_name=COLLECTION_NAME)
        sample = await QDRANT_CLIENT.scroll(
            collection_name=COLLECTION_NAME,
            limit=10,
            with_payload=True
        )
        
        total_users = len(USER_PROFILES)
        total_sessions = len(CONVERSATION_MEMORY)
        total_messages = sum(len(hist) for hist in CONVERSATION_MEMORY.values())
        
        # ğŸ†• v3.2.1: Stats TTL
        now = datetime.now()
        sessions_expiring_soon = sum(
            1 for last_activity in SESSION_LAST_ACTIVITY.values()
            if (now - last_activity).total_seconds() > (SESSION_TTL_HOURS - 1) * 3600
        )
        
        oldest_session = None
        if SESSION_LAST_ACTIVITY:
            oldest = min(SESSION_LAST_ACTIVITY.values())
            oldest_session = (now - oldest).total_seconds() / 3600  # En heures
        
        xp_total = sum(
            UserProgress(**p['progress']).points_xp 
            for p in USER_PROFILES.values()
        ) if USER_PROFILES else 0
        
        xp_moyen = xp_total / total_users if total_users > 0 else 0
        
        mots_total = sum(
            p['statistiques']['mots_appris']
            for p in USER_PROFILES.values()
        ) if USER_PROFILES else 0
        
        regles_total = sum(
            p['statistiques']['regles_apprises']
            for p in USER_PROFILES.values()
        ) if USER_PROFILES else 0
        
        badges_total = sum(
            len(UserProgress(**p['progress']).badges)
            for p in USER_PROFILES.values()
        ) if USER_PROFILES else 0
        
        return {
            'version': '3.2.1-AsyncOpenAI-GPT4o',
            'nom': 'Nkotronic AsyncOpenAI + GPT-4o',
            'total_points_qdrant': count.count,
            'total_utilisateurs': total_users,
            'total_sessions': total_sessions,
            'total_messages': total_messages,
            'mots_appris_total': mots_total,
            'regles_apprises_total': regles_total,
            'badges_debloques_total': badges_total,
            'xp_total_cumule': xp_total,
            'xp_moyen_par_user': round(xp_moyen, 2),
            'timestamp': datetime.now().isoformat(),
            'sessions_management': {  # ğŸ†• v3.2.1
                'max_sessions': MAX_SESSIONS,
                'sessions_actives': total_sessions,
                'utilisation': f'{round(total_sessions/MAX_SESSIONS*100, 1)}%',
                'ttl_heures': SESSION_TTL_HOURS,
                'cleanup_interval_min': CLEANUP_INTERVAL_MINUTES,
                'sessions_expiring_soon': sessions_expiring_soon,
                'oldest_session_hours': round(oldest_session, 1) if oldest_session else None
            },
            'capacites_v321': {
                'max_chars_embedding': MAX_CHARS_EMBEDDING,
                'max_tokens_response': MAX_TOKENS_RESPONSE,
                'chunking': 'ActivÃ©',
                'compression_auto': f'Seuil: {COMPRESSION_THRESHOLD} messages',
                'modele': LLM_MODEL,
                'contexte_llm': '128k tokens',
                'client': 'AsyncOpenAI',
                'normalisation_nfc': 'ActivÃ©',
                'retry_auto': '3x'
            },
            'sample_data': [p.payload for p in sample[0][:3]]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post('/search_direct')
async def search_direct(word: str):
    """Recherche directe dans Qdrant pour debug"""
    if QDRANT_CLIENT is None or LLM_CLIENT is None:
        raise HTTPException(status_code=503, detail='Services non disponibles')
    
    try:
        emb_resp = await LLM_CLIENT.embeddings.create(
            input=[word],
            model=EMBEDDING_MODEL
        )
        vector = emb_resp.data[0].embedding
        
        result = await QDRANT_CLIENT.query_points(
            collection_name=COLLECTION_NAME,
            query=vector,
            limit=20,
            with_payload=True
        )
        hits = result.points
        
        return {
            'query': word,
            'results_count': len(hits),
            'top_10': [
                {
                    'score': h.score,
                    'element_franÃ§ais': h.payload.get('element_franÃ§ais', 'N/A'),
                    'element_nko': h.payload.get('element_nko', 'N/A'),
                    'concept': h.payload.get('concept_identifie', 'N/A'),
                    'type': h.payload.get('type', 'mot')
                }
                for h in hits[:10]
            ]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.post('/transcribe_phonetic')
async def transcribe_phonetic(nko_text: str):
    """Transcrit un texte N'ko en phonÃ©tique latine"""
    try:
        transcription = transcrire_nko_phonetique(nko_text)
        
        return {
            'nko_original': nko_text,
            'transcription_complete': transcription
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ğŸ†• v3.2.0: Endpoint de test rapide
@app.get('/test-long-context')
async def test_long_context():
    """Test rapide des capacitÃ©s Long Context v3.2.0"""
    return {
        'status': 'ready',
        'version': '3.2.0',
        'capabilities': {
            'max_chars_embedding': f'{MAX_CHARS_EMBEDDING:,} chars (x5 vs v3.1.7)',
            'max_tokens_response': f'{MAX_TOKENS_RESPONSE:,} tokens (x4 vs v3.1.7)',
            'model': f'{LLM_MODEL} (128k context vs 8k)',
            'chunking': 'Intelligent hierarchical chunking enabled',
            'compression': f'Auto compression at {COMPRESSION_THRESHOLD} messages',
            'memory_size': f'{MAX_MEMORY_SIZE} messages (x2 vs v3.1.7)'
        },
        'test_scenarios': [
            'Message court (<2k): traitement normal',
            'Message moyen (2k-8k): limites augmentÃ©es',
            'Message long (8k-30k): chunking activÃ©',
            'Message trÃ¨s long (>30k): chunking + compression hiÃ©rarchique',
            'Conversation longue (>50 messages): compression automatique'
        ]
    }


if __name__ == '__main__':
    import uvicorn
    uvicorn.run(app, host='0.0.0.0', port=8000)