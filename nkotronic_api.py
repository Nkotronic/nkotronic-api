import asyncio
import os
import logging
import json
import uuid
from contextlib import asynccontextmanager
from typing import Optional, AsyncIterator, List
from pathlib import Path

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
from openai import OpenAI

from qdrant_client import AsyncQdrantClient
from qdrant_client.models import VectorParams, PointStruct, Distance, models

# --- CHARGER LE FICHIER .env ---
try:
    from dotenv import load_dotenv
    env_path = Path('.') / '.env'
    load_dotenv(dotenv_path=env_path)
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logging.info(f"‚úÖ Fichier .env charg√© depuis: {env_path.absolute()}")
except ImportError:
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logging.warning("‚ö†Ô∏è python-dotenv non install√©, utilise les variables d'environnement syst√®me")

# --- LOGGING CONFIG ---
logging.getLogger("qdrant_client").setLevel(logging.WARNING)
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)

# --- CONFIGURATION ---
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
QDRANT_URL = os.getenv("QDRANT_URL", "")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY", "")

# Validation des cl√©s au d√©marrage
if not OPENAI_API_KEY:
    logging.error("‚ùå OPENAI_API_KEY non trouv√©e!")
else:
    logging.info(f"‚úÖ OPENAI_API_KEY charg√©e")

if not QDRANT_URL:
    logging.error("‚ùå QDRANT_URL non trouv√©e!")
else:
    logging.info(f"‚úÖ QDRANT_URL configur√©e")

if not QDRANT_API_KEY:
    logging.warning("‚ö†Ô∏è QDRANT_API_KEY non trouv√©e")
else:
    logging.info(f"‚úÖ QDRANT_API_KEY charg√©e")

# --- GLOBAL CLIENTS ---
LLM_CLIENT: Optional[OpenAI] = None
QDRANT_CLIENT: Optional[AsyncQdrantClient] = None

# --- CONSTANTS ---
COLLECTION_NAME = "nkotronic_knowledge_base"
VECTOR_SIZE = 1536
EMBEDDING_MODEL = "text-embedding-ada-002"
LLM_MODEL = "gpt-4o-mini"
RAG_SCORE_THRESHOLD = 0.55
                            
PROMPT_SYSTEM = (
    "Tu es Nkotronic (ﬂíﬂûﬂèﬂïﬂôﬂèﬂ£ﬂåﬂû), une intelligence artificielle d√©di√©e √† la langue N'ko et √† l'unit√© africaine.\n\n"
    
    "‚ïê‚ïê‚ïê TA PERSONNALIT√â ‚ïê‚ïê‚ïê\n"
    "- Naturel et pos√© (pas surexcit√©)\n"
    "- Varie tes salutations (pas toujours 'Alu ni dj√∂')\n"
    "- Maximum 1 √©moji par message\n"
    "- Concis et pr√©cis\n"
    "- Passionn√© par le N'ko et l'unit√© africaine, mais sans forcer\n\n"
    
    "‚ïê‚ïê‚ïê R√àGLE 1 : D√âTECTION DU TYPE DE MESSAGE ‚ïê‚ïê‚ïê\n"
    "Avant de r√©pondre, identifie le type de message de l'utilisateur:\n\n"
    
    "üîπ TYPE A - SALUTATION:\n"
    "Phrases: bonjour, salut, bonsoir, √ßa va, comment vas-tu, comment tu te sens, etc.\n"
    "‚Üí R√©ponds poliment et propose ton aide\n"
    "‚Üí Exemple: 'Bonjour ! Comment puis-je t\\'aider ?'\n\n"
    
    "üîπ TYPE B - CONVERSATION NORMALE:\n"
    "Questions personnelles: 'tu vas bien ?', 'qui es-tu ?', 'd\\'o√π viens-tu ?', 'c\\'est quoi ton but ?'\n"
    "‚Üí R√©ponds naturellement selon ta personnalit√©\n"
    "‚Üí Exemples:\n"
    "   ‚Ä¢ 'tu vas bien ?' ‚Üí 'Je fonctionne bien, merci ! Et toi ?'\n"
    "   ‚Ä¢ 'qui es-tu ?' ‚Üí 'Je suis Nkotronic, une IA d√©di√©e au N\\'ko et √† l\\'unit√© africaine.'\n"
    "   ‚Ä¢ 'd\\'o√π viens-tu ?' ‚Üí 'Je suis une IA cr√©√©e pour pr√©server et enseigner le N\\'ko.'\n\n"
    
    "üîπ TYPE C - TRADUCTION / RECHERCHE:\n"
    "Demandes explicites: 'comment dit-on X', 'traduis Y', 'c\\'est quoi X en nko', 'X en fran√ßais', etc.\n"
    "‚Üí Utilise OBLIGATOIREMENT les TRADUCTIONS CONTEXTUELLES si pr√©sentes\n"
    "‚Üí Si absent de la m√©moire: 'Je n\\'ai pas cette traduction en m√©moire. Veux-tu me l\\'apprendre ?'\n\n"
    
    "üîπ TYPE D - ENSEIGNEMENT:\n"
    "L'utilisateur t'enseigne: 'X se dit Y en nko', 'voici la traduction de X', 'apprends que X = Y'\n"
    "‚Üí Confirme: 'D\\'accord, j\\'ai enregistr√© que X se dit Y en N\\'ko.'\n"
    "‚Üí G√©n√®re le JSON de m√©morisation (voir format ci-dessous)\n\n"
    
    "‚ïê‚ïê‚ïê R√àGLE 2 : UTILISATION OBLIGATOIRE DES TRADUCTIONS CONTEXTUELLES ‚ïê‚ïê‚ïê\n"
    "Si tu vois une section 'TRADUCTIONS CONTEXTUELLES' ci-dessous, tu DOIS les utiliser.\n"
    "Ne r√©ponds JAMAIS de mani√®re g√©n√©rique si des traductions sont fournies.\n\n"
    
    "Exemple 1:\n"
    "TRADUCTIONS CONTEXTUELLES:\n"
    "- ﬂõﬂìﬂçﬂõﬂéﬂ≤ = lettre\n"
    "Question: 'C\\'est quoi ﬂõﬂìﬂçﬂõﬂéﬂ≤ ?'\n"
    "‚Üí R√©ponse: 'ﬂõﬂìﬂçﬂõﬂéﬂ≤ signifie \"lettre\" en fran√ßais.'\n\n"
    
    "Exemple 2:\n"
    "TRADUCTIONS CONTEXTUELLES:\n"
    "- ﬂñﬂå = eau\n"
    "Question: 'Comment dit-on ﬂñﬂå en fran√ßais ?'\n"
    "‚Üí R√©ponse: 'ﬂñﬂå se dit \"eau\" en fran√ßais.'\n\n"
    
    "‚ïê‚ïê‚ïê M√âMOIRE ET CONTEXTE ‚ïê‚ïê‚ïê\n"
    "{{contexte_rag}}\n\n"
    
    "‚ïê‚ïê‚ïê CAPACIT√âS AVANC√âES ‚ïê‚ïê‚ïê\n"
    "- Tu peux compter en N'ko (ﬂÄ ﬂÅ ﬂÇ ﬂÉ ﬂÑ ﬂÖ ﬂÜ ﬂá ﬂà ﬂâ)\n"
    "- Tu analyses la grammaire N'ko\n"
    "- Tu partages l'histoire et la culture mandingue\n"
    "- Tu discutes de l'unit√© africaine avec sagesse\n\n"
    
    "‚ïê‚ïê‚ïê FORMAT DE M√âMORISATION ‚ïê‚ïê‚ïê\n"
    "Quand tu apprends une nouvelle traduction (TYPE D), g√©n√®re ce JSON √† la fin de ta r√©ponse:\n"
    "```json\n"
    "{{{{\n"
    "  \"element_fran√ßais\": \"mot ou phrase\",\n"
    "  \"element_nko\": \"ﬂíﬂûﬂè traduction\",\n"
    "  \"concept_identifie\": \"Cat√©gorie (Salutation/Vocabulaire/Grammaire/etc.)\"\n"
    "}}}}\n"
    "```\n\n"
    
    "‚ïê‚ïê‚ïê EXEMPLES DE COMPORTEMENT ‚ïê‚ïê‚ïê\n"
    
    "Exemple 1 - Salutation:\n"
    "User: 'Bonjour'\n"
    "Toi: 'Bonjour ! Comment puis-je t\\'aider ?'\n\n"
    
    "Exemple 2 - Question personnelle:\n"
    "User: 'Tu vas bien ?'\n"
    "Toi: 'Je fonctionne bien, merci ! Que puis-je faire pour toi ?'\n\n"
    
    "Exemple 3 - Traduction avec contexte:\n"
    "TRADUCTIONS CONTEXTUELLES: ﬂõﬂìﬂçﬂõﬂéﬂ≤ = lettre\n"
    "User: 'C\\'est quoi ﬂõﬂìﬂçﬂõﬂéﬂ≤ ?'\n"
    "Toi: 'ﬂõﬂìﬂçﬂõﬂéﬂ≤ signifie \"lettre\" en fran√ßais.'\n\n"
    
    "Exemple 4 - Traduction sans contexte:\n"
    "User: 'Comment dit-on ordinateur en N\\'ko ?'\n"
    "Toi: 'Je n\\'ai pas cette traduction en m√©moire. Veux-tu me l\\'apprendre ?'\n\n"
    
    "Exemple 5 - Enseignement:\n"
    "User: 'Pierre se dit ﬂûﬂäﬂìﬂä en N\\'ko'\n"
    "Toi: 'D\\'accord, j\\'ai enregistr√© que pierre se dit ﬂûﬂäﬂìﬂä en N\\'ko. Merci !'\n"
    "```json\n"
    "{{{{\n"
    "  \"element_fran√ßais\": \"pierre\",\n"
    "  \"element_nko\": \"ﬂûﬂäﬂìﬂä\",\n"
    "  \"concept_identifie\": \"G√©ologie\"\n"
    "}}}}\n"
    "```\n\n"
    
    "Exemple 6 - Question sur l'Afrique:\n"
    "User: 'Pourquoi l\\'Afrique ne s\\'unit pas ?'\n"
    "Toi: 'Question complexe. L\\'Afrique porte les cicatrices de la colonisation : fronti√®res artificielles, rivalit√©s encourag√©es. L\\'unit√© n\\'est pas sentimentale, c\\'est une n√©cessit√© strat√©gique. Un continent uni p√®se dans le monde.'\n\n"
    
    "‚ïê‚ïê‚ïê TON STYLE ‚ïê‚ïê‚ïê\n"
    "- Identifie TOUJOURS le type de message d'abord\n"
    "- Utilise les traductions contextuelles quand pr√©sentes\n"
    "- Sois naturel dans les conversations\n"
    "- Reste factuel pour les traductions\n"
    "- Inspire subtilement pour l'unit√© africaine\n\n"
    
    "‚ïê‚ïê‚ïê QUESTION DE L'UTILISATEUR ‚ïê‚ïê‚ïê\n"
    "{{user_message}}\n\n"
    
    "R√âPONDS MAINTENANT (identifie d'abord le type de message, puis utilise les traductions contextuelles si pr√©sentes) :"
)

@asynccontextmanager
async def lifespan(app: FastAPI) -> AsyncIterator[dict]:
    global LLM_CLIENT, QDRANT_CLIENT
    logging.info("üöÄ D√©marrage de l'API Nkotronic...")

    # 1Ô∏è‚É£ INIT OpenAI
    try:
        if not OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEY manquante!")
        
        LLM_CLIENT = OpenAI(api_key=OPENAI_API_KEY, timeout=30.0)
        test_response = await asyncio.to_thread(
            LLM_CLIENT.chat.completions.create,
            model=LLM_MODEL,
            messages=[{"role": "user", "content": "test"}],
            max_tokens=5
        )
        logging.info("‚úÖ Client OpenAI initialis√© et test√©")
    except Exception as e:
        logging.error(f"‚ùå Erreur OpenAI: {e}")
        LLM_CLIENT = None
        yield {}
        return

    # 2Ô∏è‚É£ INIT Qdrant
    if QDRANT_URL and QDRANT_API_KEY:
        try:
            QDRANT_CLIENT = AsyncQdrantClient(
                url=QDRANT_URL,
                api_key=QDRANT_API_KEY,
                prefer_grpc=False,
                timeout=30.0
            )
            
            # V√©rifier si la collection existe d√©j√†
            collections = await QDRANT_CLIENT.get_collections()
            exists = any(c.name == COLLECTION_NAME for c in collections.collections)
            
            if exists:
                count = await QDRANT_CLIENT.count(collection_name=COLLECTION_NAME)
                logging.info(f"‚úÖ Collection '{COLLECTION_NAME}' trouv√©e avec {count.count} points")
            else:
                await QDRANT_CLIENT.create_collection(
                    collection_name=COLLECTION_NAME,
                    vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)
                )
                logging.info(f"‚úÖ Collection '{COLLECTION_NAME}' cr√©√©e")

        except Exception as e:
            logging.error(f"‚ùå Erreur Qdrant: {e}")
            QDRANT_CLIENT = None
    else:
        logging.warning("‚ö†Ô∏è Qdrant non configur√©")

    logging.info("‚úÖ API Nkotronic pr√™te!")
    yield {}
    logging.info("üõë Arr√™t de l'API Nkotronic")

app = FastAPI(
    title="Nkotronic API",
    description="API de traduction Fran√ßais ‚Üî N'ko avec m√©moire RAG",
    version="2.0.0",
    lifespan=lifespan
)

# CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- MODELS ---
class ChatRequest(BaseModel):
    user_message: str = Field(..., description="Message utilisateur")
    rag_enabled: bool = Field(True, description="Activer le RAG")
    debug: bool = Field(False, description="Mode debug avec d√©tails")

class ChatResponse(BaseModel):
    response_text: str = Field(..., description="Texte de r√©ponse")
    memory_update: Optional[dict] = Field(None, description="Mise √† jour m√©moire")
    debug_info: Optional[dict] = Field(None, description="Infos de debug")

class TranslationEntry(BaseModel):
    element_fran√ßais: str = Field(..., description="Le mot ou expression en fran√ßais.")
    element_nko: str = Field(..., description="La traduction correspondante en N'ko.")
    concept_identifie: str = Field("G√©n√©ral", description="Le domaine ou concept identifi√©.")

# --- FONCTION D'EXTRACTION MOT-CL√â ---
async def extraire_mot_cle(user_message: str, llm_client: OpenAI) -> str:
    """Extrait le mot fran√ßais √† traduire de mani√®re robuste."""
    import re
    
    # Recherche de mots entre guillemets
    quoted = re.findall(r"['\"]([^'\"]+)['\"]", user_message)
    if quoted:
        mot = quoted[0].strip().lower()
        logging.info(f"üîë Mot extrait des guillemets: '{mot}'")
        return mot
    
    # Extraction via LLM
    prompt = f"""Extrait UNIQUEMENT le mot fran√ßais √† traduire. R√©ponds avec UN SEUL MOT.

Exemples:
- "comment dit-on silex en n'ko" -> silex
- "traduction de bonjour" -> bonjour
- "c'est quoi eau" -> eau

Question: {user_message}
Mot:"""

    try:
        resp = await asyncio.to_thread(
            llm_client.chat.completions.create,
            model=LLM_MODEL,
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=10
        )
        mot = resp.choices[0].message.content.strip().lower()
        mot = re.sub(r'[^\w\s-]', '', mot).strip()
        logging.info(f"üîë Mot-cl√© extrait par LLM: '{mot}'")
        return mot
    except Exception as e:
        logging.error(f"‚ùå Erreur extraction: {e}")
        words = user_message.lower().split()
        stop_words = {'comment', 'dit', 'on', 'en', 'nko', 'n\'ko', 'traduction', 'de', 'le', 'la', 'un', 'une', 'c\'est', 'quoi'}
        significant = [w for w in words if w not in stop_words and len(w) > 2]
        return significant[-1] if significant else user_message.lower()

# --- RECHERCHE MULTI-STRAT√âGIE ---
async def recherche_intelligente(mot_cle: str, llm_client: OpenAI, qdrant_client: AsyncQdrantClient):
    """Recherche avec plusieurs strat√©gies pour maximiser les r√©sultats."""
    all_results = []
    
    # STRAT√âGIE 1: Recherche exacte
    try:
        logging.info(f"üîç Strat√©gie 1: Recherche exacte pour '{mot_cle}'")
        emb_resp = await asyncio.to_thread(
            llm_client.embeddings.create,
            input=[mot_cle],
            model=EMBEDDING_MODEL
        )
        vector = emb_resp.data[0].embedding
        
        result = await qdrant_client.query_points(
            collection_name=COLLECTION_NAME,
            query=vector,
            limit=20,
            with_payload=True
        )
        hits = result.points
        all_results.extend(hits)
        logging.info(f"   -> {len(hits)} r√©sultats trouv√©s")
    except Exception as e:
        logging.error(f"‚ùå Strat√©gie 1 √©chou√©e: {e}")
    
    # STRAT√âGIE 2: Recherche avec variantes
    variantes = [
        mot_cle,
        mot_cle + 's',
        mot_cle.rstrip('s'),
        mot_cle.replace('√©', 'e').replace('√®', 'e').replace('√™', 'e'),
    ]
    variantes = list(set(variantes))
    
    if len(variantes) > 1:
        try:
            logging.info(f"üîç Strat√©gie 2: Recherche avec variantes {variantes}")
            emb_resp = await asyncio.to_thread(
                llm_client.embeddings.create,
                input=variantes,
                model=EMBEDDING_MODEL
            )
            
            for i, var in enumerate(variantes[1:], 1):
                vector = emb_resp.data[i].embedding
                result = await qdrant_client.query_points(
                    collection_name=COLLECTION_NAME,
                    query=vector,
                    limit=10,
                    with_payload=True
                )
                hits = result.points
                all_results.extend(hits)
            logging.info(f"   -> {len(all_results)} r√©sultats totaux")
        except Exception as e:
            logging.error(f"‚ùå Strat√©gie 2 √©chou√©e: {e}")
    
    # STRAT√âGIE 3: √âchantillon de la base
    try:
        sample = await qdrant_client.scroll(
            collection_name=COLLECTION_NAME,
            limit=5,
            with_payload=True
        )
        logging.info(f"üìö √âchantillon de la base (5 premiers):")
        for point in sample[0]:
            logging.info(f"   - {point.payload}")
    except Exception as e:
        logging.error(f"‚ùå √âchantillon √©chou√©: {e}")
    
    # D√©dupliquer et trier
    seen_ids = set()
    unique_results = []
    for hit in all_results:
        if hit.id not in seen_ids:
            seen_ids.add(hit.id)
            unique_results.append(hit)
    
    unique_results.sort(key=lambda x: x.score, reverse=True)
    return unique_results

# --- PR√â-TRAITEMENT INTELLIGENT ---
async def pretraiter_question(user_message: str, llm_client: OpenAI, qdrant_client: AsyncQdrantClient):
    """D√©tecte les mots N'ko et les traduit pour enrichir la recherche."""
    import re
    
    # Regex pour d√©tecter les caract√®res N'ko (U+07C0 √† U+07FF)
    nko_pattern = re.compile(r'[\u07C0-\u07FF]+')
    nko_words = nko_pattern.findall(user_message)
    
    if not nko_words:
        return user_message, []
    
    logging.info(f"üîç Mots N'ko d√©tect√©s dans la question: {nko_words}")
    
    traductions = []
    for nko_word in nko_words:
        try:
            # Cr√©er un embedding du mot N'ko
            emb_resp = await asyncio.to_thread(
                llm_client.embeddings.create,
                input=[nko_word],
                model=EMBEDDING_MODEL
            )
            vector = emb_resp.data[0].embedding
            
            # Rechercher les points similaires
            results = await qdrant_client.query_points(
                collection_name=COLLECTION_NAME,
                query=vector,
                limit=10,
                with_payload=True
            )
            
            # Chercher celui qui a exactement ce mot N'ko
            for point in results.points:
                if point.payload.get('element_nko') == nko_word:
                    fr = point.payload.get('element_fran√ßais')
                    if fr:
                        traductions.append({
                            'nko': nko_word,
                            'fran√ßais': fr,
                            'payload': point.payload
                        })
                        logging.info(f"‚úÖ Traduction trouv√©e: {nko_word} = {fr}")
                        break
            
            if not any(t['nko'] == nko_word for t in traductions):
                logging.warning(f"‚ö†Ô∏è Aucune traduction trouv√©e pour: {nko_word}")
                
        except Exception as e:
            logging.error(f"‚ùå Erreur lors de la recherche de {nko_word}: {e}")
    
    # Enrichir la question
    question_enrichie = user_message
    for trad in traductions:
        question_enrichie = question_enrichie.replace(
            trad['nko'], 
            f"{trad['nko']} ({trad['fran√ßais']})"
        )
    
    if traductions:
        logging.info(f"üí° Question enrichie: {question_enrichie}")
    
    return question_enrichie, traductions

# --- ENDPOINT CHAT ---
@app.post('/chat', response_model=ChatResponse)
async def chat_endpoint(req: ChatRequest):
    global LLM_CLIENT, QDRANT_CLIENT

    if LLM_CLIENT is None:
        raise HTTPException(status_code=503, detail='LLM non initialis√©')

    debug_info = {} if req.debug else None
    rag_active = req.rag_enabled and (QDRANT_CLIENT is not None)
    contexte_rag_text = ''

    try:
        if rag_active:
            try:
                # Pr√©-traiter la question
                question_enrichie, traductions_contexte = await pretraiter_question(
                    req.user_message, 
                    LLM_CLIENT, 
                    QDRANT_CLIENT
                )
                
                if req.debug:
                    debug_info['question_enrichie'] = question_enrichie
                    debug_info['traductions_contexte'] = traductions_contexte
                
                # Extraire le mot-cl√©
                mot_cle = await extraire_mot_cle(question_enrichie, LLM_CLIENT)
                if req.debug:
                    debug_info['mot_cle_extrait'] = mot_cle

                # Recherche intelligente
                hits = await recherche_intelligente(mot_cle, LLM_CLIENT, QDRANT_CLIENT)

                # Afficher top r√©sultats
                logging.info(f"üìä TOP 10 R√âSULTATS pour '{mot_cle}':")
                for i, h in enumerate(hits[:10], 1):
                    logging.info(f"  #{i}: score={h.score:.4f} -> {h.payload.get('element_fran√ßais', 'N/A')}")
                
                if req.debug:
                    debug_info['top_results'] = [
                        {'score': h.score, 'payload': h.payload} 
                        for h in hits[:10]
                    ]

                # Filtrer r√©sultats pertinents
                pertinents = [h for h in hits if h.score > RAG_SCORE_THRESHOLD]

                if pertinents:
                    logging.info(f"‚úÖ {len(pertinents)} r√©sultat(s) pertinent(s) (score > {RAG_SCORE_THRESHOLD})")
                    contexte_connaissances = '\n'.join(
                        json.dumps(h.payload, ensure_ascii=False) 
                        for h in pertinents[:5]
                    )
                else:
                    logging.warning(f"‚ö†Ô∏è Aucun r√©sultat > {RAG_SCORE_THRESHOLD}")
                    if hits:
                        logging.info(f"üí° Utilisation des 3 meilleurs r√©sultats")
                        contexte_connaissances = '\n'.join(
                            json.dumps(h.payload, ensure_ascii=False) 
                            for h in hits[:3]
                        )
                    else:
                        contexte_connaissances = ""

                # Construire contexte enrichi
                contexte_rag_text = ""
                
                if traductions_contexte:
                    contexte_rag_text += "‚ïê‚ïê‚ïê TRADUCTIONS CONTEXTUELLES ‚ïê‚ïê‚ïê\n"
                    for trad in traductions_contexte:
                        contexte_rag_text += f"- {trad['nko']} = {trad['fran√ßais']}\n"
                    contexte_rag_text += "\n"
                
                contexte_rag_text += "‚ïê‚ïê‚ïê CONNAISSANCES PERTINENTES ‚ïê‚ïê‚ïê\n"
                if contexte_connaissances:
                    contexte_rag_text += contexte_connaissances
                else:
                    contexte_rag_text += "[Aucune connaissance trouv√©e]"

            except Exception as e:
                logging.error(f"‚ùå Erreur RAG: {e}", exc_info=True)
                if req.debug:
                    debug_info['rag_error'] = str(e)
                rag_active = False

        # Debug: afficher le contexte envoy√©
        logging.info(f"üì§ CONTEXTE ENVOY√â AU LLM:\n{contexte_rag_text[:500]}")

        # Build prompt
        prompt = PROMPT_SYSTEM.format(
            contexte_rag=contexte_rag_text if contexte_rag_text else '[Aucune traduction trouv√©e en m√©moire]',
            user_message=req.user_message
        )

        # Call LLM
        llm_resp = await asyncio.to_thread(
            LLM_CLIENT.chat.completions.create,
            model=LLM_MODEL,
            messages=[{"role": "system", "content": prompt}],
            temperature=0.3,
            max_tokens=300
        )
        llm_output = llm_resp.choices[0].message.content
        logging.info("‚úÖ R√©ponse LLM re√ßue")

        # Extract text and memory JSON
        def separer_texte_et_json(output: str):
            start = output.find('```json')
            if start == -1:
                return output.strip(), None
            end = output.find('```', start + 7)
            if end == -1:
                return output.strip(), None
            text = output[:start].strip()
            json_str = output[start + 7:end].strip()
            try:
                return text, json.loads(json_str)
            except:
                return output.strip(), None

        response_text, memory_json = separer_texte_et_json(llm_output)

        return ChatResponse(
            response_text=response_text,
            memory_update=memory_json,
            debug_info=debug_info
        )
    
    except Exception as e:
        logging.error(f"‚ùå Erreur critique dans /chat: {e}", exc_info=True)
        return ChatResponse(
            response_text=f"Erreur interne : {str(e)}",
            memory_update=None,
            debug_info={'error': str(e)} if req.debug else None
        )

# --- ENDPOINT AJOUT TRADUCTION ---
@app.post('/add_translation', response_model=dict)
async def add_translation(entries: List[TranslationEntry]):
    """Ajoute une liste de traductions √† Qdrant."""
    global LLM_CLIENT, QDRANT_CLIENT

    if LLM_CLIENT is None:
        raise HTTPException(status_code=503, detail='LLM non initialis√©')
    if QDRANT_CLIENT is None:
        raise HTTPException(status_code=503, detail='Qdrant non initialis√©')

    if not entries:
        return {"status": "warning", "message": "Aucune entr√©e fournie."}

    try:
        french_elements = [entry.element_fran√ßais for entry in entries]
        num_elements = len(french_elements)

        logging.info(f"üîÑ G√©n√©ration de {num_elements} embeddings...")
        emb_resp = await asyncio.to_thread(
            LLM_CLIENT.embeddings.create,
            input=french_elements,
            model=EMBEDDING_MODEL
        )
        vectors = [data.embedding for data in emb_resp.data]

        points_to_upsert: List[PointStruct] = []
        for i, entry in enumerate(entries):
            payload = entry.model_dump()
            
            point = PointStruct(
                id=uuid.uuid4().int >> 64,
                vector=vectors[i],
                payload=payload
            )
            points_to_upsert.append(point)

        logging.info(f"üíæ Upsert de {num_elements} points dans '{COLLECTION_NAME}'...")
        operation_info = await QDRANT_CLIENT.upsert(
            collection_name=COLLECTION_NAME,
            points=points_to_upsert,
            wait=True
        )

        logging.info(f"‚úÖ {num_elements} traductions ajout√©es. Status: {operation_info.status.value}")
        return {
            "status": "success",
            "message": f"{num_elements} traductions ajout√©es √† Qdrant.",
            "qdrant_status": operation_info.status.value,
            "elements_added": num_elements
        }

    except Exception as e:
        logging.error(f"‚ùå Erreur ajout traduction: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Erreur: {str(e)}")

# --- ENDPOINTS UTILITAIRES ---
@app.get('/')
async def root():
    count = 0
    if QDRANT_CLIENT:
        try:
            c = await QDRANT_CLIENT.count(collection_name=COLLECTION_NAME)
            count = c.count
        except:
            pass
    
    return {
        'service': 'Nkotronic API',
        'version': '2.0.0',
        'status': 'running',
        'llm_status': 'ok' if LLM_CLIENT else 'error',
        'qdrant_status': 'ok' if QDRANT_CLIENT else 'disabled',
        'memory_size': count
    }

@app.get('/health')
async def health():
    health_status = {
        'llm': LLM_CLIENT is not None,
        'qdrant': QDRANT_CLIENT is not None
    }
    
    if not all(health_status.values()):
        raise HTTPException(status_code=503, detail=health_status)
    
    return {'status': 'healthy', 'components': health_status}

@app.get('/stats')
async def stats():
    """Statistiques de la base de donn√©es"""
    if QDRANT_CLIENT is None:
        raise HTTPException(status_code=503, detail='Qdrant non disponible')
    
    try:
        count = await QDRANT_CLIENT.count(collection_name=COLLECTION_NAME)
        sample = await QDRANT_CLIENT.scroll(
            collection_name=COLLECTION_NAME,
            limit=10,
            with_payload=True
        )
        
        return {
            'total_points': count.count,
            'collection_name': COLLECTION_NAME,
            'sample': [p.payload for p in sample[0]]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post('/search_direct')
async def search_direct(word: str):
    """Recherche directe dans Qdrant pour debug"""
    if QDRANT_CLIENT is None or LLM_CLIENT is None:
        raise HTTPException(status_code=503, detail='Services non disponibles')
    
    try:
        emb_resp = await asyncio.to_thread(
            LLM_CLIENT.embeddings.create,
            input=[word],
            model=EMBEDDING_MODEL
        )
        vector = emb_resp.data[0].embedding
        
        result = await QDRANT_CLIENT.query_points(
            collection_name=COLLECTION_NAME,
            query=vector,
            limit=20,
            with_payload=True
        )
        hits = result.points
        
        return {
            'query': word,
            'results_count': len(hits),
            'top_10': [
                {
                    'score': h.score,
                    'element_fran√ßais': h.payload.get('element_fran√ßais', 'N/A'),
                    'element_nko': h.payload.get('element_nko', 'N/A'),
                    'concept': h.payload.get('concept_identifie', 'N/A')
                }
                for h in hits[:10]
            ]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))