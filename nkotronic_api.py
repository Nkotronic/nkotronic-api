# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  NKOTRONIC BACKEND - VERSION GEMINI ULTIME (FULL)          â•‘
# â•‘  âœ… Prompt systÃ¨me complet + Grammaire                     â•‘
# â•‘  âœ… Gestion de l'historique Persistant                     â•‘
# â•‘  âœ… ParamÃ¨tres de sÃ©curitÃ© dÃ©sactivÃ©s (pour la prÃ©cision)  â•‘
# â•‘  âœ… Streaming SSE optimisÃ©                                  â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import os
import json
from typing import List, Optional, Dict
from datetime import datetime
from collections import OrderedDict

app = FastAPI(title="Nkotronic API - Gemini Edition", version="5.1.0")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PROMPT SYSTÃˆME & CHARGEMENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

EXPLANATORY_PROMPT = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ“š CONTEXTE ESSENTIEL SUR LE N'KO                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Le N'ko (ß’ßß) est SIMULTANÃ‰MENT un Alphabet et une Langue.
Inventeur : FodÃ© SolomÄna KantÃª (ßßß˜ß‹ß« ß›ßŸßß¬ß¡ßŠß¯ß£ßŠß« ßßŠß²ß•ßß«).

ğŸ¯ TON RÃ”LE :
- Tu es Nkotronic, l'assistant IA expert en N'ko.
- Tu maÃ®trises la grammaire ß’ßß ßßŠß²ßœß.
- TRADUCTION TECHNIQUE : Ne simplifie JAMAIS. Utilise les termes exacts fournis :
  Ex: MÃ©tÃ©orologie = ß“ßŠß²ß˜ßŠß¦ß‹ß™ß‹ß²ßŸßß²ß˜ßß¦ßŠ, AÃ©ronautique = ß›ßŠß²ß¡ßŠß–ßŠß²ß“ßŠß²ß¡ßŠ, Pilote = ßŸßŠßœßŠß²ß ßŠ.
"""

GRAMMAR_FILE_PATH = "Tu es Nkotronic, l'IA.txt"
NKOTRONIC_SYSTEM_PROMPT = ""
LOADING_STATUS = {"status": "init", "message": "Attente...", "progress": 0, "loaded": False}

def load_system_prompt():
    global NKOTRONIC_SYSTEM_PROMPT, LOADING_STATUS
    try:
        if os.path.exists(GRAMMAR_FILE_PATH):
            with open(GRAMMAR_FILE_PATH, 'r', encoding='utf-8') as f:
                grammar_content = f.read()
            NKOTRONIC_SYSTEM_PROMPT = EXPLANATORY_PROMPT + "\n\n" + grammar_content
            LOADING_STATUS.update({"status": "ready", "loaded": True, "progress": 100})
            print(f"âœ… Grammaire chargÃ©e ({len(NKOTRONIC_SYSTEM_PROMPT)} chars)")
            return True
        else:
            NKOTRONIC_SYSTEM_PROMPT = EXPLANATORY_PROMPT
            LOADING_STATUS.update({"status": "warning", "message": "Fichier grammaire manquant", "loaded": True})
            return False
    except Exception as e:
        LOADING_STATUS.update({"status": "error", "message": str(e), "loaded": False})
        return False

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GESTION DES SESSIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class SessionData(BaseModel):
    history: List[Dict] = []
    last_activity: datetime

sessions: OrderedDict[str, SessionData] = OrderedDict()

def get_session(session_id: str) -> SessionData:
    if session_id in sessions:
        sessions.move_to_end(session_id)
        return sessions[session_id]
    new_session = SessionData(history=[], last_activity=datetime.now())
    sessions[session_id] = new_session
    if len(sessions) > 1000: sessions.popitem(last=False)
    return new_session

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# LOGIQUE CHAT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class ChatRequest(BaseModel):
    message: str
    session_id: str = "default"
    model: str = "gemini-1.5-flash" # Ou gemini-1.5-pro
    temperature: float = 0.7
    max_tokens: int = 4000

@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    async def generate():
        try:
            api_key = os.environ.get("GEMINI_API_KEY")
            if not api_key:
                yield f"data: {json.dumps({'error': 'ClÃ© API manquante'})}\n\n"
                return

            genai.configure(api_key=api_key)
            
            # Configuration de sÃ©curitÃ© : on autorise tout pour Ã©viter les blocages sur les termes techniques
            safety_settings = {
                HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
                HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
            }

            model = genai.GenerativeModel(
                model_name=request.model,
                system_instruction=NKOTRONIC_SYSTEM_PROMPT,
                safety_settings=safety_settings
            )

            session = get_session(request.session_id)
            chat = model.start_chat(history=session.history)
            
            full_response = ""
            response_stream = chat.send_message(
                request.message,
                stream=True,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=request.max_tokens,
                    temperature=request.temperature,
                )
            )
            
            for chunk in response_stream:
                if chunk.text:
                    full_response += chunk.text
                    yield f"data: {json.dumps({'content': chunk.text})}\n\n"

            # Mise Ã  jour de l'historique
            session.history = chat.history
            session.last_activity = datetime.now()
            
            yield f"data: {json.dumps({'done': True, 'session_id': request.session_id})}\n\n"
            
        except Exception as e:
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return StreamingResponse(generate(), media_type="text/event-stream")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENDPOINTS DE MAINTENANCE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@app.get("/")
async def root():
    return {"status": "online", "model": "Gemini-Nkotronic"}

@app.get("/loading-status")
async def status():
    return LOADING_STATUS

@app.on_event("startup")
async def startup():
    load_system_prompt()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)